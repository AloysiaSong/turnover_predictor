# Week 1-2 å®æ–½æŒ‡å—ï¼šæ•°æ®å‡†å¤‡ä¸åŸºçº¿æ¨¡å‹

**é¡¹ç›®**: å¼‚æ„å›¾ç¥ç»ç½‘ç»œç¦»èŒé¢„æµ‹  
**æ—¶é—´**: Week 1-2 (10-14ä¸ªå·¥ä½œæ—¥)  
**ç›®æ ‡**: å®Œæˆæ•°æ®é¢„å¤„ç†å¹¶å»ºç«‹MLPåŸºçº¿æ¨¡å‹  
**è¾“å‡º**: å¹²å‡€çš„è®­ç»ƒæ•°æ® + MLPåŸºçº¿æ€§èƒ½æŠ¥å‘Š

---

## ğŸ“‹ æ€»ä½“å·¥ä½œæµç¨‹

```
Week 1-2 å·¥ä½œæµç¨‹
â”œâ”€â”€ Day 1-2: ç¯å¢ƒé…ç½®ä¸æ•°æ®æ¢ç´¢
â”œâ”€â”€ Day 3-4: ç‰¹å¾å·¥ç¨‹ä¸æ•°æ®æ¸…æ´—
â”œâ”€â”€ Day 5-6: è¾¹æ„å»ºä¸å›¾æ•°æ®å‡†å¤‡
â”œâ”€â”€ Day 7-8: æ•°æ®é›†åˆ’åˆ†ä¸éªŒè¯
â”œâ”€â”€ Day 9-10: MLPåŸºçº¿æ¨¡å‹å®ç°
â””â”€â”€ Day 11-14: åŸºçº¿è¯„ä¼°ä¸æ–‡æ¡£æ•´ç†
```

---

## ğŸ¯ Day 1-2: ç¯å¢ƒé…ç½®ä¸æ•°æ®æ¢ç´¢

### ä»»åŠ¡æ¸…å•
- [ ] é…ç½®Pythonç¯å¢ƒ
- [ ] å®‰è£…å¿…è¦çš„åº“
- [ ] åŠ è½½å¹¶æ¢ç´¢åŸå§‹æ•°æ®
- [ ] ç”Ÿæˆæ•°æ®è´¨é‡æŠ¥å‘Š

### 1.1 ç¯å¢ƒé…ç½®

#### åˆ›å»ºè™šæ‹Ÿç¯å¢ƒ
```bash
# ä½¿ç”¨conda (æ¨è)
conda create -n hgnn_project python=3.9
conda activate hgnn_project

# æˆ–ä½¿ç”¨venv
python -m venv hgnn_env
source hgnn_env/bin/activate  # Linux/Mac
# hgnn_env\Scripts\activate  # Windows
```

#### å®‰è£…ä¾èµ–åŒ…
```bash
# æ ¸å¿ƒä¾èµ–
pip install pandas==1.5.3
pip install numpy==1.24.3
pip install scikit-learn==1.3.0
pip install matplotlib==3.7.1
pip install seaborn==0.12.2

# æ·±åº¦å­¦ä¹ æ¡†æ¶
pip install torch==2.0.1
pip install torch-geometric==2.3.1

# å¯é€‰å·¥å…·
pip install jupyter
pip install tqdm
pip install wandb  # å®éªŒè·Ÿè¸ª
```

#### åˆ›å»ºé¡¹ç›®ç›®å½•ç»“æ„
```bash
mkdir -p hgnn_turnover_prediction
cd hgnn_turnover_prediction

# åˆ›å»ºå­ç›®å½•
mkdir -p data/{raw,processed,splits}
mkdir -p models/{mlp,gnn}
mkdir -p notebooks
mkdir -p outputs/{figures,reports,logs}
mkdir -p src/{data_processing,features,models,evaluation}
mkdir -p configs

# åˆ›å»ºå¿…è¦æ–‡ä»¶
touch README.md
touch requirements.txt
touch .gitignore
```

#### requirements.txt å†…å®¹
```txt
pandas==1.5.3
numpy==1.24.3
scikit-learn==1.3.0
matplotlib==3.7.1
seaborn==0.12.2
torch==2.0.1
torch-geometric==2.3.1
tqdm==4.65.0
jupyter==1.0.0
wandb==0.15.3
```

---

### 1.2 æ•°æ®æ¢ç´¢

#### åˆ›å»ºæ•°æ®åŠ è½½è„šæœ¬

**æ–‡ä»¶**: `src/data_processing/load_data.py`

```python
"""
æ•°æ®åŠ è½½æ¨¡å—
"""
import pandas as pd
import numpy as np
from pathlib import Path

class DataLoader:
    """åŸå§‹æ•°æ®åŠ è½½å™¨"""
    
    def __init__(self, data_path: str):
        """
        Args:
            data_path: åŸå§‹CSVæ–‡ä»¶è·¯å¾„
        """
        self.data_path = Path(data_path)
        self.df = None
        
    def load(self, encoding='gbk', skiprows=1):
        """
        åŠ è½½CSVæ•°æ®
        
        Args:
            encoding: æ–‡ä»¶ç¼–ç 
            skiprows: è·³è¿‡çš„è¡Œæ•°ï¼ˆé€šå¸¸è·³è¿‡é‡å¤è¡¨å¤´ï¼‰
            
        Returns:
            pd.DataFrame: åŠ è½½çš„æ•°æ®æ¡†
        """
        print(f"ğŸ”„ æ­£åœ¨åŠ è½½æ•°æ®: {self.data_path}")
        
        try:
            self.df = pd.read_csv(self.data_path, encoding=encoding, skiprows=skiprows)
            print(f"âœ… æˆåŠŸåŠ è½½ {len(self.df)} æ¡æ ·æœ¬, {len(self.df.columns)} ä¸ªå­—æ®µ")
            return self.df
        except Exception as e:
            print(f"âŒ åŠ è½½å¤±è´¥: {e}")
            raise
    
    def get_basic_info(self):
        """è·å–æ•°æ®é›†åŸºç¡€ä¿¡æ¯"""
        if self.df is None:
            raise ValueError("è¯·å…ˆè°ƒç”¨load()æ–¹æ³•åŠ è½½æ•°æ®")
        
        info = {
            'n_samples': len(self.df),
            'n_features': len(self.df.columns),
            'memory_usage': f"{self.df.memory_usage(deep=True).sum() / 1024**2:.2f} MB",
            'columns': list(self.df.columns),
            'dtypes': self.df.dtypes.value_counts().to_dict()
        }
        
        return info
    
    def get_missing_stats(self):
        """è·å–ç¼ºå¤±å€¼ç»Ÿè®¡"""
        if self.df is None:
            raise ValueError("è¯·å…ˆè°ƒç”¨load()æ–¹æ³•åŠ è½½æ•°æ®")
        
        missing = self.df.isnull().sum()
        missing_pct = (missing / len(self.df) * 100).round(2)
        
        stats = pd.DataFrame({
            'missing_count': missing,
            'missing_pct': missing_pct
        })
        
        return stats[stats['missing_count'] > 0].sort_values('missing_count', ascending=False)
    
    def get_column_info(self, column_name: str):
        """è·å–ç‰¹å®šåˆ—çš„è¯¦ç»†ä¿¡æ¯"""
        if self.df is None:
            raise ValueError("è¯·å…ˆè°ƒç”¨load()æ–¹æ³•åŠ è½½æ•°æ®")
        
        col = self.df[column_name]
        
        info = {
            'dtype': str(col.dtype),
            'n_unique': col.nunique(),
            'n_missing': col.isnull().sum(),
            'missing_pct': f"{col.isnull().sum() / len(col) * 100:.2f}%"
        }
        
        # è·å–å€¼åˆ†å¸ƒ
        if col.nunique() < 50:
            info['value_counts'] = col.value_counts().to_dict()
        else:
            info['sample_values'] = col.dropna().head(5).tolist()
        
        # æ•°å€¼å‹ç»Ÿè®¡
        if pd.api.types.is_numeric_dtype(col):
            info['statistics'] = {
                'mean': col.mean(),
                'std': col.std(),
                'min': col.min(),
                'max': col.max(),
                'median': col.median()
            }
        
        return info


def main():
    """æ¼”ç¤ºæ•°æ®åŠ è½½"""
    # åŠ è½½æ•°æ®
    loader = DataLoader('data/raw/originaldata.csv')
    df = loader.load()
    
    # åŸºç¡€ä¿¡æ¯
    print("\n" + "="*80)
    print("åŸºç¡€ä¿¡æ¯")
    print("="*80)
    info = loader.get_basic_info()
    for key, value in info.items():
        if key != 'columns':
            print(f"{key}: {value}")
    
    # ç¼ºå¤±å€¼ç»Ÿè®¡
    print("\n" + "="*80)
    print("ç¼ºå¤±å€¼ç»Ÿè®¡")
    print("="*80)
    missing = loader.get_missing_stats()
    if len(missing) > 0:
        print(missing)
    else:
        print("âœ… æ— ç¼ºå¤±å€¼ï¼")
    
    # æŸ¥çœ‹å…³é”®åˆ—
    print("\n" + "="*80)
    print("å…³é”®åˆ—ä¿¡æ¯")
    print("="*80)
    
    key_columns = ['Q30', 'Q3', 'Q4']
    for col in key_columns:
        print(f"\nã€{col}ã€‘")
        col_info = loader.get_column_info(col)
        for k, v in col_info.items():
            print(f"  {k}: {v}")


if __name__ == '__main__':
    main()
```

#### åˆ›å»ºæ•°æ®æ¢ç´¢ç¬”è®°æœ¬

**æ–‡ä»¶**: `notebooks/01_data_exploration.ipynb`

```python
# åœ¨Jupyter Notebookä¸­è¿è¡Œ

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from pathlib import Path
import sys

# æ·»åŠ srcåˆ°è·¯å¾„
sys.path.append('../src')
from data_processing.load_data import DataLoader

# è®¾ç½®æ ·å¼
sns.set_style('whitegrid')
plt.rcParams['font.sans-serif'] = ['SimHei']  # ä¸­æ–‡æ”¯æŒ
plt.rcParams['axes.unicode_minus'] = False

%matplotlib inline

# ================================
# 1. åŠ è½½æ•°æ®
# ================================
loader = DataLoader('../data/raw/originaldata.csv')
df = loader.load()

print(f"æ•°æ®å½¢çŠ¶: {df.shape}")
df.head()

# ================================
# 2. ç›®æ ‡å˜é‡åˆ†æ
# ================================
print("\n" + "="*60)
print("ç›®æ ‡å˜é‡: Q30 - æœªæ¥3ä¸ªæœˆç¦»èŒæ‰“ç®—")
print("="*60)

# åˆ†å¸ƒ
turnover_dist = df['Q30'].value_counts()
print(turnover_dist)
print(f"\næ­£æ ·æœ¬æ¯”ä¾‹: {turnover_dist.get('ä¼š', 0) / len(df) * 100:.2f}%")
print(f"ä¸å¹³è¡¡æ¯”: {turnover_dist.get('ä¸ä¼š', 0) / turnover_dist.get('ä¼š', 1):.1f}:1")

# å¯è§†åŒ–
fig, axes = plt.subplots(1, 2, figsize=(12, 4))

# æŸ±çŠ¶å›¾
turnover_dist.plot(kind='bar', ax=axes[0], color=['#2ecc71', '#e74c3c'])
axes[0].set_title('ç¦»èŒæ„æ„¿åˆ†å¸ƒ', fontsize=14, fontweight='bold')
axes[0].set_xlabel('ç¦»èŒæ‰“ç®—')
axes[0].set_ylabel('äººæ•°')
axes[0].set_xticklabels(axes[0].get_xticklabels(), rotation=0)

# é¥¼å›¾
turnover_dist.plot(kind='pie', ax=axes[1], autopct='%1.1f%%', 
                    colors=['#2ecc71', '#e74c3c'])
axes[1].set_title('ç¦»èŒæ„æ„¿æ¯”ä¾‹', fontsize=14, fontweight='bold')
axes[1].set_ylabel('')

plt.tight_layout()
plt.savefig('../outputs/figures/turnover_distribution.png', dpi=300, bbox_inches='tight')
plt.show()

# ================================
# 3. å²—ä½ç±»åˆ«åˆ†æ
# ================================
print("\n" + "="*60)
print("å²—ä½ç±»åˆ«åˆ†å¸ƒ (Q5ç³»åˆ—)")
print("="*60)

post_cols = [f'Q5_{i}' for i in range(1, 14)]
post_names = ['æ•°æ®', 'ç®—æ³•', 'åˆ†æ', 'äº§å“', 'è¿è¥', 'é”€å”®', 
              'äººåŠ›', 'è´¢åŠ¡', 'æ³•åŠ¡', 'è¡Œæ”¿', 'ç ”å‘', 'ç”Ÿäº§', 'å…¶ä»–']

post_counts = df[post_cols].sum()
post_df = pd.DataFrame({
    'å²—ä½': post_names,
    'äººæ•°': post_counts.values
}).sort_values('äººæ•°', ascending=False)

print(post_df)

# å¯è§†åŒ–
fig, ax = plt.subplots(figsize=(10, 6))
bars = ax.barh(post_df['å²—ä½'], post_df['äººæ•°'], color='steelblue')

# æ·»åŠ æ•°å€¼æ ‡ç­¾
for bar in bars:
    width = bar.get_width()
    ax.text(width, bar.get_y() + bar.get_height()/2, 
            f'{int(width)}äºº', ha='left', va='center', fontsize=10)

ax.set_xlabel('å‘˜å·¥æ•°é‡', fontsize=12)
ax.set_title('å²—ä½ç±»åˆ«åˆ†å¸ƒ', fontsize=14, fontweight='bold')
ax.invert_yaxis()
plt.tight_layout()
plt.savefig('../outputs/figures/post_distribution.png', dpi=300, bbox_inches='tight')
plt.show()

# å¤šå²—ä½ç»Ÿè®¡
multi_post = df[post_cols].sum(axis=1)
print(f"\nå¤šå²—ä½å‘˜å·¥: {(multi_post > 1).sum()} äºº ({(multi_post > 1).sum()/len(df)*100:.1f}%)")
print(f"å•ä¸€å²—ä½å‘˜å·¥: {(multi_post == 1).sum()} äºº ({(multi_post == 1).sum()/len(df)*100:.1f}%)")
print(f"æ— å²—ä½å‘˜å·¥: {(multi_post == 0).sum()} äºº")

# ================================
# 4. å…¬å¸å±æ€§åˆ†æ
# ================================
print("\n" + "="*60)
print("å…¬å¸å±æ€§åˆ†å¸ƒ")
print("="*60)

fig, axes = plt.subplots(1, 2, figsize=(14, 5))

# å…¬å¸ç±»å‹
company_type_dist = df['Q3'].value_counts()
company_type_dist.plot(kind='bar', ax=axes[0], color='coral')
axes[0].set_title('å…¬å¸ç±»å‹åˆ†å¸ƒ', fontsize=14, fontweight='bold')
axes[0].set_xlabel('å…¬å¸ç±»å‹')
axes[0].set_ylabel('äººæ•°')
axes[0].tick_params(axis='x', rotation=45)

# å…¬å¸è§„æ¨¡
company_size_dist = df['Q4'].value_counts()
size_order = ['<50', '50?99', '100?499', '500?999', '1000?4999', '5000+']
company_size_dist = company_size_dist.reindex(size_order)
company_size_dist.plot(kind='bar', ax=axes[1], color='lightblue')
axes[1].set_title('å…¬å¸è§„æ¨¡åˆ†å¸ƒ', fontsize=14, fontweight='bold')
axes[1].set_xlabel('å…¬å¸è§„æ¨¡ï¼ˆäººï¼‰')
axes[1].set_ylabel('äººæ•°')
axes[1].tick_params(axis='x', rotation=45)

plt.tight_layout()
plt.savefig('../outputs/figures/company_distribution.png', dpi=300, bbox_inches='tight')
plt.show()

# ================================
# 5. å²—ä½åå¥½ä»»åŠ¡åˆ†æ
# ================================
print("\n" + "="*60)
print("å²—ä½åå¥½ä»»åŠ¡ (Q18-Q29)")
print("="*60)

preference_cols = ['Q18', 'Q20', 'Q22', 'Q23', 'Q25', 'Q27', 'Q29']
preference_data = []

for col in preference_cols:
    dist = df[col].value_counts()
    preference_data.append({
        'ä»»åŠ¡': col,
        'å²—ä½A': dist.get('å²—ä½A', 0),
        'å²—ä½B': dist.get('å²—ä½B', 0),
        'æ€»æ•°': dist.sum()
    })

pref_df = pd.DataFrame(preference_data)
print(pref_df)

# å¯è§†åŒ–
fig, ax = plt.subplots(figsize=(10, 6))
x = np.arange(len(pref_df))
width = 0.35

bars1 = ax.bar(x - width/2, pref_df['å²—ä½A'], width, label='å²—ä½A', color='#3498db')
bars2 = ax.bar(x + width/2, pref_df['å²—ä½B'], width, label='å²—ä½B', color='#e74c3c')

ax.set_xlabel('ä»»åŠ¡', fontsize=12)
ax.set_ylabel('é€‰æ‹©äººæ•°', fontsize=12)
ax.set_title('7ä¸ªæƒ…æ™¯ä»»åŠ¡çš„å²—ä½åå¥½åˆ†å¸ƒ', fontsize=14, fontweight='bold')
ax.set_xticks(x)
ax.set_xticklabels(pref_df['ä»»åŠ¡'])
ax.legend()
ax.grid(axis='y', alpha=0.3)

plt.tight_layout()
plt.savefig('../outputs/figures/preference_tasks.png', dpi=300, bbox_inches='tight')
plt.show()

print(f"\nâœ… å…±æœ‰ {len(pref_df) * len(df)} ä¸ªåå¥½å¯¹ (7ä»»åŠ¡ Ã— {len(df)}å‘˜å·¥)")

# ================================
# 6. ç‰¹å¾ç»Ÿè®¡æ‘˜è¦
# ================================
print("\n" + "="*60)
print("æ•°å€¼å‹ç‰¹å¾ç»Ÿè®¡")
print("="*60)

numeric_cols = ['Q6', 'Q7', 'Q9', 'Q10', 'Q11', 'Q31_1']
numeric_stats = df[numeric_cols].describe()
print(numeric_stats)

# ç›¸å…³æ€§åˆ†æ
print("\n" + "="*60)
print("ç‰¹å¾ä¸ç¦»èŒæ„æ„¿çš„å…³ç³»")
print("="*60)

# è½¬æ¢ç¦»èŒæ ‡ç­¾ä¸ºæ•°å€¼
df['turnover_binary'] = (df['Q30'] == 'ä¼š').astype(int)

# è®¡ç®—ç›¸å…³æ€§
correlations = []
for col in numeric_cols:
    if col in df.columns:
        values = pd.to_numeric(df[col], errors='coerce')
        corr = values.corr(df['turnover_binary'])
        correlations.append({'ç‰¹å¾': col, 'ç›¸å…³ç³»æ•°': corr})

corr_df = pd.DataFrame(correlations).sort_values('ç›¸å…³ç³»æ•°', key=abs, ascending=False)
print(corr_df)

# ================================
# 7. ç”Ÿæˆæ•°æ®è´¨é‡æŠ¥å‘Š
# ================================
print("\n" + "="*60)
print("æ•°æ®è´¨é‡æŠ¥å‘Š")
print("="*60)

quality_report = {
    'æ€»æ ·æœ¬æ•°': len(df),
    'æ€»ç‰¹å¾æ•°': len(df.columns),
    'ç¼ºå¤±å€¼æ€»æ•°': df.isnull().sum().sum(),
    'ç¼ºå¤±å€¼æ¯”ä¾‹': f"{df.isnull().sum().sum() / df.size * 100:.2f}%",
    'é‡å¤æ ·æœ¬æ•°': df.duplicated().sum(),
    'ç¦»èŒæ ‡ç­¾å®Œæ•´æ€§': f"{df['Q30'].notna().sum() / len(df) * 100:.2f}%",
    'å²—ä½æ ‡ç­¾å®Œæ•´æ€§': f"{df[post_cols].notna().all(axis=1).sum() / len(df) * 100:.2f}%",
    'åå¥½ä»»åŠ¡å®Œæ•´æ€§': f"{df[preference_cols].notna().all(axis=1).sum() / len(df) * 100:.2f}%"
}

for key, value in quality_report.items():
    print(f"{key}: {value}")

# ä¿å­˜æŠ¥å‘Š
with open('../outputs/reports/data_quality_report.txt', 'w', encoding='utf-8') as f:
    f.write("æ•°æ®è´¨é‡æŠ¥å‘Š\n")
    f.write("="*60 + "\n")
    for key, value in quality_report.items():
        f.write(f"{key}: {value}\n")

print("\nâœ… æ•°æ®æ¢ç´¢å®Œæˆï¼æŠ¥å‘Šå·²ä¿å­˜åˆ° outputs/reports/")
```

#### è¿è¡Œæ•°æ®æ¢ç´¢

```bash
# å¤åˆ¶åŸå§‹æ•°æ®åˆ°é¡¹ç›®ç›®å½•
cp /path/to/originaldata.csv data/raw/

# è¿è¡ŒåŠ è½½è„šæœ¬
python src/data_processing/load_data.py

# å¯åŠ¨Jupyter
jupyter notebook notebooks/01_data_exploration.ipynb
```

---

## ğŸ¯ Day 3-4: ç‰¹å¾å·¥ç¨‹ä¸æ•°æ®æ¸…æ´—

### ä»»åŠ¡æ¸…å•
- [ ] å®ç°ç‰¹å¾æå–å™¨
- [ ] å¤„ç†ç±»åˆ«å˜é‡ç¼–ç 
- [ ] æ ‡å‡†åŒ–æ•°å€¼ç‰¹å¾
- [ ] éªŒè¯ç‰¹å¾è´¨é‡

### 2.1 ç‰¹å¾æå–å®ç°

**æ–‡ä»¶**: `src/features/feature_extractor.py`

```python
"""
ç‰¹å¾æå–æ¨¡å—
"""
import pandas as pd
import numpy as np
from sklearn.preprocessing import StandardScaler, LabelEncoder
from typing import Dict, List, Tuple
import pickle
from pathlib import Path


class FeatureExtractor:
    """å‘˜å·¥ç‰¹å¾æå–å™¨"""
    
    def __init__(self):
        self.scaler = StandardScaler()
        self.label_encoders = {}
        self.feature_names = []
        
    def extract_basic_features(self, df: pd.DataFrame) -> np.ndarray:
        """
        æå–åŸºç¡€å±æ€§ç‰¹å¾ (7ç»´)
        
        Args:
            df: åŸå§‹æ•°æ®æ¡†
            
        Returns:
            np.ndarray: (n_samples, 7)
        """
        print("ğŸ“Š æå–åŸºç¡€ç‰¹å¾...")
        
        features = []
        feature_names = []
        
        # Q6: æ€»å·¥é¾„ (æ•°å€¼å‹)
        q6 = pd.to_numeric(df['Q6'], errors='coerce').fillna(0).values
        features.append(q6)
        feature_names.append('tenure_total')
        
        # Q7: åœ¨å²—å¹´é™ (æ•°å€¼å‹)
        q7 = pd.to_numeric(df['Q7'], errors='coerce').fillna(0).values
        features.append(q7)
        feature_names.append('tenure_current')
        
        # Q8: æœ€è¿‘æ¢å·¥ä½œæ—¶é—´ (åºæ•°å‹)
        q8_mapping = {
            'ä»æœª': 0, '<1å¹´': 1, '1å¹´': 2, '2å¹´': 3, 
            '3å¹´': 4, '4å¹´': 5, '5å¹´+': 6
        }
        q8 = df['Q8'].map(q8_mapping).fillna(0).values
        features.append(q8)
        feature_names.append('last_job_change')
        
        # Q9: åŸ¹è®­æ—¶é•¿ (æ•°å€¼å‹)
        q9 = pd.to_numeric(df['Q9'], errors='coerce').fillna(0).values
        features.append(q9)
        feature_names.append('training_hours')
        
        # Q10: é€šå‹¤æ—¶é—´ (æ•°å€¼å‹)
        q10 = pd.to_numeric(df['Q10'], errors='coerce').fillna(0).values
        features.append(q10)
        feature_names.append('commute_minutes')
        
        # Q11: åŸå¸‚æ»¡æ„åº¦ (1-10åˆ†)
        q11 = pd.to_numeric(df['Q11'], errors='coerce').fillna(5).values
        features.append(q11)
        feature_names.append('city_satisfaction')
        
        # Q15: æœˆè–ªåŒºé—´ (åºæ•°å‹)
        q15_mapping = {
            '<5k': 0, '5?8k': 1, '8?12k': 2, 
            '12?20k': 3, '20?35k': 4, '35k+': 5
        }
        q15 = df['Q15'].map(q15_mapping).fillna(1).values
        features.append(q15)
        feature_names.append('salary_band')
        
        result = np.column_stack(features)
        print(f"  âœ… åŸºç¡€ç‰¹å¾: {result.shape}")
        
        return result, feature_names
    
    def extract_fit_features(self, df: pd.DataFrame) -> np.ndarray:
        """
        æå–äººå²—åŒ¹é…åº¦ç‰¹å¾ (5ç»´)
        Q12ç³»åˆ—: Likert 7åˆ†åˆ¶
        
        Args:
            df: åŸå§‹æ•°æ®æ¡†
            
        Returns:
            np.ndarray: (n_samples, 5)
        """
        print("ğŸ“Š æå–äººå²—åŒ¹é…åº¦ç‰¹å¾...")
        
        likert_mapping = {
            'éå¸¸ä¸åŒæ„': 1, 'ä¸åŒæ„': 2, 'ç•¥ä¸åŒæ„': 3, 'ä¸€èˆ¬': 4,
            'ç•¥åŒæ„': 5, 'åŒæ„': 6, 'éå¸¸åŒæ„': 7
        }
        
        features = []
        feature_names = []
        
        for i in range(1, 6):
            col = f'Q12_{i}'
            values = df[col].map(likert_mapping).fillna(4).values  # ç¼ºå¤±å¡«å……ä¸º"ä¸€èˆ¬"
            features.append(values)
            feature_names.append(f'fit_{i}')
        
        result = np.column_stack(features)
        print(f"  âœ… äººå²—åŒ¹é…ç‰¹å¾: {result.shape}")
        
        return result, feature_names
    
    def extract_skill_features(self, df: pd.DataFrame) -> np.ndarray:
        """
        æå–æŠ€èƒ½ç‰¹å¾ (30ç»´: 15é¢‘ç‡ + 15ç†Ÿç»ƒåº¦)
        Q13ç³»åˆ—: ä½¿ç”¨é¢‘ç‡ (1-5)
        Q14ç³»åˆ—: ç†Ÿç»ƒåº¦ (1-5)
        
        Args:
            df: åŸå§‹æ•°æ®æ¡†
            
        Returns:
            np.ndarray: (n_samples, 30)
        """
        print("ğŸ“Š æå–æŠ€èƒ½ç‰¹å¾...")
        
        # é¢‘ç‡æ˜ å°„
        freq_mapping = {
            'å‡ ä¹ä¸ç”¨': 1, 'å¶å°”ä½¿ç”¨': 2, 'ä¸€èˆ¬': 3, 
            'è¾ƒé¢‘ç¹': 4, 'æ¯å¤©é«˜å¼ºåº¦': 5
        }
        
        # ç†Ÿç»ƒåº¦æ˜ å°„
        prof_mapping = {
            'åˆå­¦': 1, 'å…¥é—¨': 2, 'ç†Ÿç»ƒ': 3, 'ç²¾é€š': 4, 'ä¸“å®¶': 5
        }
        
        features = []
        feature_names = []
        
        # Q13: ä½¿ç”¨é¢‘ç‡
        for i in range(1, 16):
            col = f'Q13_{i}'
            values = df[col].map(freq_mapping).fillna(3).values
            features.append(values)
            feature_names.append(f'skill_freq_{i}')
        
        # Q14: ç†Ÿç»ƒåº¦
        for i in range(1, 16):
            col = f'Q14_{i}'
            values = df[col].map(prof_mapping).fillna(3).values
            features.append(values)
            feature_names.append(f'skill_prof_{i}')
        
        result = np.column_stack(features)
        print(f"  âœ… æŠ€èƒ½ç‰¹å¾: {result.shape}")
        
        return result, feature_names
    
    def extract_economic_features(self, df: pd.DataFrame) -> np.ndarray:
        """
        æå–ç»æµæŸå¤±æ„ŸçŸ¥ç‰¹å¾ (5ç»´)
        Q16ç³»åˆ—: Likert 7åˆ†åˆ¶
        
        Args:
            df: åŸå§‹æ•°æ®æ¡†
            
        Returns:
            np.ndarray: (n_samples, 5)
        """
        print("ğŸ“Š æå–ç»æµæŸå¤±ç‰¹å¾...")
        
        likert_mapping = {
            'éå¸¸ä¸åŒæ„': 1, 'ä¸åŒæ„': 2, 'ç•¥ä¸åŒæ„': 3, 'ä¸€èˆ¬': 4,
            'ç•¥åŒæ„': 5, 'åŒæ„': 6, 'éå¸¸åŒæ„': 7
        }
        
        features = []
        feature_names = []
        
        for i in range(1, 6):
            col = f'Q16_{i}'
            values = df[col].map(likert_mapping).fillna(4).values
            features.append(values)
            feature_names.append(f'economic_{i}')
        
        result = np.column_stack(features)
        print(f"  âœ… ç»æµæŸå¤±ç‰¹å¾: {result.shape}")
        
        return result, feature_names
    
    def extract_all_features(self, df: pd.DataFrame, 
                            fit=True) -> Tuple[np.ndarray, List[str]]:
        """
        æå–æ‰€æœ‰ç‰¹å¾
        
        Args:
            df: åŸå§‹æ•°æ®æ¡†
            fit: æ˜¯å¦æ‹Ÿåˆæ ‡å‡†åŒ–å™¨ï¼ˆè®­ç»ƒé›†ä¸ºTrueï¼Œæµ‹è¯•é›†ä¸ºFalseï¼‰
            
        Returns:
            features: (n_samples, 47) ç‰¹å¾çŸ©é˜µ
            feature_names: ç‰¹å¾åç§°åˆ—è¡¨
        """
        print("\n" + "="*60)
        print("ç‰¹å¾æå–ç®¡é“")
        print("="*60)
        
        # æå–å„ç»„ç‰¹å¾
        basic_feats, basic_names = self.extract_basic_features(df)
        fit_feats, fit_names = self.extract_fit_features(df)
        skill_feats, skill_names = self.extract_skill_features(df)
        econ_feats, econ_names = self.extract_economic_features(df)
        
        # åˆå¹¶ç‰¹å¾
        all_features = np.column_stack([
            basic_feats, fit_feats, skill_feats, econ_feats
        ])
        
        all_names = basic_names + fit_names + skill_names + econ_names
        
        print(f"\nğŸ“Š ç‰¹å¾åˆå¹¶å®Œæˆ: {all_features.shape}")
        print(f"   - åŸºç¡€ç‰¹å¾: {len(basic_names)}ç»´")
        print(f"   - äººå²—åŒ¹é…: {len(fit_names)}ç»´")
        print(f"   - æŠ€èƒ½ç‰¹å¾: {len(skill_names)}ç»´")
        print(f"   - ç»æµæŸå¤±: {len(econ_names)}ç»´")
        print(f"   - æ€»è®¡: {len(all_names)}ç»´")
        
        # æ ‡å‡†åŒ–
        if fit:
            print("\nğŸ”§ æ‹Ÿåˆæ ‡å‡†åŒ–å™¨...")
            features_scaled = self.scaler.fit_transform(all_features)
        else:
            print("\nğŸ”§ åº”ç”¨æ ‡å‡†åŒ–å™¨...")
            features_scaled = self.scaler.transform(all_features)
        
        self.feature_names = all_names
        
        print(f"âœ… ç‰¹å¾æå–å®Œæˆï¼")
        print(f"   - ç‰¹å¾å½¢çŠ¶: {features_scaled.shape}")
        print(f"   - ç‰¹å¾å‡å€¼: {features_scaled.mean():.4f}")
        print(f"   - ç‰¹å¾æ ‡å‡†å·®: {features_scaled.std():.4f}")
        
        return features_scaled, all_names
    
    def save(self, path: str):
        """ä¿å­˜ç‰¹å¾æå–å™¨"""
        Path(path).parent.mkdir(parents=True, exist_ok=True)
        
        with open(path, 'wb') as f:
            pickle.dump({
                'scaler': self.scaler,
                'feature_names': self.feature_names,
                'label_encoders': self.label_encoders
            }, f)
        
        print(f"âœ… ç‰¹å¾æå–å™¨å·²ä¿å­˜: {path}")
    
    def load(self, path: str):
        """åŠ è½½ç‰¹å¾æå–å™¨"""
        with open(path, 'rb') as f:
            data = pickle.load(f)
            self.scaler = data['scaler']
            self.feature_names = data['feature_names']
            self.label_encoders = data['label_encoders']
        
        print(f"âœ… ç‰¹å¾æå–å™¨å·²åŠ è½½: {path}")


def main():
    """æ¼”ç¤ºç‰¹å¾æå–"""
    from data_processing.load_data import DataLoader
    
    # åŠ è½½æ•°æ®
    loader = DataLoader('data/raw/originaldata.csv')
    df = loader.load()
    
    # æå–ç‰¹å¾
    extractor = FeatureExtractor()
    features, feature_names = extractor.extract_all_features(df, fit=True)
    
    # ä¿å­˜ç‰¹å¾
    np.save('data/processed/employee_features.npy', features)
    with open('data/processed/feature_names.txt', 'w') as f:
        f.write('\n'.join(feature_names))
    
    # ä¿å­˜æå–å™¨
    extractor.save('models/feature_extractor.pkl')
    
    print("\nâœ… ç‰¹å¾å·²ä¿å­˜åˆ° data/processed/")


if __name__ == '__main__':
    main()
```

### 2.2 æ ‡ç­¾æå–

**æ–‡ä»¶**: `src/data_processing/label_extractor.py`

```python
"""
æ ‡ç­¾æå–æ¨¡å—
"""
import pandas as pd
import numpy as np
from typing import Tuple


class LabelExtractor:
    """ç¦»èŒæ ‡ç­¾æå–å™¨"""
    
    @staticmethod
    def extract_turnover_labels(df: pd.DataFrame) -> Tuple[np.ndarray, np.ndarray]:
        """
        æå–ç¦»èŒæ ‡ç­¾
        
        Args:
            df: åŸå§‹æ•°æ®æ¡†
            
        Returns:
            y_binary: äºŒåˆ†ç±»æ ‡ç­¾ (0/1)
            y_prob: ç¦»èŒæ¦‚ç‡ (0-1è¿ç»­å€¼)
        """
        print("\n" + "="*60)
        print("æ ‡ç­¾æå–")
        print("="*60)
        
        # Q30: 3ä¸ªæœˆç¦»èŒæ‰“ç®— (äºŒåˆ†ç±»)
        print("ğŸ“Œ æå–Q30 - 3ä¸ªæœˆç¦»èŒæ‰“ç®—...")
        y_binary = (df['Q30'] == 'ä¼š').astype(int).values
        
        pos_count = y_binary.sum()
        neg_count = len(y_binary) - pos_count
        
        print(f"   æ­£æ ·æœ¬: {pos_count} ({pos_count/len(y_binary)*100:.1f}%)")
        print(f"   è´Ÿæ ·æœ¬: {neg_count} ({neg_count/len(y_binary)*100:.1f}%)")
        print(f"   ä¸å¹³è¡¡æ¯”: {neg_count/pos_count:.1f}:1")
        
        # Q31_1: 6-12ä¸ªæœˆç¦»èŒå¯èƒ½æ€§ (å›å½’)
        print("\nğŸ“Œ æå–Q31_1 - 6-12ä¸ªæœˆç¦»èŒå¯èƒ½æ€§...")
        y_prob = pd.to_numeric(df['Q31_1'], errors='coerce').fillna(0).values / 100.0
        
        print(f"   èŒƒå›´: {y_prob.min():.2f} - {y_prob.max():.2f}")
        print(f"   å‡å€¼: {y_prob.mean():.2f}")
        print(f"   ä¸­ä½æ•°: {np.median(y_prob):.2f}")
        
        print("\nâœ… æ ‡ç­¾æå–å®Œæˆ!")
        
        return y_binary, y_prob
    
    @staticmethod
    def extract_preference_pairs(df: pd.DataFrame) -> list:
        """
        æå–å²—ä½åå¥½å¯¹
        
        Args:
            df: åŸå§‹æ•°æ®æ¡†
            
        Returns:
            preference_pairs: [(emp_idx, task_idx, choice), ...]
        """
        print("\n" + "="*60)
        print("å²—ä½åå¥½å¯¹æå–")
        print("="*60)
        
        task_cols = ['Q18', 'Q20', 'Q22', 'Q23', 'Q25', 'Q27', 'Q29']
        preference_pairs = []
        
        for task_idx, q_col in enumerate(task_cols):
            for emp_idx in range(len(df)):
                choice = df.iloc[emp_idx][q_col]
                
                # ç¼–ç : å²—ä½A=0, å²—ä½B=1
                choice_binary = 0 if choice == 'å²—ä½A' else 1
                
                preference_pairs.append({
                    'employee_idx': emp_idx,
                    'task_idx': task_idx,
                    'choice': choice_binary,
                    'post_A_id': task_idx * 2,      # è™šæ‹Ÿå²—ä½ID
                    'post_B_id': task_idx * 2 + 1
                })
        
        print(f"âœ… å…±æå– {len(preference_pairs)} ä¸ªåå¥½å¯¹")
        print(f"   - 7ä¸ªä»»åŠ¡ Ã— {len(df)} å‘˜å·¥ = {7 * len(df)} å¯¹")
        
        return preference_pairs


def main():
    """æ¼”ç¤ºæ ‡ç­¾æå–"""
    import sys
    sys.path.append('..')
    from data_processing.load_data import DataLoader
    
    # åŠ è½½æ•°æ®
    loader = DataLoader('../data/raw/originaldata.csv')
    df = loader.load()
    
    # æå–ç¦»èŒæ ‡ç­¾
    extractor = LabelExtractor()
    y_binary, y_prob = extractor.extract_turnover_labels(df)
    
    # ä¿å­˜æ ‡ç­¾
    np.save('../data/processed/y_turnover_binary.npy', y_binary)
    np.save('../data/processed/y_turnover_prob.npy', y_prob)
    
    # æå–åå¥½å¯¹
    preference_pairs = extractor.extract_preference_pairs(df)
    pd.DataFrame(preference_pairs).to_csv(
        '../data/processed/preference_pairs.csv', index=False
    )
    
    print("\nâœ… æ ‡ç­¾å·²ä¿å­˜åˆ° data/processed/")


if __name__ == '__main__':
    main()
```

### è¿è¡Œç‰¹å¾æå–

```bash
# æå–ç‰¹å¾
python src/features/feature_extractor.py

# æå–æ ‡ç­¾
python src/data_processing/label_extractor.py

# éªŒè¯è¾“å‡º
ls -lh data/processed/
```

**é¢„æœŸè¾“å‡ºæ–‡ä»¶**:
```
data/processed/
â”œâ”€â”€ employee_features.npy        # (500, 47) ç‰¹å¾çŸ©é˜µ
â”œâ”€â”€ feature_names.txt            # 47ä¸ªç‰¹å¾åç§°
â”œâ”€â”€ y_turnover_binary.npy        # (500,) ç¦»èŒäºŒåˆ†ç±»æ ‡ç­¾
â”œâ”€â”€ y_turnover_prob.npy          # (500,) ç¦»èŒæ¦‚ç‡
â””â”€â”€ preference_pairs.csv         # 3500è¡Œåå¥½å¯¹æ•°æ®
```

---

## ğŸ¯ Day 5-6: è¾¹æ„å»ºä¸å›¾æ•°æ®å‡†å¤‡

### ä»»åŠ¡æ¸…å•
- [ ] æ„å»ºå‘˜å·¥-å²—ä½è¾¹
- [ ] æ„å»ºå‘˜å·¥-å…¬å¸å±æ€§è¾¹
- [ ] æ„å»ºåå¥½è¾¹ï¼ˆå¯é€‰ï¼‰
- [ ] éªŒè¯å›¾ç»“æ„

### 3.1 è¾¹æ„å»ºå™¨

**æ–‡ä»¶**: `src/data_processing/edge_builder.py`

```python
"""
å›¾è¾¹æ„å»ºæ¨¡å—
"""
import pandas as pd
import numpy as np
import torch
from typing import Dict, Tuple
from pathlib import Path


class EdgeBuilder:
    """å¼‚æ„å›¾è¾¹æ„å»ºå™¨"""
    
    def __init__(self):
        self.edge_index_dict = {}
        
    def build_employee_post_edges(self, df: pd.DataFrame) -> torch.LongTensor:
        """
        æ„å»ºå‘˜å·¥-å²—ä½ç±»åˆ«è¾¹
        Employee â†’ PostType
        
        Args:
            df: åŸå§‹æ•°æ®æ¡†
            
        Returns:
            edge_index: [2, num_edges]
        """
        print("\nğŸ”— æ„å»º Employee â†’ PostType è¾¹...")
        
        edges = []
        post_cols = [f'Q5_{i}' for i in range(1, 14)]
        
        for emp_idx in range(len(df)):
            for post_idx, col in enumerate(post_cols):
                if df.iloc[emp_idx][col] == 1:
                    edges.append([emp_idx, post_idx])
        
        edge_index = torch.LongTensor(edges).t()
        
        print(f"   âœ… è¾¹æ•°: {edge_index.shape[1]}")
        print(f"   âœ… å¹³å‡æ¯å‘˜å·¥è¿æ¥: {edge_index.shape[1] / len(df):.2f} ä¸ªå²—ä½")
        
        return edge_index
    
    def build_employee_company_edges(self, 
                                      df: pd.DataFrame
                                      ) -> Tuple[torch.LongTensor, torch.LongTensor]:
        """
        æ„å»ºå‘˜å·¥-å…¬å¸å±æ€§è¾¹
        Employee â†’ CompanySize
        Employee â†’ CompanyType
        
        Args:
            df: åŸå§‹æ•°æ®æ¡†
            
        Returns:
            size_edges: [2, 500]
            type_edges: [2, 500]
        """
        print("\nğŸ”— æ„å»º Employee â†’ Company è¾¹...")
        
        # å…¬å¸è§„æ¨¡æ˜ å°„
        size_mapping = {
            '<50': 0, '50?99': 1, '100?499': 2,
            '500?999': 3, '1000?4999': 4, '5000+': 5
        }
        
        # å…¬å¸ç±»å‹æ˜ å°„
        type_mapping = {
            'æ°‘è¥': 0, 'å›½ä¼': 1, 'å¤–èµ„': 2,
            'äº‹ä¸šå•ä½': 3, 'åˆèµ„': 4, 'å…¶ä»–': 5
        }
        
        # æ„å»ºè§„æ¨¡è¾¹
        size_edges = []
        for emp_idx in range(len(df)):
            size_id = size_mapping[df.iloc[emp_idx]['Q4']]
            size_edges.append([emp_idx, size_id])
        
        size_edge_index = torch.LongTensor(size_edges).t()
        print(f"   âœ… Employee â†’ CompanySize: {size_edge_index.shape[1]} æ¡è¾¹")
        
        # æ„å»ºç±»å‹è¾¹
        type_edges = []
        for emp_idx in range(len(df)):
            type_id = type_mapping[df.iloc[emp_idx]['Q3']]
            type_edges.append([emp_idx, type_id])
        
        type_edge_index = torch.LongTensor(type_edges).t()
        print(f"   âœ… Employee â†’ CompanyType: {type_edge_index.shape[1]} æ¡è¾¹")
        
        return size_edge_index, type_edge_index
    
    def build_preference_edges(self, 
                                preference_pairs: pd.DataFrame
                                ) -> Tuple[torch.LongTensor, torch.LongTensor]:
        """
        æ„å»ºå‘˜å·¥-è™šæ‹Ÿå²—ä½åå¥½è¾¹
        Employee â†’ HypotheticalPost (prefer / disprefer)
        
        Args:
            preference_pairs: åå¥½å¯¹æ•°æ®æ¡†
            
        Returns:
            prefer_edges: [2, 3500]
            disprefer_edges: [2, 3500]
        """
        print("\nğŸ”— æ„å»º Employee â†’ HypotheticalPost åå¥½è¾¹...")
        
        prefer_edges = []
        disprefer_edges = []
        
        for _, row in preference_pairs.iterrows():
            emp_idx = row['employee_idx']
            post_A_id = row['post_A_id']
            post_B_id = row['post_B_id']
            choice = row['choice']
            
            if choice == 0:  # é€‰æ‹©å²—ä½A
                prefer_edges.append([emp_idx, post_A_id])
                disprefer_edges.append([emp_idx, post_B_id])
            else:  # é€‰æ‹©å²—ä½B
                prefer_edges.append([emp_idx, post_B_id])
                disprefer_edges.append([emp_idx, post_A_id])
        
        prefer_edge_index = torch.LongTensor(prefer_edges).t()
        disprefer_edge_index = torch.LongTensor(disprefer_edges).t()
        
        print(f"   âœ… Prefer è¾¹: {prefer_edge_index.shape[1]}")
        print(f"   âœ… Disprefer è¾¹: {disprefer_edge_index.shape[1]}")
        
        return prefer_edge_index, disprefer_edge_index
    
    def build_all_edges(self, 
                        df: pd.DataFrame,
                        preference_pairs: pd.DataFrame = None,
                        use_preference: bool = True) -> Dict:
        """
        æ„å»ºæ‰€æœ‰è¾¹
        
        Args:
            df: åŸå§‹æ•°æ®æ¡†
            preference_pairs: åå¥½å¯¹æ•°æ®
            use_preference: æ˜¯å¦ä½¿ç”¨åå¥½è¾¹
            
        Returns:
            edge_index_dict: {edge_type: edge_index}
        """
        print("\n" + "="*60)
        print("å›¾è¾¹æ„å»ºç®¡é“")
        print("="*60)
        
        edge_dict = {}
        
        # 1. Employee â†’ PostType
        emp_post_edges = self.build_employee_post_edges(df)
        edge_dict[('employee', 'works_as', 'post_type')] = emp_post_edges
        
        # 2. Employee â†’ Company
        size_edges, type_edges = self.build_employee_company_edges(df)
        edge_dict[('employee', 'at_size', 'company_size')] = size_edges
        edge_dict[('employee', 'at_type', 'company_type')] = type_edges
        
        # 3. Preference edges (å¯é€‰)
        if use_preference and preference_pairs is not None:
            prefer_edges, disprefer_edges = self.build_preference_edges(preference_pairs)
            edge_dict[('employee', 'prefer', 'hypothetical_post')] = prefer_edges
            edge_dict[('employee', 'disprefer', 'hypothetical_post')] = disprefer_edges
        
        # ç»Ÿè®¡
        total_edges = sum(e.shape[1] for e in edge_dict.values())
        print(f"\nâœ… è¾¹æ„å»ºå®Œæˆï¼")
        print(f"   - è¾¹ç±»å‹æ•°: {len(edge_dict)}")
        print(f"   - æ€»è¾¹æ•°: {total_edges}")
        
        self.edge_index_dict = edge_dict
        
        return edge_dict
    
    def save(self, output_dir: str):
        """ä¿å­˜æ‰€æœ‰è¾¹"""
        output_dir = Path(output_dir)
        output_dir.mkdir(parents=True, exist_ok=True)
        
        for edge_type, edge_index in self.edge_index_dict.items():
            # è½¬æ¢è¾¹ç±»å‹åç§°ä¸ºæ–‡ä»¶å
            src, relation, dst = edge_type
            filename = f"{src}_{relation}_{dst}.pt"
            
            torch.save(edge_index, output_dir / filename)
            print(f"   âœ… å·²ä¿å­˜: {filename}")
        
        print(f"\nâœ… æ‰€æœ‰è¾¹å·²ä¿å­˜åˆ°: {output_dir}")


def main():
    """æ¼”ç¤ºè¾¹æ„å»º"""
    import sys
    sys.path.append('..')
    from data_processing.load_data import DataLoader
    
    # åŠ è½½æ•°æ®
    loader = DataLoader('../data/raw/originaldata.csv')
    df = loader.load()
    
    # åŠ è½½åå¥½å¯¹
    preference_pairs = pd.read_csv('../data/processed/preference_pairs.csv')
    
    # æ„å»ºè¾¹
    builder = EdgeBuilder()
    edge_dict = builder.build_all_edges(
        df, 
        preference_pairs=preference_pairs,
        use_preference=True
    )
    
    # ä¿å­˜
    builder.save('../data/processed/edges')
    
    # éªŒè¯
    print("\n" + "="*60)
    print("è¾¹éªŒè¯")
    print("="*60)
    for edge_type, edge_index in edge_dict.items():
        print(f"{edge_type}:")
        print(f"  å½¢çŠ¶: {edge_index.shape}")
        print(f"  æºèŠ‚ç‚¹èŒƒå›´: {edge_index[0].min()} - {edge_index[0].max()}")
        print(f"  ç›®æ ‡èŠ‚ç‚¹èŒƒå›´: {edge_index[1].min()} - {edge_index[1].max()}")
        print()


if __name__ == '__main__':
    main()
```

### è¿è¡Œè¾¹æ„å»º

```bash
# æ„å»ºè¾¹
python src/data_processing/edge_builder.py

# éªŒè¯è¾“å‡º
ls -lh data/processed/edges/
```

---

## ğŸ¯ Day 7-8: æ•°æ®é›†åˆ’åˆ†ä¸éªŒè¯

*ï¼ˆç”±äºç¯‡å¹…é™åˆ¶ï¼Œå°†åœ¨ä¸‹ä¸€éƒ¨åˆ†ç»§ç»­...ï¼‰*

æ˜¯å¦ç»§ç»­è¾“å‡ºDay 7-14çš„è¯¦ç»†å†…å®¹ï¼Ÿæˆ‘å¯ä»¥ç»§ç»­åˆ›å»º:
- Day 7-8: æ•°æ®é›†åˆ’åˆ†è„šæœ¬
- Day 9-10: MLPåŸºçº¿æ¨¡å‹å®ç°
- Day 11-14: æ¨¡å‹è®­ç»ƒä¸è¯„ä¼°ä»£ç 

æ‰€æœ‰ä»£ç éƒ½æ˜¯å¯ç›´æ¥è¿è¡Œçš„å®Œæ•´å®ç°ï¼
