"""
GCNé›†æˆå­¦ä¹ 
è®­ç»ƒå¤šä¸ªæ¨¡å‹ï¼ŒæŠ•ç¥¨æˆ–å¹³å‡é¢„æµ‹
"""

import torch
import numpy as np
import sys
sys.path.insert(0, 'src/models')

from gcn import create_gcn_model
from trainer2 import GCNTrainer
from evaluator2 import GCNEvaluator
from sklearn.metrics import f1_score, precision_score, recall_score, confusion_matrix
import json


def train_single_model(data, model_id, config):
    """è®­ç»ƒå•ä¸ªæ¨¡å‹"""
    print(f"\nè®­ç»ƒæ¨¡å‹ #{model_id}...")
    
    model = create_gcn_model(
        in_channels=data.num_node_features,
        architecture=config['architecture'],
        dropout=config['dropout']
    )
    
    # è®¡ç®—pos_weight
    num_pos = data.y[data.train_mask].sum().item()
    num_neg = data.train_mask.sum().item() - num_pos
    pos_weight = num_neg / num_pos
    
    trainer = GCNTrainer(
        model=model,
        data=data,
        device='cpu',
        lr=config['lr'],
        weight_decay=config['weight_decay'],
        pos_weight=pos_weight
    )
    
    history = trainer.train(
        epochs=config['epochs'],
        early_stopping_patience=config['patience'],
        save_dir=f"outputs/models_ensemble/model_{model_id}",
        verbose=False
    )
    
    # è¯„ä¼°
    test_loss, test_acc, test_f1 = trainer.evaluate(data.test_mask)
    
    print(f"   âœ“ Test F1: {test_f1:.4f}, Epoch: {trainer.best_epoch+1}")
    
    return model, {
        'f1': test_f1,
        'acc': test_acc,
        'best_epoch': trainer.best_epoch + 1
    }


def ensemble_predict(models, data, method='vote'):
    """
    é›†æˆé¢„æµ‹
    
    Args:
        models: æ¨¡å‹åˆ—è¡¨
        data: æ•°æ®
        method: 'vote' (æŠ•ç¥¨) æˆ– 'average' (å¹³å‡æ¦‚ç‡)
    """
    all_probs = []
    
    for model in models:
        model.eval()
        with torch.no_grad():
            out = model(
                data.x,
                data.edge_index,
                data.edge_attr if hasattr(data, 'edge_attr') else None
            )
            probs = torch.sigmoid(out[data.test_mask]).squeeze().cpu().numpy()
            all_probs.append(probs)
    
    all_probs = np.array(all_probs)  # (n_models, n_samples)
    
    if method == 'vote':
        # æŠ•ç¥¨æ³•ï¼šæ¯ä¸ªæ¨¡å‹é¢„æµ‹0æˆ–1ï¼Œå¤šæ•°æŠ•ç¥¨
        votes = (all_probs > 0.5).astype(int)
        preds = (votes.sum(axis=0) > len(models) / 2).astype(int)
        avg_probs = all_probs.mean(axis=0)
    else:  # average
        # å¹³å‡æ³•ï¼šå¹³å‡æ‰€æœ‰æ¨¡å‹çš„æ¦‚ç‡
        avg_probs = all_probs.mean(axis=0)
        preds = (avg_probs > 0.5).astype(int)
    
    labels = data.y[data.test_mask].cpu().numpy()
    
    return preds, avg_probs, labels


print("\n" + "="*70)
print("ğŸ¯ GCNé›†æˆå­¦ä¹ ")
print("="*70)

# é…ç½®
config = {
    'architecture': 'default',
    'dropout': 0.5,
    'lr': 0.01,
    'weight_decay': 5e-4,
    'epochs': 200,
    'patience': 20
}

n_models = 5  # è®­ç»ƒ5ä¸ªæ¨¡å‹

print(f"\né…ç½®:")
for k, v in config.items():
    print(f"   {k}: {v}")
print(f"   n_models: {n_models}")

# åŠ è½½æ•°æ®
print(f"\nåŠ è½½æ•°æ®...")
data = torch.load('data/processed/homo_graph.pt')
print(f"   âœ“ èŠ‚ç‚¹: {data.num_nodes}")

# è®­ç»ƒå¤šä¸ªæ¨¡å‹
print(f"\n{'='*70}")
print(f"è®­ç»ƒ {n_models} ä¸ªæ¨¡å‹")
print(f"{'='*70}")

models = []
individual_results = []

for i in range(n_models):
    model, result = train_single_model(data, i+1, config)
    models.append(model)
    individual_results.append(result)

# å•ä¸ªæ¨¡å‹æ€§èƒ½
print(f"\nå•ä¸ªæ¨¡å‹æ€§èƒ½:")
print(f"{'æ¨¡å‹':<10} {'F1':<10} {'Accuracy':<12} {'Best Epoch'}")
print("="*50)
for i, result in enumerate(individual_results, 1):
    print(f"Model {i:<4} {result['f1']:<10.4f} {result['acc']:<12.4f} {result['best_epoch']}")

print("="*50)
print(f"å¹³å‡:      {np.mean([r['f1'] for r in individual_results]):.4f}")
print(f"æœ€å¥½:      {max([r['f1'] for r in individual_results]):.4f}")
print(f"æœ€å·®:      {min([r['f1'] for r in individual_results]):.4f}")

# é›†æˆé¢„æµ‹
print(f"\n{'='*70}")
print("é›†æˆé¢„æµ‹")
print(f"{'='*70}")

methods = ['vote', 'average']
ensemble_results = {}

for method in methods:
    print(f"\næ–¹æ³•: {method.upper()}")
    preds, probs, labels = ensemble_predict(models, data, method=method)
    
    f1 = f1_score(labels, preds)
    recall = recall_score(labels, preds)
    precision = precision_score(labels, preds)
    
    print(f"   F1-Score:  {f1:.4f}")
    print(f"   Recall:    {recall:.4f}")
    print(f"   Precision: {precision:.4f}")
    
    cm = confusion_matrix(labels, preds)
    print(f"\n   æ··æ·†çŸ©é˜µ:")
    print(f"                é¢„æµ‹ä¸ç¦»èŒ    é¢„æµ‹ç¦»èŒ")
    print(f"   å®é™…ä¸ç¦»èŒ    {cm[0,0]:>8}      {cm[0,1]:>8}")
    print(f"   å®é™…ç¦»èŒ      {cm[1,0]:>8}      {cm[1,1]:>8}")
    
    ensemble_results[method] = {
        'f1': float(f1),
        'recall': float(recall),
        'precision': float(precision)
    }

# å¯¹æ¯”æ€»ç»“
print(f"\n{'='*70}")
print("ğŸ“Š é›†æˆ vs å•æ¨¡å‹å¯¹æ¯”")
print(f"{'='*70}")

print(f"\n{'æ–¹æ³•':<20} {'F1':<10} {'Recall':<12} {'Precision':<12} {'æå‡'}")
print("="*70)

# æœ€å¥½çš„å•æ¨¡å‹
best_single_f1 = max([r['f1'] for r in individual_results])
print(f"{'æœ€å¥½å•æ¨¡å‹':<20} {best_single_f1:<10.4f} {'':12} {'':12} {'åŸºçº¿'}")

# é›†æˆæ–¹æ³•
for method, result in ensemble_results.items():
    improvement = (result['f1'] - best_single_f1) / best_single_f1 * 100
    marker = "â­" if result['f1'] > best_single_f1 else ""
    print(f"{'é›†æˆ-' + method:<20} {result['f1']:<10.4f} {result['recall']:<12.4f} "
          f"{result['precision']:<12.4f} {improvement:+.1f}% {marker}")

print("="*70)

# ä¿å­˜ç»“æœ
results_summary = {
    'config': config,
    'n_models': n_models,
    'individual_results': individual_results,
    'ensemble_results': ensemble_results,
    'best_single_f1': float(best_single_f1),
    'best_ensemble_f1': float(max([r['f1'] for r in ensemble_results.values()]))
}

with open('outputs/ensemble_results.json', 'w') as f:
    json.dump(results_summary, f, indent=2)

print(f"\nâœ“ ç»“æœå·²ä¿å­˜: outputs/ensemble_results.json")

# æ¨è
best_ensemble_method = max(ensemble_results, key=lambda k: ensemble_results[k]['f1'])
best_ensemble_f1 = ensemble_results[best_ensemble_method]['f1']

print(f"\nğŸ† æ¨èæ–¹æ³•: é›†æˆ-{best_ensemble_method.upper()}")
print(f"   F1-Score: {best_ensemble_f1:.4f}")
print(f"   æå‡: {(best_ensemble_f1 - best_single_f1) / best_single_f1 * 100:+.1f}%")

print("\n" + "="*70)
print("âœ… é›†æˆå­¦ä¹ å®Œæˆï¼")
print("="*70)
