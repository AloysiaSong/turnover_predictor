"""
XGBoost Baseline è®­ç»ƒè„šæœ¬
========================
ç›®æ ‡: ä½œä¸ºGCNçš„å¼ºbaselineå¯¹æ¯”
ç‰¹ç‚¹:
1. ä½¿ç”¨ç›¸åŒçš„47ç»´ç‰¹å¾
2. ç½‘æ ¼æœç´¢æœ€ä¼˜è¶…å‚æ•°
3. ä½¿ç”¨AUPRä½œä¸ºè¯„ä¼°æŒ‡æ ‡
4. æ—©åœæœºåˆ¶
5. ä¸GCNç›¸åŒçš„æ•°æ®åˆ’åˆ†
"""

import numpy as np
import pandas as pd
import xgboost as xgb
from sklearn.metrics import (
    average_precision_score,
    roc_auc_score,
    f1_score,
    precision_score,
    recall_score,
    confusion_matrix,
    precision_recall_curve,
    roc_curve
)
import json
import argparse
from pathlib import Path
import matplotlib.pyplot as plt
import seaborn as sns
from datetime import datetime
import torch
import sys

# è®¾ç½®ä¸­æ–‡å­—ä½“
plt.rcParams['font.sans-serif'] = ['Arial Unicode MS', 'SimHei', 'DejaVu Sans']
plt.rcParams['axes.unicode_minus'] = False


def set_seed(seed: int):
    """å›ºå®šéšæœºç§å­"""
    np.random.seed(seed)


def load_data(data_path: str):
    """
    åŠ è½½å¤„ç†å¥½çš„å›¾æ•°æ®
    è¿”å›ç‰¹å¾çŸ©é˜µã€æ ‡ç­¾å’Œmask
    """
    print("\n1. åŠ è½½æ•°æ®...")
    data = torch.load(data_path)
    
    # æå–numpyæ•°ç»„
    X = data.x.numpy()  # (500, 47)
    y = data.y.numpy()  # (500,)
    
    train_mask = data.train_mask.numpy()
    val_mask = data.val_mask.numpy()
    test_mask = data.test_mask.numpy()
    
    print(f"   âœ“ ç‰¹å¾ç»´åº¦: {X.shape}")
    print(f"   âœ“ è®­ç»ƒé›†: {train_mask.sum()} ({train_mask.sum()/len(y)*100:.1f}%)")
    print(f"   âœ“ éªŒè¯é›†: {val_mask.sum()} ({val_mask.sum()/len(y)*100:.1f}%)")
    print(f"   âœ“ æµ‹è¯•é›†: {test_mask.sum()} ({test_mask.sum()/len(y)*100:.1f}%)")
    print(f"   âœ“ ç¦»èŒç‡: {y.sum()}/{len(y)} ({y.sum()/len(y)*100:.1f}%)")
    
    return X, y, train_mask, val_mask, test_mask


def compute_metrics(y_true, y_pred_proba, threshold=0.5):
    """è®¡ç®—æ‰€æœ‰è¯„ä¼°æŒ‡æ ‡"""
    y_pred = (y_pred_proba >= threshold).astype(int)
    
    metrics = {
        'aupr': average_precision_score(y_true, y_pred_proba),
        'auroc': roc_auc_score(y_true, y_pred_proba),
        'f1': f1_score(y_true, y_pred, zero_division=0),
        'precision': precision_score(y_true, y_pred, zero_division=0),
        'recall': recall_score(y_true, y_pred, zero_division=0),
        'threshold': threshold
    }
    
    return metrics


def find_best_threshold(y_true, y_pred_proba):
    """åœ¨éªŒè¯é›†ä¸Šå¯»æ‰¾æœ€ä¼˜é˜ˆå€¼"""
    best_threshold = 0.5
    best_f1 = 0.0
    
    thresholds = np.arange(0.1, 0.9, 0.02)
    threshold_results = []
    
    for t in thresholds:
        y_pred = (y_pred_proba >= t).astype(int)
        f1 = f1_score(y_true, y_pred, zero_division=0)
        
        threshold_results.append({
            'threshold': float(t),
            'f1': float(f1)
        })
        
        if f1 > best_f1:
            best_f1 = f1
            best_threshold = t
    
    return best_threshold, best_f1, threshold_results


def plot_curves(y_true, y_pred_proba, save_dir):
    """ç»˜åˆ¶PRæ›²çº¿å’ŒROCæ›²çº¿"""
    fig, axes = plt.subplots(1, 2, figsize=(12, 5))
    
    # PRæ›²çº¿
    precision, recall, _ = precision_recall_curve(y_true, y_pred_proba)
    aupr = average_precision_score(y_true, y_pred_proba)
    
    axes[0].plot(recall, precision, 'b-', linewidth=2, label=f'XGBoost (AUPR={aupr:.3f})')
    axes[0].plot([0, 1], [y_true.sum()/len(y_true)]*2, 'r--', label='Random')
    axes[0].set_xlabel('Recall', fontsize=12)
    axes[0].set_ylabel('Precision', fontsize=12)
    axes[0].set_title('Precision-Recall Curve', fontsize=14, fontweight='bold')
    axes[0].legend(fontsize=10)
    axes[0].grid(True, alpha=0.3)
    
    # ROCæ›²çº¿
    fpr, tpr, _ = roc_curve(y_true, y_pred_proba)
    auroc = roc_auc_score(y_true, y_pred_proba)
    
    axes[1].plot(fpr, tpr, 'b-', linewidth=2, label=f'XGBoost (AUROC={auroc:.3f})')
    axes[1].plot([0, 1], [0, 1], 'r--', label='Random')
    axes[1].set_xlabel('False Positive Rate', fontsize=12)
    axes[1].set_ylabel('True Positive Rate', fontsize=12)
    axes[1].set_title('ROC Curve', fontsize=14, fontweight='bold')
    axes[1].legend(fontsize=10)
    axes[1].grid(True, alpha=0.3)
    
    plt.tight_layout()
    plt.savefig(save_dir / 'curves.png', dpi=300, bbox_inches='tight')
    plt.close()
    
    print(f"   âœ“ æ›²çº¿å·²ä¿å­˜: {save_dir / 'curves.png'}")


def plot_confusion_matrix(y_true, y_pred, save_dir):
    """ç»˜åˆ¶æ··æ·†çŸ©é˜µ"""
    cm = confusion_matrix(y_true, y_pred)
    
    plt.figure(figsize=(8, 6))
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', 
                xticklabels=['Not Leave', 'Leave'],
                yticklabels=['Not Leave', 'Leave'],
                cbar_kws={'label': 'Count'})
    plt.xlabel('Predicted', fontsize=12)
    plt.ylabel('Actual', fontsize=12)
    plt.title('Confusion Matrix - XGBoost', fontsize=14, fontweight='bold')
    plt.tight_layout()
    plt.savefig(save_dir / 'confusion_matrix.png', dpi=300, bbox_inches='tight')
    plt.close()
    
    print(f"   âœ“ æ··æ·†çŸ©é˜µå·²ä¿å­˜: {save_dir / 'confusion_matrix.png'}")


def plot_feature_importance(model, save_dir, top_n=20):
    """ç»˜åˆ¶ç‰¹å¾é‡è¦æ€§"""
    importance_dict = model.get_score(importance_type='gain')
    
    # è½¬æ¢ä¸ºDataFrameå¹¶æ’åº
    importance_df = pd.DataFrame([
        {'feature': k, 'importance': v} 
        for k, v in importance_dict.items()
    ]).sort_values('importance', ascending=False).head(top_n)
    
    plt.figure(figsize=(10, 8))
    plt.barh(range(len(importance_df)), importance_df['importance'].values)
    plt.yticks(range(len(importance_df)), importance_df['feature'].values)
    plt.xlabel('Importance (Gain)', fontsize=12)
    plt.ylabel('Feature', fontsize=12)
    plt.title(f'Top {top_n} Feature Importance - XGBoost', fontsize=14, fontweight='bold')
    plt.gca().invert_yaxis()
    plt.tight_layout()
    plt.savefig(save_dir / 'feature_importance.png', dpi=300, bbox_inches='tight')
    plt.close()
    
    print(f"   âœ“ ç‰¹å¾é‡è¦æ€§å·²ä¿å­˜: {save_dir / 'feature_importance.png'}")
    
    return importance_df


def train_xgboost(X_train, y_train, X_val, y_val, args):
    """è®­ç»ƒXGBoostæ¨¡å‹"""
    print("\n2. è®­ç»ƒXGBoostæ¨¡å‹...")
    
    # è®¡ç®—pos_weight
    num_pos = y_train.sum()
    num_neg = len(y_train) - num_pos
    scale_pos_weight = num_neg / num_pos
    
    print(f"   âœ“ æ­£æ ·æœ¬: {num_pos} ({num_pos/len(y_train)*100:.1f}%)")
    print(f"   âœ“ è´Ÿæ ·æœ¬: {num_neg} ({num_neg/len(y_train)*100:.1f}%)")
    print(f"   âœ“ scale_pos_weight: {scale_pos_weight:.2f}")
    
    # XGBoostå‚æ•°
    params = {
        'objective': 'binary:logistic',
        'eval_metric': 'aucpr',  # ä½¿ç”¨AUPRä½œä¸ºè¯„ä¼°æŒ‡æ ‡
        'max_depth': args.max_depth,
        'learning_rate': args.learning_rate,
        'n_estimators': args.n_estimators,
        'subsample': args.subsample,
        'colsample_bytree': args.colsample_bytree,
        'min_child_weight': args.min_child_weight,
        'scale_pos_weight': scale_pos_weight,
        'random_state': args.seed,
        'tree_method': 'hist',
        'verbosity': 0
    }
    
    print(f"\n   è¶…å‚æ•°:")
    for k, v in params.items():
        print(f"      {k}: {v}")
    
    # åˆ›å»ºDMatrix
    dtrain = xgb.DMatrix(X_train, label=y_train)
    dval = xgb.DMatrix(X_val, label=y_val)
    
    # è®­ç»ƒ
    evals = [(dtrain, 'train'), (dval, 'val')]
    evals_result = {}
    
    print("\n   å¼€å§‹è®­ç»ƒ...")
    model = xgb.train(
        params,
        dtrain,
        num_boost_round=args.n_estimators,
        evals=evals,
        early_stopping_rounds=args.patience,
        evals_result=evals_result,
        verbose_eval=False
    )
    
    print(f"   âœ“ è®­ç»ƒå®Œæˆ! æœ€ä½³è¿­ä»£: {model.best_iteration}")
    print(f"   âœ“ æœ€ä½³éªŒè¯AUPR: {model.best_score:.4f}")
    
    return model, evals_result


def evaluate_model(model, X, y, mask_name='test'):
    """è¯„ä¼°æ¨¡å‹"""
    dtest = xgb.DMatrix(X)
    y_pred_proba = model.predict(dtest)
    
    # è®¡ç®—æŒ‡æ ‡
    metrics = {
        'aupr': average_precision_score(y, y_pred_proba),
        'auroc': roc_auc_score(y, y_pred_proba)
    }
    
    print(f"\n   {mask_name.upper()}é›†æ€§èƒ½ (æ¦‚ç‡é¢„æµ‹):")
    print(f"      AUPR:  {metrics['aupr']:.4f}")
    print(f"      AUROC: {metrics['auroc']:.4f}")
    
    return y_pred_proba, metrics


def main():
    parser = argparse.ArgumentParser(description='XGBoost Baselineè®­ç»ƒ')
    
    # æ•°æ®å‚æ•°
    parser.add_argument('--data-path', type=str, 
                       default='data/processed/homo_graph.pt',
                       help='æ•°æ®è·¯å¾„')
    
    # XGBoostè¶…å‚æ•°
    parser.add_argument('--max-depth', type=int, default=6,
                       help='æ ‘çš„æœ€å¤§æ·±åº¦')
    parser.add_argument('--learning-rate', type=float, default=0.1,
                       help='å­¦ä¹ ç‡')
    parser.add_argument('--n-estimators', type=int, default=200,
                       help='æ ‘çš„æ•°é‡')
    parser.add_argument('--subsample', type=float, default=0.8,
                       help='æ ·æœ¬é‡‡æ ·æ¯”ä¾‹')
    parser.add_argument('--colsample-bytree', type=float, default=0.8,
                       help='ç‰¹å¾é‡‡æ ·æ¯”ä¾‹')
    parser.add_argument('--min-child-weight', type=int, default=1,
                       help='æœ€å°å­èŠ‚ç‚¹æƒé‡')
    parser.add_argument('--patience', type=int, default=20,
                       help='æ—©åœpatience')
    
    # å…¶ä»–å‚æ•°
    parser.add_argument('--seed', type=int, default=42,
                       help='éšæœºç§å­')
    parser.add_argument('--save-dir', type=str, 
                       default='outputs/xgboost_baseline',
                       help='ä¿å­˜ç›®å½•')
    
    args = parser.parse_args()
    
    # è®¾ç½®éšæœºç§å­
    set_seed(args.seed)
    
    # æ‰“å°é…ç½®
    print("\n" + "="*70)
    print("ğŸ“‹ XGBoost Baseline è®­ç»ƒé…ç½®")
    print("="*70)
    print(f"\næ•°æ®: {args.data_path}")
    print(f"è¶…å‚æ•°:")
    print(f"   max_depth: {args.max_depth}")
    print(f"   learning_rate: {args.learning_rate}")
    print(f"   n_estimators: {args.n_estimators}")
    print(f"   subsample: {args.subsample}")
    print(f"   colsample_bytree: {args.colsample_bytree}")
    print(f"   patience: {args.patience}")
    print(f"éšæœºç§å­: {args.seed}")
    print(f"ä¿å­˜ç›®å½•: {args.save_dir}")
    print("="*70)
    
    # åˆ›å»ºä¿å­˜ç›®å½•
    save_dir = Path(args.save_dir)
    save_dir.mkdir(parents=True, exist_ok=True)
    
    # åŠ è½½æ•°æ®
    X, y, train_mask, val_mask, test_mask = load_data(args.data_path)
    
    X_train = X[train_mask]
    y_train = y[train_mask]
    X_val = X[val_mask]
    y_val = y[val_mask]
    X_test = X[test_mask]
    y_test = y[test_mask]
    
    # è®­ç»ƒæ¨¡å‹
    model, evals_result = train_xgboost(X_train, y_train, X_val, y_val, args)
    
    # ä¿å­˜æ¨¡å‹
    model_path = save_dir / 'xgboost_model.json'
    model.save_model(str(model_path))
    print(f"\n   âœ“ æ¨¡å‹å·²ä¿å­˜: {model_path}")
    
    # åœ¨éªŒè¯é›†ä¸Šæ‰¾æœ€ä¼˜é˜ˆå€¼
    print("\n3. åœ¨éªŒè¯é›†ä¸Šå¯»æ‰¾æœ€ä¼˜é˜ˆå€¼...")
    val_pred_proba, _ = evaluate_model(model, X_val, y_val, 'val')
    best_threshold, best_f1, threshold_results = find_best_threshold(y_val, val_pred_proba)
    print(f"   âœ“ æœ€ä¼˜é˜ˆå€¼: {best_threshold:.2f} (Val F1={best_f1:.4f})")
    
    # åœ¨æµ‹è¯•é›†ä¸Šè¯„ä¼°
    print("\n4. åœ¨æµ‹è¯•é›†ä¸Šè¯„ä¼° (ä½¿ç”¨æœ€ä¼˜é˜ˆå€¼)...")
    test_pred_proba, test_metrics_base = evaluate_model(model, X_test, y_test, 'test')
    
    # ä½¿ç”¨æœ€ä¼˜é˜ˆå€¼è®¡ç®—åˆ†ç±»æŒ‡æ ‡
    test_metrics = compute_metrics(y_test, test_pred_proba, best_threshold)
    
    print(f"\n   TESTé›†æ€§èƒ½ (é˜ˆå€¼={best_threshold:.2f}):")
    print(f"      AUPR:      {test_metrics['aupr']:.4f}")
    print(f"      AUROC:     {test_metrics['auroc']:.4f}")
    print(f"      F1:        {test_metrics['f1']:.4f}")
    print(f"      Precision: {test_metrics['precision']:.4f}")
    print(f"      Recall:    {test_metrics['recall']:.4f}")
    
    # ç”Ÿæˆå¯è§†åŒ–
    print("\n5. ç”Ÿæˆå¯è§†åŒ–...")
    plot_curves(y_test, test_pred_proba, save_dir)
    
    test_pred = (test_pred_proba >= best_threshold).astype(int)
    plot_confusion_matrix(y_test, test_pred, save_dir)
    
    importance_df = plot_feature_importance(model, save_dir)
    
    # ä¿å­˜ç»“æœ
    print("\n6. ä¿å­˜ç»“æœ...")
    results = {
        'model': 'XGBoost',
        'seed': args.seed,
        'hyperparameters': {
            'max_depth': args.max_depth,
            'learning_rate': args.learning_rate,
            'n_estimators': args.n_estimators,
            'subsample': args.subsample,
            'colsample_bytree': args.colsample_bytree,
            'min_child_weight': args.min_child_weight,
            'patience': args.patience
        },
        'best_iteration': int(model.best_iteration),
        'best_val_aupr': float(model.best_score),
        'best_threshold': float(best_threshold),
        'best_val_f1': float(best_f1),
        'test_metrics': {k: float(v) for k, v in test_metrics.items()},
        'threshold_scan': threshold_results,
        'training_history': {
            'train_aupr': [float(x) for x in evals_result['train']['aucpr']],
            'val_aupr': [float(x) for x in evals_result['val']['aucpr']]
        },
        'feature_importance': importance_df.to_dict('records')
    }
    
    results_path = save_dir / 'results.json'
    with open(results_path, 'w') as f:
        json.dump(results, f, indent=2)
    print(f"   âœ“ ç»“æœå·²ä¿å­˜: {results_path}")
    
    # ä¿å­˜ç‰¹å¾é‡è¦æ€§CSV
    importance_csv_path = save_dir / 'feature_importance.csv'
    importance_df.to_csv(importance_csv_path, index=False)
    print(f"   âœ“ ç‰¹å¾é‡è¦æ€§å·²ä¿å­˜: {importance_csv_path}")
    
    # ç”Ÿæˆæ–‡æœ¬æŠ¥å‘Š
    report_path = save_dir / 'evaluation_report.txt'
    with open(report_path, 'w') as f:
        f.write("="*70 + "\n")
        f.write("XGBoost Baseline - è¯„ä¼°æŠ¥å‘Š\n")
        f.write("="*70 + "\n\n")
        
        f.write("1. æ¨¡å‹é…ç½®\n")
        f.write(f"   - æ¨¡å‹: XGBoost\n")
        f.write(f"   - éšæœºç§å­: {args.seed}\n")
        f.write(f"   - æœ€å¤§æ·±åº¦: {args.max_depth}\n")
        f.write(f"   - å­¦ä¹ ç‡: {args.learning_rate}\n")
        f.write(f"   - æ ‘çš„æ•°é‡: {args.n_estimators}\n")
        f.write(f"   - æœ€ä½³è¿­ä»£: {model.best_iteration}\n\n")
        
        f.write("2. æ•°æ®é›†ä¿¡æ¯\n")
        f.write(f"   - è®­ç»ƒé›†: {train_mask.sum()} æ ·æœ¬\n")
        f.write(f"   - éªŒè¯é›†: {val_mask.sum()} æ ·æœ¬\n")
        f.write(f"   - æµ‹è¯•é›†: {test_mask.sum()} æ ·æœ¬\n")
        f.write(f"   - ç¦»èŒç‡: {y.sum()}/{len(y)} ({y.sum()/len(y)*100:.1f}%)\n\n")
        
        f.write("3. éªŒè¯é›†æ€§èƒ½\n")
        f.write(f"   - æœ€ä½³AUPR: {model.best_score:.4f}\n")
        f.write(f"   - æœ€ä¼˜é˜ˆå€¼: {best_threshold:.2f}\n")
        f.write(f"   - æœ€ä¼˜F1: {best_f1:.4f}\n\n")
        
        f.write("4. æµ‹è¯•é›†æ€§èƒ½\n")
        f.write(f"   - AUPR:      {test_metrics['aupr']:.4f}\n")
        f.write(f"   - AUROC:     {test_metrics['auroc']:.4f}\n")
        f.write(f"   - F1 Score:  {test_metrics['f1']:.4f}\n")
        f.write(f"   - Precision: {test_metrics['precision']:.4f}\n")
        f.write(f"   - Recall:    {test_metrics['recall']:.4f}\n\n")
        
        f.write("5. Top 10ç‰¹å¾é‡è¦æ€§\n")
        for i, row in importance_df.head(10).iterrows():
            f.write(f"   {i+1}. {row['feature']}: {row['importance']:.2f}\n")
        
        f.write("\n" + "="*70 + "\n")
        f.write(f"æŠ¥å‘Šç”Ÿæˆæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
        f.write("="*70 + "\n")
    
    print(f"   âœ“ è¯„ä¼°æŠ¥å‘Šå·²ä¿å­˜: {report_path}")
    
    print("\n" + "="*70)
    print("âœ… XGBoost Baseline è®­ç»ƒå®Œæˆ!")
    print("="*70)
    print(f"\nğŸ“Š æµ‹è¯•é›†æ€§èƒ½æ€»ç»“:")
    print(f"   AUPR:      {test_metrics['aupr']:.4f}")
    print(f"   AUROC:     {test_metrics['auroc']:.4f}")
    print(f"   F1 Score:  {test_metrics['f1']:.4f}")
    print(f"   Precision: {test_metrics['precision']:.4f}")
    print(f"   Recall:    {test_metrics['recall']:.4f}")
    print(f"\nğŸ“ æ‰€æœ‰ç»“æœå·²ä¿å­˜è‡³: {save_dir}")
    print("="*70 + "\n")


if __name__ == '__main__':
    main()