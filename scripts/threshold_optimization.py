"""
é˜ˆå€¼ä¼˜åŒ–è„šæœ¬
æ— éœ€é‡æ–°è®­ç»ƒï¼Œç›´æ¥æ‰¾åˆ°æœ€ä¼˜å†³ç­–é˜ˆå€¼
"""

import torch
import numpy as np
import sys
from pathlib import Path

sys.path.insert(0, 'src/models')

from gcn import create_gcn_model
from evaluator2 import GCNEvaluator
from sklearn.metrics import f1_score, precision_score, recall_score, confusion_matrix

print("\n" + "="*70)
print("ğŸ¯ é˜ˆå€¼ä¼˜åŒ– - å¯»æ‰¾æœ€ä¼˜å†³ç­–é˜ˆå€¼")
print("="*70)

# åŠ è½½æ•°æ®å’Œæ¨¡å‹
print("\n1. åŠ è½½æ¨¡å‹...")
data = torch.load('data/processed/homo_graph.pt')
model = create_gcn_model(data.num_node_features, architecture='default')

checkpoint = torch.load('outputs/models/best_model.pt')
model.load_state_dict(checkpoint['model_state_dict'])
print("   âœ“ æ¨¡å‹åŠ è½½æˆåŠŸ")

# è·å–é¢„æµ‹æ¦‚ç‡
print("\n2. è·å–é¢„æµ‹æ¦‚ç‡...")
evaluator = GCNEvaluator(model, data)
probs, preds_default, labels = evaluator.predict(data.test_mask)
print(f"   âœ“ æµ‹è¯•é›†æ ·æœ¬æ•°: {len(labels)}")
print(f"   âœ“ ç¦»èŒæ ·æœ¬æ•°: {labels.sum()}")
print(f"   âœ“ æ¦‚ç‡èŒƒå›´: [{probs.min():.3f}, {probs.max():.3f}]")

# æ‰«æé˜ˆå€¼
print("\n3. æ‰«æé˜ˆå€¼ (0.05-0.70)...")
print("="*90)
print(f"{'é˜ˆå€¼':<8} {'F1':<8} {'Recall':<10} {'Precision':<12} {'TP':<6} {'FP':<6} {'FN':<6} {'æ ‡è®°'}")
print("="*90)

best_f1 = 0
best_threshold = 0.5
best_stats = {}
results = []

for threshold in np.arange(0.05, 0.75, 0.05):
    preds = (probs > threshold).astype(int)
    
    if preds.sum() == 0:
        continue
    
    f1 = f1_score(labels, preds, zero_division=0)
    recall = recall_score(labels, preds, zero_division=0)
    precision = precision_score(labels, preds, zero_division=0)
    
    tp = ((preds == 1) & (labels == 1)).sum()
    fp = ((preds == 1) & (labels == 0)).sum()
    fn = ((preds == 0) & (labels == 1)).sum()
    
    marker = "â­ æœ€ä¼˜" if f1 > best_f1 else ""
    
    if f1 > best_f1:
        best_f1 = f1
        best_threshold = threshold
        best_stats = {
            'f1': float(f1), 'recall': float(recall), 'precision': float(precision),
            'tp': int(tp), 'fp': int(fp), 'fn': int(fn),
            'threshold': float(threshold)
        }
    
    results.append({
        'threshold': threshold, 'f1': f1, 'recall': recall,
        'precision': precision, 'tp': tp, 'fp': fp, 'fn': fn
    })
    
    print(f"{threshold:<8.2f} {f1:<8.3f} {recall:<10.3f} {precision:<12.3f} "
          f"{tp:<6} {fp:<6} {fn:<6} {marker}")

print("="*90)

# æœ€ä¼˜ç»“æœ
print("\n" + "="*70)
print("âœ… æœ€ä¼˜é˜ˆå€¼ç»“æœ")
print("="*70)
print(f"\næœ€ä¼˜é˜ˆå€¼:    {best_stats['threshold']:.2f} (é»˜è®¤: 0.50)")
print(f"F1-Score:    {best_stats['f1']:.4f} (åŸ: {f1_score(labels, preds_default):.4f}, æå‡: {(best_stats['f1']/f1_score(labels, preds_default)-1)*100:+.1f}%)")
print(f"Recall:      {best_stats['recall']:.4f} (åŸ: {recall_score(labels, preds_default):.4f}, æå‡: {(best_stats['recall']/recall_score(labels, preds_default)-1)*100:+.1f}%)")
print(f"Precision:   {best_stats['precision']:.4f} (åŸ: {precision_score(labels, preds_default):.4f}, æå‡: {(best_stats['precision']/precision_score(labels, preds_default)-1)*100:+.1f}%)")

print(f"\næ··æ·†çŸ©é˜µ:")
preds_opt = (probs > best_stats['threshold']).astype(int)
cm = confusion_matrix(labels, preds_opt)
print(f"                é¢„æµ‹ä¸ç¦»èŒ    é¢„æµ‹ç¦»èŒ")
print(f"  å®é™…ä¸ç¦»èŒ    {cm[0,0]:>8}      {cm[0,1]:>8}")
print(f"  å®é™…ç¦»èŒ      {cm[1,0]:>8}      {cm[1,1]:>8}")

print(f"\nä¸šåŠ¡å«ä¹‰:")
print(f"   âœ“ æ­£ç¡®è¯†åˆ«ç¦»èŒ: {best_stats['tp']}/{labels.sum()} ({best_stats['recall']*100:.1f}%)")
print(f"   âœ“ è¯¯æŠ¥åœ¨èŒ:     {best_stats['fp']}/{len(labels)-labels.sum()} ({best_stats['fp']/(len(labels)-labels.sum())*100:.1f}%)")
print(f"   âœ“ æ¼æŠ¥ç¦»èŒ:     {best_stats['fn']}/{labels.sum()} ({best_stats['fn']/labels.sum()*100:.1f}%)")

# ä¿å­˜ç»“æœ
print("\n4. ä¿å­˜æœ€ä¼˜é˜ˆå€¼...")
import json
with open('outputs/evaluation/optimal_threshold.json', 'w') as f:
    json.dump(best_stats, f, indent=2)
print("   âœ“ å·²ä¿å­˜: outputs/evaluation/optimal_threshold.json")

# ç»˜åˆ¶é˜ˆå€¼-æ€§èƒ½æ›²çº¿
print("\n5. ç»˜åˆ¶é˜ˆå€¼æ›²çº¿...")
import matplotlib
matplotlib.use('Agg')
import matplotlib.pyplot as plt

fig, axes = plt.subplots(2, 2, figsize=(14, 10))

thresholds = [r['threshold'] for r in results]
f1s = [r['f1'] for r in results]
recalls = [r['recall'] for r in results]
precisions = [r['precision'] for r in results]

# F1 vs é˜ˆå€¼
ax = axes[0, 0]
ax.plot(thresholds, f1s, 'b-', linewidth=2)
ax.axvline(best_threshold, color='r', linestyle='--', label=f'æœ€ä¼˜={best_threshold:.2f}')
ax.axhline(best_f1, color='r', linestyle='--', alpha=0.3)
ax.set_xlabel('é˜ˆå€¼')
ax.set_ylabel('F1-Score')
ax.set_title('F1-Score vs é˜ˆå€¼')
ax.legend()
ax.grid(alpha=0.3)

# Recall vs é˜ˆå€¼
ax = axes[0, 1]
ax.plot(thresholds, recalls, 'g-', linewidth=2)
ax.axvline(best_threshold, color='r', linestyle='--')
ax.set_xlabel('é˜ˆå€¼')
ax.set_ylabel('Recall')
ax.set_title('Recall vs é˜ˆå€¼')
ax.grid(alpha=0.3)

# Precision vs é˜ˆå€¼
ax = axes[1, 0]
ax.plot(thresholds, precisions, 'orange', linewidth=2)
ax.axvline(best_threshold, color='r', linestyle='--')
ax.set_xlabel('é˜ˆå€¼')
ax.set_ylabel('Precision')
ax.set_title('Precision vs é˜ˆå€¼')
ax.grid(alpha=0.3)

# Precision-Recallæ›²çº¿
ax = axes[1, 1]
ax.plot(recalls, precisions, 'purple', linewidth=2)
ax.scatter([best_stats['recall']], [best_stats['precision']], 
          color='r', s=100, zorder=5, label=f'æœ€ä¼˜ç‚¹ (é˜ˆå€¼={best_threshold:.2f})')
ax.set_xlabel('Recall')
ax.set_ylabel('Precision')
ax.set_title('Precision-Recallæ›²çº¿')
ax.legend()
ax.grid(alpha=0.3)

plt.tight_layout()
plt.savefig('outputs/evaluation/threshold_optimization.png', dpi=150, bbox_inches='tight')
print("   âœ“ å·²ä¿å­˜: outputs/evaluation/threshold_optimization.png")

print("\n" + "="*70)
print("ğŸ‰ é˜ˆå€¼ä¼˜åŒ–å®Œæˆï¼")
print("="*70)
print(f"\nä½¿ç”¨æœ€ä¼˜é˜ˆå€¼ {best_threshold:.2f} å¯ä½¿F1æå‡è‡³ {best_stats['f1']:.4f}")
print(f"æ— éœ€é‡æ–°è®­ç»ƒï¼Œç›´æ¥ä½¿ç”¨æ­¤é˜ˆå€¼å³å¯ï¼")