"""
å¤šä»»åŠ¡GCNè®­ç»ƒè„šæœ¬ v3 - ç¦»èŒé¢„æµ‹ + å²—ä½åå¥½æ’åº
========================================================
Loss = Î± Ã— ç¦»èŒåˆ†ç±»Loss + Î² Ã— å²—ä½åå¥½Ranking Loss

å²—ä½åå¥½Loss: Pairwise Ranking Loss
å¯¹æ¯ä¸ªå‘˜å·¥çš„7ä¸ªå²—ä½è¿›è¡Œä¸¤ä¸¤æ¯”è¾ƒ:
  å¦‚æœ rank(å²—ä½i) < rank(å²—ä½j)ï¼Œåˆ™ score(å‘˜å·¥,å²—ä½i) > score(å‘˜å·¥,å²—ä½j)
"""

import numpy as np
import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
from torch_geometric.nn import GCNConv
from sklearn.metrics import (
    average_precision_score,
    roc_auc_score,
    f1_score,
    precision_score,
    recall_score,
    ndcg_score
)
import json
import argparse
from pathlib import Path
from datetime import datetime
from tqdm import tqdm
import matplotlib.pyplot as plt
import seaborn as sns

# è®¾ç½®ä¸­æ–‡å­—ä½“
plt.rcParams['font.sans-serif'] = ['Arial Unicode MS', 'SimHei', 'DejaVu Sans']
plt.rcParams['axes.unicode_minus'] = False


class MultiTaskGCN(nn.Module):
    """
    å¤šä»»åŠ¡GCN: ç¦»èŒé¢„æµ‹ + å²—ä½åå¥½æ’åº
    """
    def __init__(
        self,
        in_features,
        hidden_dims=[128, 64, 32],
        n_positions=7,
        position_embed_dim=32,
        dropout=0.5,
        preference_loss_type='pairwise'
    ):
        super().__init__()
        
        # GCNç¼–ç å™¨
        self.convs = nn.ModuleList()
        prev_dim = in_features
        for hidden_dim in hidden_dims:
            self.convs.append(GCNConv(prev_dim, hidden_dim))
            prev_dim = hidden_dim
        
        self.final_dim = hidden_dims[-1]
        
        # å²—ä½embedding
        self.position_embedding = nn.Embedding(n_positions, position_embed_dim)
        
        # ç¦»èŒé¢„æµ‹å¤´
        self.turnover_head = nn.Sequential(
            nn.Linear(self.final_dim, self.final_dim // 2),
            nn.ReLU(),
            nn.Dropout(dropout),
            nn.Linear(self.final_dim // 2, 1)
        )
        
        # åå¥½é¢„æµ‹: æŠ•å½±åˆ°ç›¸åŒç©ºé—´
        self.employee_proj = nn.Linear(self.final_dim, position_embed_dim)
        
        self.dropout = dropout
        self.n_positions = n_positions
        self.preference_loss_type = preference_loss_type
        
    def encode_employees(self, x, edge_index):
        """ç¼–ç å‘˜å·¥ç‰¹å¾"""
        h = x
        for i, conv in enumerate(self.convs):
            h = conv(h, edge_index)
            h = F.relu(h)
            if i < len(self.convs) - 1:  # æœ€åä¸€å±‚ä¸dropout
                h = F.dropout(h, p=self.dropout, training=self.training)
        return h
    
    def forward(self, x, edge_index):
        """
        å‰å‘ä¼ æ’­
        
        Returns:
            employee_emb: (N, final_dim) å‘˜å·¥embedding
            turnover_logits: (N,) ç¦»èŒé¢„æµ‹logits
            preference_scores: (N, n_positions) å²—ä½åå¥½å¾—åˆ†
        """
        # ç¼–ç å‘˜å·¥
        employee_emb = self.encode_employees(x, edge_index)  # (N, final_dim)
        
        # ç¦»èŒé¢„æµ‹
        turnover_logits = self.turnover_head(employee_emb).squeeze()  # (N,)
        
        # å²—ä½åå¥½å¾—åˆ†
        employee_proj = self.employee_proj(employee_emb)  # (N, position_embed_dim)
        position_emb = self.position_embedding.weight  # (n_positions, position_embed_dim)
        preference_scores = employee_proj @ position_emb.T  # (N, n_positions)
        
        return employee_emb, turnover_logits, preference_scores
    
    def compute_loss(
        self,
        turnover_logits,
        turnover_labels,
        preference_scores,
        preference_ranks,
        alpha=0.5,
        beta=0.5,
        pos_weight=None
    ):
        """
        è®¡ç®—å¤šä»»åŠ¡æŸå¤±
        
        Args:
            turnover_logits: (N,) ç¦»èŒé¢„æµ‹logits
            turnover_labels: (N,) ç¦»èŒæ ‡ç­¾ 0/1
            preference_scores: (N, K) å²—ä½å¾—åˆ† K=n_positions
            preference_ranks: (N, K) å²—ä½æ’åº 1-7 (1=æœ€åå¥½)
            alpha: ç¦»èŒlossæƒé‡
            beta: åå¥½lossæƒé‡
            pos_weight: æ­£æ ·æœ¬æƒé‡
        
        Returns:
            total_loss, turnover_loss, preference_loss
        """
        # Loss 1: ç¦»èŒåˆ†ç±»
        if pos_weight is not None:
            turnover_loss = F.binary_cross_entropy_with_logits(
                turnover_logits,
                turnover_labels.float(),
                pos_weight=pos_weight
            )
        else:
            turnover_loss = F.binary_cross_entropy_with_logits(
                turnover_logits,
                turnover_labels.float()
            )
        
        # Loss 2: å²—ä½åå¥½æ’åº
        if self.preference_loss_type == 'pairwise':
            preference_loss = self._pairwise_ranking_loss(
                preference_scores, preference_ranks
            )
        elif self.preference_loss_type == 'listnet':
            preference_loss = self._listnet_loss(
                preference_scores, preference_ranks
            )
        else:
            raise ValueError(f"Unknown loss type: {self.preference_loss_type}")
        
        # æ€»æŸå¤±
        total_loss = alpha * turnover_loss + beta * preference_loss
        
        return total_loss, turnover_loss, preference_loss
    
    def _pairwise_ranking_loss(self, scores, ranks):
        """
        Pairwise Ranking Loss (å‘é‡åŒ–å®ç°)
        
        å¯¹äºæ¯ä¸ªå‘˜å·¥ï¼Œå¦‚æœ rank[i] < rank[j] (å²—ä½iæ›´åå¥½)
        åˆ™å¸Œæœ› score[i] > score[j]
        
        Loss = max(0, margin - (score[i] - score[j]))
        """
        N, K = scores.shape  # N=å‘˜å·¥æ•°, K=å²—ä½æ•°
        
        # æ„å»ºpairwise mask: (N, K, K)
        # mask[n, i, j] = 1 if rank[n, i] < rank[n, j]
        ranks_i = ranks.unsqueeze(2)  # (N, K, 1)
        ranks_j = ranks.unsqueeze(1)  # (N, 1, K)
        prefer_mask = (ranks_i < ranks_j).float()  # (N, K, K)
        
        # è®¡ç®—æ‰€æœ‰pairwiseå¾—åˆ†å·®å€¼: (N, K, K)
        scores_i = scores.unsqueeze(2)  # (N, K, 1)
        scores_j = scores.unsqueeze(1)  # (N, 1, K)
        score_diff = scores_i - scores_j  # (N, K, K)
        
        # Margin ranking loss: max(0, margin - score_diff)
        margin = 1.0
        margin_loss = F.relu(margin - score_diff) * prefer_mask
        
        # å¹³å‡
        num_pairs = prefer_mask.sum()
        if num_pairs > 0:
            loss = margin_loss.sum() / num_pairs
        else:
            loss = torch.tensor(0.0, device=scores.device)
        
        return loss
    
    def _listnet_loss(self, scores, ranks):
        """
        ListNet Loss: ç›´æ¥ä¼˜åŒ–æ’åºåˆ†å¸ƒ
        
        ä½¿ç”¨KLæ•£åº¦æ¯”è¾ƒçœŸå®æ’åºåˆ†å¸ƒå’Œé¢„æµ‹æ’åºåˆ†å¸ƒ
        """
        # çœŸå®æ’åº â†’ æ¦‚ç‡åˆ†å¸ƒ (rankè¶Šå°ï¼Œæ¦‚ç‡è¶Šå¤§)
        # ä½¿ç”¨ softmax(-rank) æ¥è½¬æ¢
        true_probs = F.softmax(-ranks.float(), dim=1)  # (N, K)
        
        # é¢„æµ‹å¾—åˆ† â†’ æ¦‚ç‡åˆ†å¸ƒ
        pred_probs = F.softmax(scores, dim=1)  # (N, K)
        
        # KLæ•£åº¦: KL(true || pred)
        loss = F.kl_div(
            pred_probs.log(),
            true_probs,
            reduction='batchmean'
        )
        
        return loss


def set_seed(seed: int):
    """å›ºå®šéšæœºç§å­"""
    np.random.seed(seed)
    torch.manual_seed(seed)
    if torch.cuda.is_available():
        torch.cuda.manual_seed(seed)


def load_data(data_path: str):
    """
    åŠ è½½æ•°æ®
    
    éœ€è¦åŒ…å«:
        - x: å‘˜å·¥ç‰¹å¾
        - y: ç¦»èŒæ ‡ç­¾
        - edge_index: å›¾è¾¹
        - preference_ranks: å²—ä½åå¥½æ’åº (N, 7)
        - train_mask, val_mask, test_mask
    """
    print("\n1. åŠ è½½æ•°æ®...")
    data = torch.load(data_path)
    
    print(f"   âœ“ èŠ‚ç‚¹: {data.x.shape[0]}")
    print(f"   âœ“ è¾¹: {data.edge_index.shape[1]}")
    print(f"   âœ“ ç‰¹å¾: {data.x.shape[1]}")
    print(f"   âœ“ è®­ç»ƒé›†: {data.train_mask.sum()}")
    print(f"   âœ“ éªŒè¯é›†: {data.val_mask.sum()}")
    print(f"   âœ“ æµ‹è¯•é›†: {data.test_mask.sum()}")
    
    # æ£€æŸ¥æ˜¯å¦æœ‰preference_ranks
    if not hasattr(data, 'preference_ranks'):
        print("\n   âš ï¸  æ•°æ®ä¸­æ²¡æœ‰preference_ranksï¼Œéœ€è¦å…ˆå‡†å¤‡å²—ä½åå¥½æ•°æ®ï¼")
        print("   æç¤º: ä»åŸå§‹æ•°æ®ä¸­æå–7ä¸ªæƒ…æ™¯ä»»åŠ¡çš„å²—ä½åå¥½æ’åº")
        raise ValueError("Missing preference_ranks in data")
    
    print(f"   âœ“ å²—ä½æ•°: {data.preference_ranks.shape[1]}")
    
    return data


def evaluate_turnover(model, data, mask, device):
    """è¯„ä¼°ç¦»èŒé¢„æµ‹æ€§èƒ½"""
    model.eval()
    with torch.no_grad():
        _, turnover_logits, _ = model(data.x.to(device), data.edge_index.to(device))
        
        logits = turnover_logits[mask].cpu()
        labels = data.y[mask].cpu()
        
        probs = torch.sigmoid(logits).numpy()
        labels_np = labels.numpy()
        
        aupr = average_precision_score(labels_np, probs)
        auroc = roc_auc_score(labels_np, probs)
        
        return aupr, auroc, probs, labels_np


def evaluate_preference(model, data, mask, device):
    """
    è¯„ä¼°å²—ä½åå¥½é¢„æµ‹æ€§èƒ½
    
    ä½¿ç”¨NDCG@Kå’ŒKendall's Tau
    """
    model.eval()
    with torch.no_grad():
        _, _, preference_scores = model(data.x.to(device), data.edge_index.to(device))
        
        scores = preference_scores[mask].cpu().numpy()  # (N, K)
        ranks = data.preference_ranks[mask].cpu().numpy()  # (N, K)
        
        # NDCG@3: è¯„ä¼°Top-3æ¨èè´¨é‡
        # éœ€è¦å°†rankè½¬ä¸ºrelevance score (rank 1 â†’ score 7)
        relevance = 8 - ranks  # rank 1-7 â†’ relevance 7-1
        
        ndcg3_scores = []
        ndcg5_scores = []
        
        for i in range(len(scores)):
            # NDCG@3
            ndcg3 = ndcg_score([relevance[i]], [scores[i]], k=3)
            ndcg3_scores.append(ndcg3)
            
            # NDCG@5
            ndcg5 = ndcg_score([relevance[i]], [scores[i]], k=5)
            ndcg5_scores.append(ndcg5)
        
        ndcg3 = np.mean(ndcg3_scores)
        ndcg5 = np.mean(ndcg5_scores)
        
        # Pairwise accuracy
        pairwise_acc = compute_pairwise_accuracy(scores, ranks)
        
        return ndcg3, ndcg5, pairwise_acc


def compute_pairwise_accuracy(scores, ranks):
    """
    è®¡ç®—æˆå¯¹æ¯”è¾ƒçš„å‡†ç¡®ç‡
    
    å¯¹äºæ¯å¯¹å²—ä½(i,j)ï¼Œå¦‚æœrank[i] < rank[j]ï¼Œæ£€æŸ¥æ˜¯å¦score[i] > score[j]
    """
    N, K = scores.shape
    correct = 0
    total = 0
    
    for n in range(N):
        for i in range(K):
            for j in range(i+1, K):
                if ranks[n, i] != ranks[n, j]:  # æ’åä¸åŒ
                    total += 1
                    # æ£€æŸ¥é¢„æµ‹æ˜¯å¦æ­£ç¡®
                    if ranks[n, i] < ranks[n, j]:  # iæ›´åå¥½
                        if scores[n, i] > scores[n, j]:
                            correct += 1
                    else:  # jæ›´åå¥½
                        if scores[n, j] > scores[n, i]:
                            correct += 1
    
    return correct / total if total > 0 else 0.0


def find_best_threshold(y_true, y_pred_proba):
    """åœ¨éªŒè¯é›†ä¸Šå¯»æ‰¾æœ€ä¼˜é˜ˆå€¼"""
    best_threshold = 0.5
    best_f1 = 0.0
    
    thresholds = np.arange(0.1, 0.9, 0.02)
    
    for t in thresholds:
        y_pred = (y_pred_proba >= t).astype(int)
        f1 = f1_score(y_true, y_pred, zero_division=0)
        
        if f1 > best_f1:
            best_f1 = f1
            best_threshold = t
    
    return best_threshold, best_f1


def compute_metrics(y_true, y_pred_proba, threshold=0.5):
    """è®¡ç®—æ‰€æœ‰è¯„ä¼°æŒ‡æ ‡"""
    y_pred = (y_pred_proba >= threshold).astype(int)
    
    metrics = {
        'aupr': average_precision_score(y_true, y_pred_proba),
        'auroc': roc_auc_score(y_true, y_pred_proba),
        'f1': f1_score(y_true, y_pred, zero_division=0),
        'precision': precision_score(y_true, y_pred, zero_division=0),
        'recall': recall_score(y_true, y_pred, zero_division=0),
        'threshold': threshold
    }
    
    return metrics


def train_epoch(model, data, optimizer, criterion_fn, device, alpha, beta, pos_weight):
    """è®­ç»ƒä¸€ä¸ªepoch"""
    model.train()
    
    # å‰å‘ä¼ æ’­
    _, turnover_logits, preference_scores = model(
        data.x.to(device), 
        data.edge_index.to(device)
    )
    
    # åªåœ¨è®­ç»ƒé›†ä¸Šè®¡ç®—loss
    train_mask = data.train_mask
    
    total_loss, turnover_loss, preference_loss = model.compute_loss(
        turnover_logits[train_mask],
        data.y[train_mask].to(device),
        preference_scores[train_mask],
        data.preference_ranks[train_mask].to(device),
        alpha=alpha,
        beta=beta,
        pos_weight=pos_weight
    )
    
    # åå‘ä¼ æ’­
    optimizer.zero_grad()
    total_loss.backward()
    optimizer.step()
    
    return (
        total_loss.item(),
        turnover_loss.item(),
        preference_loss.item()
    )


def main():
    parser = argparse.ArgumentParser(description='å¤šä»»åŠ¡GCNè®­ç»ƒ - ç¦»èŒ+å²—ä½åå¥½')
    
    # æ•°æ®å‚æ•°
    parser.add_argument('--data-path', type=str,
                       default='data/processed/homo_graph_with_preferences.pt',
                       help='æ•°æ®è·¯å¾„ï¼ˆéœ€åŒ…å«preference_ranksï¼‰')
    
    # æ¨¡å‹å‚æ•°
    parser.add_argument('--architecture', type=str, default='default',
                       choices=['shallow', 'default', 'deep'],
                       help='æ¨¡å‹æ¶æ„')
    parser.add_argument('--position-embed-dim', type=int, default=32,
                       help='å²—ä½embeddingç»´åº¦')
    parser.add_argument('--dropout', type=float, default=0.5,
                       help='Dropoutç‡')
    parser.add_argument('--preference-loss', type=str, default='pairwise',
                       choices=['pairwise', 'listnet'],
                       help='åå¥½lossç±»å‹')
    
    # æŸå¤±æƒé‡
    parser.add_argument('--alpha', type=float, default=0.5,
                       help='ç¦»èŒlossæƒé‡')
    parser.add_argument('--beta', type=float, default=0.5,
                       help='åå¥½lossæƒé‡')
    
    # è®­ç»ƒå‚æ•°
    parser.add_argument('--lr', type=float, default=0.01,
                       help='å­¦ä¹ ç‡')
    parser.add_argument('--weight-decay', type=float, default=5e-4,
                       help='æƒé‡è¡°å‡')
    parser.add_argument('--epochs', type=int, default=200,
                       help='æœ€å¤§è®­ç»ƒè½®æ•°')
    parser.add_argument('--patience', type=int, default=20,
                       help='æ—©åœpatience')
    
    # å…¶ä»–å‚æ•°
    parser.add_argument('--device', type=str, default='cpu',
                       choices=['cpu', 'cuda'],
                       help='è®¾å¤‡')
    parser.add_argument('--seed', type=int, default=42,
                       help='éšæœºç§å­')
    parser.add_argument('--save-dir', type=str,
                       default='outputs/multitask_gcn',
                       help='ä¿å­˜ç›®å½•')
    
    args = parser.parse_args()
    
    # è®¾ç½®éšæœºç§å­
    set_seed(args.seed)
    
    # æ‰“å°é…ç½®
    print("\n" + "="*70)
    print("ğŸ¯ å¤šä»»åŠ¡GCNè®­ç»ƒé…ç½®")
    print("="*70)
    print(f"\næ•°æ®: {args.data_path}")
    print(f"\næ¨¡å‹:")
    print(f"   architecture: {args.architecture}")
    print(f"   position_embed_dim: {args.position_embed_dim}")
    print(f"   dropout: {args.dropout}")
    print(f"   preference_loss: {args.preference_loss}")
    print(f"\næŸå¤±æƒé‡:")
    print(f"   Î± (ç¦»èŒ): {args.alpha}")
    print(f"   Î² (åå¥½): {args.beta}")
    print(f"\nè®­ç»ƒ:")
    print(f"   lr: {args.lr}")
    print(f"   weight_decay: {args.weight_decay}")
    print(f"   epochs: {args.epochs}")
    print(f"   patience: {args.patience}")
    print(f"\nå…¶ä»–:")
    print(f"   device: {args.device}")
    print(f"   seed: {args.seed}")
    print("="*70)
    
    # åˆ›å»ºä¿å­˜ç›®å½•
    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
    save_dir = Path(args.save_dir) / f"run_{timestamp}"
    save_dir.mkdir(parents=True, exist_ok=True)
    
    # åŠ è½½æ•°æ®
    data = load_data(args.data_path)
    
    # åˆ›å»ºæ¨¡å‹
    print("\n2. åˆ›å»ºå¤šä»»åŠ¡GCNæ¨¡å‹...")
    architectures = {
        'shallow': [64, 32],
        'default': [128, 64, 32],
        'deep': [256, 128, 64, 32]
    }
    hidden_dims = architectures[args.architecture]
    
    model = MultiTaskGCN(
        in_features=data.x.shape[1],
        hidden_dims=hidden_dims,
        n_positions=data.preference_ranks.shape[1],
        position_embed_dim=args.position_embed_dim,
        dropout=args.dropout,
        preference_loss_type=args.preference_loss
    )
    
    num_params = sum(p.numel() for p in model.parameters())
    print(f"   âœ“ æ¶æ„: {hidden_dims}")
    print(f"   âœ“ å‚æ•°é‡: {num_params:,}")
    print(f"   âœ“ å²—ä½æ•°: {data.preference_ranks.shape[1]}")
    print(f"   âœ“ å²—ä½embeddingç»´åº¦: {args.position_embed_dim}")
    
    device = torch.device(args.device)
    model = model.to(device)
    
    # è®¡ç®—pos_weight
    num_pos = data.y[data.train_mask].sum().item()
    num_neg = data.train_mask.sum().item() - num_pos
    pos_weight = torch.tensor([num_neg / num_pos]).to(device)
    
    print(f"\n3. è®­ç»ƒé…ç½®...")
    print(f"   âœ“ æ­£æ ·æœ¬: {num_pos} ({num_pos/(num_pos+num_neg)*100:.1f}%)")
    print(f"   âœ“ è´Ÿæ ·æœ¬: {num_neg} ({num_neg/(num_pos+num_neg)*100:.1f}%)")
    print(f"   âœ“ pos_weight: {pos_weight.item():.2f}")
    
    # ä¼˜åŒ–å™¨
    optimizer = optim.Adam(
        model.parameters(),
        lr=args.lr,
        weight_decay=args.weight_decay
    )
    
    # å­¦ä¹ ç‡è°ƒåº¦å™¨
    scheduler = optim.lr_scheduler.ReduceLROnPlateau(
        optimizer, mode='max', factor=0.5, patience=10, verbose=True
    )
    
    # è®­ç»ƒ
    print("\n4. å¼€å§‹è®­ç»ƒ...")
    print("="*70)
    
    best_val_aupr = 0
    best_epoch = 0
    patience_counter = 0
    
    history = {
        'train_total_loss': [],
        'train_turnover_loss': [],
        'train_preference_loss': [],
        'val_aupr': [],
        'val_auroc': [],
        'val_ndcg3': [],
        'val_ndcg5': [],
        'lr': []
    }
    
    for epoch in tqdm(range(args.epochs), desc='Training'):
        # è®­ç»ƒ
        train_total_loss, train_turn_loss, train_pref_loss = train_epoch(
            model, data, optimizer, None, device,
            args.alpha, args.beta, pos_weight
        )
        
        # éªŒè¯
        val_aupr, val_auroc, _, _ = evaluate_turnover(
            model, data, data.val_mask, device
        )
        val_ndcg3, val_ndcg5, val_pairwise = evaluate_preference(
            model, data, data.val_mask, device
        )
        
        # å­¦ä¹ ç‡è°ƒæ•´
        scheduler.step(val_aupr)
        current_lr = optimizer.param_groups[0]['lr']
        
        # è®°å½•å†å²
        history['train_total_loss'].append(train_total_loss)
        history['train_turnover_loss'].append(train_turn_loss)
        history['train_preference_loss'].append(train_pref_loss)
        history['val_aupr'].append(val_aupr)
        history['val_auroc'].append(val_auroc)
        history['val_ndcg3'].append(val_ndcg3)
        history['val_ndcg5'].append(val_ndcg5)
        history['lr'].append(current_lr)
        
        # æ‰“å°è¿›åº¦ï¼ˆæ¯10è½®ï¼‰
        if (epoch + 1) % 10 == 0:
            print(f"\nEpoch {epoch+1:3d}:")
            print(f"  Train - Total: {train_total_loss:.4f}, "
                  f"Turnover: {train_turn_loss:.4f}, "
                  f"Preference: {train_pref_loss:.4f}")
            print(f"  Val   - AUPR: {val_aupr:.4f}, AUROC: {val_auroc:.4f}")
            print(f"          NDCG@3: {val_ndcg3:.4f}, NDCG@5: {val_ndcg5:.4f}")
            print(f"          LR: {current_lr:.6f}")
        
        # æ—©åœæ£€æŸ¥
        if val_aupr > best_val_aupr:
            best_val_aupr = val_aupr
            best_epoch = epoch
            patience_counter = 0
            # ä¿å­˜æœ€ä½³æ¨¡å‹
            torch.save(model.state_dict(), save_dir / 'best_model.pt')
            if (epoch + 1) % 10 == 0:
                print(f"  â­ æ–°çš„æœ€ä½³æ¨¡å‹!")
        else:
            patience_counter += 1
        
        if patience_counter >= args.patience:
            print(f"\n   ğŸ›‘ æ—©åœè§¦å‘äºepoch {epoch+1}")
            print(f"   ğŸ“Œ æœ€ä½³epoch: {best_epoch+1}")
            break
    
    print("\n" + "="*70)
    print("âœ… è®­ç»ƒå®Œæˆ!")
    print("="*70)
    print(f"   æœ€ä½³Epoch: {best_epoch+1}")
    print(f"   æœ€ä½³Val AUPR: {best_val_aupr:.4f}")
    
    # åŠ è½½æœ€ä½³æ¨¡å‹
    model.load_state_dict(torch.load(save_dir / 'best_model.pt'))
    
    # åœ¨éªŒè¯é›†ä¸Šæ‰¾æœ€ä¼˜é˜ˆå€¼
    print("\n5. åœ¨éªŒè¯é›†ä¸Šå¯»æ‰¾æœ€ä¼˜é˜ˆå€¼...")
    _, _, val_probs, val_labels = evaluate_turnover(
        model, data, data.val_mask, device
    )
    best_threshold, best_f1 = find_best_threshold(val_labels, val_probs)
    print(f"   âœ“ æœ€ä¼˜é˜ˆå€¼: {best_threshold:.2f} (Val F1={best_f1:.4f})")
    
    # åœ¨æµ‹è¯•é›†ä¸Šè¯„ä¼°
    print("\n6. åœ¨æµ‹è¯•é›†ä¸Šè¯„ä¼°...")
    print("\nğŸ“Š ç¦»èŒé¢„æµ‹æ€§èƒ½:")
    _, _, test_probs, test_labels = evaluate_turnover(
        model, data, data.test_mask, device
    )
    test_turn_metrics = compute_metrics(test_labels, test_probs, best_threshold)
    
    print(f"   AUPR:      {test_turn_metrics['aupr']:.4f}")
    print(f"   AUROC:     {test_turn_metrics['auroc']:.4f}")
    print(f"   F1:        {test_turn_metrics['f1']:.4f}")
    print(f"   Precision: {test_turn_metrics['precision']:.4f}")
    print(f"   Recall:    {test_turn_metrics['recall']:.4f}")
    
    print("\nğŸ“Š å²—ä½åå¥½æ€§èƒ½:")
    test_ndcg3, test_ndcg5, test_pairwise = evaluate_preference(
        model, data, data.test_mask, device
    )
    print(f"   NDCG@3:         {test_ndcg3:.4f}")
    print(f"   NDCG@5:         {test_ndcg5:.4f}")
    print(f"   Pairwise Acc:   {test_pairwise:.4f}")
    
    # ä¿å­˜ç»“æœ
    print("\n7. ä¿å­˜ç»“æœ...")
    results = {
        'config': vars(args),
        'model_params': num_params,
        'best_epoch': best_epoch + 1,
        'best_val_aupr': best_val_aupr,
        'best_threshold': best_threshold,
        'test_turnover_metrics': {k: float(v) for k, v in test_turn_metrics.items()},
        'test_preference_metrics': {
            'ndcg@3': float(test_ndcg3),
            'ndcg@5': float(test_ndcg5),
            'pairwise_accuracy': float(test_pairwise)
        },
        'training_history': history
    }
    
    results_path = save_dir / 'results.json'
    with open(results_path, 'w') as f:
        json.dump(results, f, indent=2)
    print(f"   âœ“ ç»“æœå·²ä¿å­˜: {results_path}")
    
    # ç”Ÿæˆè®­ç»ƒæ›²çº¿
    print("\n8. ç”Ÿæˆè®­ç»ƒæ›²çº¿...")
    plot_training_curves(history, save_dir)
    
    print("\n" + "="*70)
    print("ğŸ‰ å¤šä»»åŠ¡GCNè®­ç»ƒå®Œæˆ!")
    print("="*70)
    print(f"\nğŸ“Š æœ€ç»ˆæ€§èƒ½æ€»ç»“:")
    print(f"\nç¦»èŒé¢„æµ‹:")
    print(f"   AUPR:  {test_turn_metrics['aupr']:.4f}")
    print(f"   F1:    {test_turn_metrics['f1']:.4f}")
    print(f"\nå²—ä½åå¥½:")
    print(f"   NDCG@3: {test_ndcg3:.4f}")
    print(f"   NDCG@5: {test_ndcg5:.4f}")
    
    # ä¸å•ä»»åŠ¡å¯¹æ¯”
    print(f"\nğŸ’¡ ä¸å•ä»»åŠ¡GCNå¯¹æ¯”:")
    print(f"   å•ä»»åŠ¡GCN: AUPR = 0.3153")
    print(f"   å¤šä»»åŠ¡GCN: AUPR = {test_turn_metrics['aupr']:.4f}")
    
    if test_turn_metrics['aupr'] > 0.3153:
        improvement = (test_turn_metrics['aupr'] - 0.3153) / 0.3153 * 100
        print(f"   âœ… æå‡: +{improvement:.1f}%")
    else:
        decline = (0.3153 - test_turn_metrics['aupr']) / 0.3153 * 100
        print(f"   âš ï¸  ä¸‹é™: -{decline:.1f}%")
    
    print("="*70 + "\n")


def plot_training_curves(history, save_dir):
    """ç»˜åˆ¶è®­ç»ƒæ›²çº¿"""
    fig, axes = plt.subplots(2, 3, figsize=(18, 12))
    
    epochs = range(1, len(history['train_total_loss']) + 1)
    
    # æ€»æŸå¤±
    axes[0, 0].plot(epochs, history['train_total_loss'], 'b-', label='Total Loss')
    axes[0, 0].set_xlabel('Epoch')
    axes[0, 0].set_ylabel('Loss')
    axes[0, 0].set_title('Training Total Loss')
    axes[0, 0].legend()
    axes[0, 0].grid(alpha=0.3)
    
    # ç¦»èŒæŸå¤±
    axes[0, 1].plot(epochs, history['train_turnover_loss'], 'r-', label='Turnover Loss')
    axes[0, 1].set_xlabel('Epoch')
    axes[0, 1].set_ylabel('Loss')
    axes[0, 1].set_title('Training Turnover Loss')
    axes[0, 1].legend()
    axes[0, 1].grid(alpha=0.3)
    
    # åå¥½æŸå¤±
    axes[0, 2].plot(epochs, history['train_preference_loss'], 'g-', label='Preference Loss')
    axes[0, 2].set_xlabel('Epoch')
    axes[0, 2].set_ylabel('Loss')
    axes[0, 2].set_title('Training Preference Loss')
    axes[0, 2].legend()
    axes[0, 2].grid(alpha=0.3)
    
    # Val AUPR
    axes[1, 0].plot(epochs, history['val_aupr'], 'b-', label='Val AUPR')
    axes[1, 0].axhline(y=0.3153, color='r', linestyle='--', label='å•ä»»åŠ¡GCN')
    axes[1, 0].set_xlabel('Epoch')
    axes[1, 0].set_ylabel('AUPR')
    axes[1, 0].set_title('Validation AUPR')
    axes[1, 0].legend()
    axes[1, 0].grid(alpha=0.3)
    
    # Val NDCG
    axes[1, 1].plot(epochs, history['val_ndcg3'], 'r-', label='NDCG@3')
    axes[1, 1].plot(epochs, history['val_ndcg5'], 'g-', label='NDCG@5')
    axes[1, 1].set_xlabel('Epoch')
    axes[1, 1].set_ylabel('NDCG')
    axes[1, 1].set_title('Validation NDCG')
    axes[1, 1].legend()
    axes[1, 1].grid(alpha=0.3)
    
    # Learning Rate
    axes[1, 2].plot(epochs, history['lr'], 'k-', label='Learning Rate')
    axes[1, 2].set_xlabel('Epoch')
    axes[1, 2].set_ylabel('LR')
    axes[1, 2].set_title('Learning Rate Schedule')
    axes[1, 2].set_yscale('log')
    axes[1, 2].legend()
    axes[1, 2].grid(alpha=0.3)
    
    plt.tight_layout()
    plt.savefig(save_dir / 'training_curves.png', dpi=300, bbox_inches='tight')
    plt.close()
    
    print(f"   âœ“ è®­ç»ƒæ›²çº¿å·²ä¿å­˜: {save_dir / 'training_curves.png'}")


if __name__ == '__main__':
    main()
