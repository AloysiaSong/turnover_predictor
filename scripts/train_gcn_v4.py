"""
GCN è®­ç»ƒè„šæœ¬ v4
================
ç›®æ ‡:
1. ä½¿ç”¨GCNç¼–ç å‘˜å·¥èŠ‚ç‚¹ç‰¹å¾
2. å°†å‘˜å·¥embeddingä¸å½“å‰å²—ä½embeddingèåˆ
3. é€šè¿‡ä¸¤å±‚MLPå®Œæˆç¦»èŒé¢„æµ‹
4. é»˜è®¤ä½¿ç”¨å¸¦pos_weightçš„BCEWithLogitsLossï¼Œå¯é€‰Focal Loss
"""

import argparse
import json
from datetime import datetime
from pathlib import Path
from typing import Dict, Optional, Tuple
import sys

import numpy as np
import pandas as pd
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.optim import Adam
from torch.optim.lr_scheduler import ReduceLROnPlateau
from torch_geometric.data import Data

# ç¡®ä¿å¯ä»¥å¯¼å…¥ src æ¨¡å—
REPO_ROOT = Path(__file__).resolve().parents[1]
if str(REPO_ROOT) not in sys.path:
    sys.path.insert(0, str(REPO_ROOT))

from src.models.gcn import drop_edges
from src.models.trainer import compute_metrics


def set_seed(seed: int):
    """ç»Ÿä¸€è®¾ç½®éšæœºç§å­"""
    torch.manual_seed(seed)
    np.random.seed(seed)
    if torch.cuda.is_available():
        torch.cuda.manual_seed(seed)
        torch.cuda.manual_seed_all(seed)
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False


class FocalLoss(nn.Module):
    """Sigmoidç‰ˆæœ¬çš„Focal Lossï¼Œå…¼å®¹pos_weight"""

    def __init__(
        self,
        alpha: Optional[float] = 0.25,
        gamma: float = 2.0,
        pos_weight: Optional[torch.Tensor] = None,
    ):
        super().__init__()
        self.alpha = alpha
        self.gamma = gamma
        if pos_weight is not None:
            self.register_buffer("pos_weight", pos_weight)
        else:
            self.pos_weight = None

    def forward(self, logits: torch.Tensor, targets: torch.Tensor) -> torch.Tensor:
        """
        Args:
            logits: [N] raw logits
            targets: [N] binary labels (float)
        """
        bce = F.binary_cross_entropy_with_logits(
            logits,
            targets,
            pos_weight=self.pos_weight,
            reduction="none",
        )
        probs = torch.sigmoid(logits)
        pt = torch.where(targets == 1, probs, 1 - probs)
        focal_weight = (1 - pt) ** self.gamma
        if self.alpha is not None:
            alpha_t = torch.where(targets == 1, self.alpha, 1 - self.alpha)
            focal_weight = focal_weight * alpha_t
        loss = bce * focal_weight
        return loss.mean()


class EmployeeGCNEncoder(nn.Module):
    """è¿”å›å‘˜å·¥embeddingçš„GCNç¼–ç å™¨"""

    def __init__(
        self,
        in_channels: int,
        hidden_channels: int = 128,  # å¢å¤§éšè—å±‚ç»´åº¦
        num_layers: int = 3,         # å¢åŠ å±‚æ•°
        dropout: float = 0.6,       # å¢å¤§dropout
        edge_dropout: float = 0.2,  # å¢åŠ è¾¹dropout
        feature_dropout: float = 0.3
    ):
        super().__init__()
        if num_layers < 1:
            raise ValueError("num_layers must be >= 1")

        self.num_layers = num_layers
        self.hidden_channels = hidden_channels
        self.dropout = dropout
        self.edge_dropout = edge_dropout
        self.feat_dropout = nn.Dropout(feature_dropout) if feature_dropout > 0 else nn.Identity()

        from torch_geometric.nn import GCNConv  # lazy import to avoid global dependency issues

        self.convs = nn.ModuleList()
        self.convs.append(GCNConv(in_channels, hidden_channels))
        for _ in range(num_layers - 1):
            self.convs.append(GCNConv(hidden_channels, hidden_channels))

    @property
    def out_channels(self) -> int:
        return self.hidden_channels

    def forward(self, x: torch.Tensor, edge_index: torch.Tensor) -> torch.Tensor:
        x = self.feat_dropout(x)
        for i, conv in enumerate(self.convs):
            if self.training and self.edge_dropout > 0.0:
                edge_index = drop_edges(edge_index, self.edge_dropout, x.size(0), training=True)
            x = conv(x, edge_index)
            if i < self.num_layers - 1:
                x = F.relu(x)
                x = F.dropout(x, p=self.dropout, training=self.training)
        return x


class EmployeeJobFusionModel(nn.Module):
    """
    ç»„åˆæ¨¡å‹:
    1. ä½¿ç”¨GCNè·å–å‘˜å·¥embedding
    2. æŸ¥è¡¨è·å¾—å½“å‰å²—ä½embedding
    3. æ‹¼æ¥åé€å…¥åŒå±‚MLPè¾“å‡ºç¦»èŒlogit
    """

    def __init__(
        self,
        encoder: EmployeeGCNEncoder,
        num_jobs: int,
        job_embed_dim: int = 32,
        mlp_hidden_dim: int = 128,
        mlp_dropout: float = 0.5,
    ):
        super().__init__()
        self.encoder = encoder
        self.job_embedding = nn.Embedding(num_jobs, job_embed_dim)

        fused_dim = encoder.out_channels + job_embed_dim
        self.classifier = nn.Sequential(
            nn.Linear(fused_dim, mlp_hidden_dim),
            nn.ReLU(),
            nn.Dropout(mlp_dropout),
            nn.Linear(mlp_hidden_dim, 1),
        )

    def forward(
        self,
        x: torch.Tensor,
        edge_index: torch.Tensor,
        job_indices: torch.Tensor,
    ) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:
        employee_emb = self.encoder(x, edge_index)
        job_emb = self.job_embedding(job_indices)
        fused = torch.cat([employee_emb, job_emb], dim=-1)
        logits = self.classifier(fused).squeeze(-1)
        return logits, employee_emb, job_emb


def detect_job_column(df: pd.DataFrame) -> Optional[str]:
    """
    ç®€å•å¯å‘å¼: æŸ¥æ‰¾åŒ…å«å…³é”®å­—çš„åˆ—å
    ä¼˜å…ˆçº§: 'ç°å²—ä½' > 'å½“å‰å²—ä½' > 'å²—ä½ç±»å‹' > 'Q3'/'Q4'ç­‰
    """
    candidates = []
    keywords = ["ç°å²—ä½", "å½“å‰å²—ä½", "å²—ä½ç±»å‹", "å²—ä½ç±»åˆ«"]
    for kw in keywords:
        matches = [col for col in df.columns if kw in col]
        candidates.extend(matches)

    if candidates:
        return candidates[0]

    fallback = [col for col in df.columns if col.startswith(("Q2", "Q3", "Q4")) and "_" not in col]
    return fallback[0] if fallback else None


def _normalize_multiselect_value(val) -> int:
    if pd.isna(val):
        return 0
    s = str(val).strip()
    if s == "":
        return 0
    positives = {"1", "æ˜¯", "æœ‰", "yes", "Yes", "TRUE", "True", "true", "å‹¾é€‰", "é€‰æ‹©", "on"}
    negatives = {"0", "å¦", "æ— ", "no", "No", "FALSE", "False", "false"}
    if s in positives:
        return 1
    if s in negatives:
        return 0
    # æ–°å¢: å¤„ç†å¤šé€‰å­—æ®µçš„å¤šä¸ªå€¼ï¼ˆå¦‚"1,2,3"ï¼‰
    if "," in s:
        parts = [part.strip() for part in s.split(",")]
        if any(part in positives for part in parts):
            return 1
        if all(part in negatives for part in parts if part):
            return 0
    try:
        return 1 if float(s) > 0 else 0
    except ValueError:
        return 0


def _is_multiselect_job_col(col_name: str) -> bool:
    text = str(col_name)
    normalized = (
        text.replace("\ufeff", "")
        .replace("ï¼ˆ", "(")
        .replace("ï¼‰", ")")
        .replace("ï¼š", ":")
        .strip()
    )
    if ("å²—ä½å±äº" in normalized or "å²—ä½é¡" in normalized or "å²—ä½ç±»" in normalized) and (
        "å¤šé€‰" in normalized or "å¤šé¸" in normalized or "å¯å¤šé€‰" in normalized or "-" in normalized
    ):
        return True
    return False


def _load_multiselect_jobs(
    df: pd.DataFrame,
    job_cols: list,
    raw_header_map: Optional[Dict[str, str]] = None
) -> Tuple[np.ndarray, Dict[str, int]]:
    option_names = []
    for col in job_cols:
        source_name = raw_header_map.get(col, col) if raw_header_map else col
        if "-" in source_name:
            option_names.append(source_name.split("-", 1)[-1].strip())
        else:
            option_names.append(str(source_name).strip())

    values = df[job_cols].applymap(_normalize_multiselect_value).values
    num_rows = values.shape[0]
    job_indices = np.zeros(num_rows, dtype=np.int64)

    for idx, row in enumerate(values):
        if row.sum() == 0:
            job_indices[idx] = len(option_names)  # unknown bucket
        else:
            job_indices[idx] = int(np.argmax(row))

    mapping = {name: i for i, name in enumerate(option_names)}
    mapping["æœªæ ‡æ³¨"] = len(option_names)

    return job_indices, mapping


def load_job_indices(
    raw_csv: Path,
    num_nodes: int,
    job_column: Optional[str] = None,
    encoding: str = "gbk",
    skiprows: int = 1,
) -> Tuple[torch.LongTensor, Dict[str, int]]:
    """
    ä»åŸå§‹CSVä¸­æå–å½“å‰å²—ä½ç´¢å¼•
    """
    df = pd.read_csv(raw_csv, encoding=encoding, skiprows=skiprows)
    if not df.empty and df.iloc[0].astype(str).tolist() == df.columns.astype(str).tolist():
        df = df.iloc[1:].reset_index(drop=True)
    if len(df) < num_nodes:
        raise ValueError(f"åŸå§‹CSVæ ·æœ¬æ•°({len(df)})å°‘äºå›¾èŠ‚ç‚¹æ•°({num_nodes})")
    if len(df) > num_nodes:
        df = df.iloc[:num_nodes].copy()

    # ä¼˜å…ˆä½¿ç”¨ç”¨æˆ·æŒ‡å®šåˆ—
    if job_column is not None:
        column = job_column
        if column not in df.columns:
            raise ValueError(f"æŒ‡å®šçš„å²—ä½åˆ— {column} ä¸å­˜åœ¨äºCSVä¸­")
        series = df[column].fillna("æœªçŸ¥å²—ä½")
        if series.dtype.kind in {"i", "u"}:
            codes = series.astype(int).values
            unique_vals = np.unique(codes)
            mapping = {str(val): idx for idx, val in enumerate(sorted(unique_vals))}
            remapped = np.vectorize(lambda v: mapping[str(v)])(codes)
        else:
            categories = series.astype("category")
            remapped = categories.cat.codes.values
            mapping = {
                str(cat): int(code)
                for cat, code in zip(categories.cat.categories, range(len(categories.cat.categories)))
            }
        remapped = remapped.astype(np.int64)
        if remapped.min() < 0:
            raise ValueError("å²—ä½ç¼–ç å­˜åœ¨ç¼ºå¤±ï¼Œæ— æ³•è½¬æ¢ï¼Œè¯·æ£€æŸ¥åŸå§‹CSV")
        return torch.from_numpy(remapped), mapping

    # è¯»å–åŸå§‹é¦–è¡Œåˆ—åï¼ˆç”¨äºå‹å¥½æ˜¾ç¤ºï¼‰
    raw_header_map: Dict[str, str] = {}
    try:
        import csv

        with open(raw_csv, "r", encoding=encoding) as f:
            reader = csv.reader(f)
            original_header = next(reader)
        if len(original_header) == len(df.columns):
            raw_header_map = dict(zip(df.columns, original_header))
    except Exception:
        raw_header_map = {}

    # è°ƒè¯•ä¿¡æ¯ï¼šå±•ç¤ºéƒ¨åˆ†åˆ—å
    sample_cols = list(df.columns[:10])
    print("\n   ğŸ” CSVåˆ—åæ ·ä¾‹:")
    for col in sample_cols:
        print(f"      â€¢ {repr(col)}")
    
    job_keyword_cols = [col for col in df.columns if "å²—ä½" in str(col)]
    if job_keyword_cols:
        print(f"\n   ğŸ” å«â€œå²—ä½â€çš„åˆ—æ•°é‡: {len(job_keyword_cols)}")
        for col in job_keyword_cols[:10]:
            print(f"      â€¢ {repr(col)}")
        if len(job_keyword_cols) > 10:
            print(f"      ... å…± {len(job_keyword_cols)} åˆ—")
    else:
        print("\n   ğŸ” æœªå‘ç°åŒ…å«â€œå²—ä½â€çš„åˆ—")

    # ä¼˜å…ˆä½¿ç”¨ Q5 å¤šé€‰åˆ—ï¼ˆå²—ä½ç±»åˆ«ï¼‰
    q5_cols = [col for col in df.columns if str(col).startswith("Q5_")]
    if q5_cols:
        q5_cols = list(dict.fromkeys(q5_cols))
        print(f"\n   ğŸ§© æ£€æµ‹åˆ° Q5 å¤šé€‰åˆ—ï¼ˆå²—ä½ç±»åˆ«ï¼‰: {len(q5_cols)} ä¸ª")
        for col in q5_cols[:10]:
            display_name = raw_header_map.get(col, col)
            print(f"      - {col} ({display_name})")
        job_indices, mapping = _load_multiselect_jobs(df, q5_cols, raw_header_map)
        return torch.from_numpy(job_indices), mapping

    # å…¶æ¬¡å°è¯•å¤šé€‰åˆ—
    multiselect_cols = [col for col in df.columns if _is_multiselect_job_col(col)]
    print(f"\n   ğŸ” å¤šé€‰åˆ—åŒ¹é…æ•°é‡: {len(multiselect_cols)}")
    if multiselect_cols:
        multiselect_cols = list(dict.fromkeys(multiselect_cols))  # å»é‡å¹¶ä¿æŒé¡ºåº
        print("\n   ğŸ§© æ£€æµ‹åˆ°å²—ä½å¤šé€‰åˆ—:")
        for col in multiselect_cols[:10]:
            display_name = raw_header_map.get(col, col)
            print(f"      - {col} ({display_name})")
        if len(multiselect_cols) > 10:
            print(f"      ... å…± {len(multiselect_cols)} åˆ—")

        job_indices, mapping = _load_multiselect_jobs(df, multiselect_cols, raw_header_map)
        return torch.from_numpy(job_indices), mapping

    # å¦åˆ™å›é€€åˆ°å¯å‘å¼å•åˆ—
    column = detect_job_column(df)
    if column is None:
        raise ValueError("æœªæ‰¾åˆ°å²—ä½åˆ—ï¼Œè¯·é€šè¿‡ --job-column æŒ‡å®š")
    else:
        display_name = raw_header_map.get(column, column)
        print(f"\n   âš ï¸ æœªæ£€æµ‹åˆ°å²—ä½å¤šé€‰åˆ—ï¼Œå°†ä½¿ç”¨å•åˆ— `{column}` ({display_name}) ä½œä¸ºå²—ä½ç±»åˆ«æ¥æº")

    series = df[column].fillna("æœªçŸ¥å²—ä½")
    if series.dtype.kind in {"i", "u"}:
        codes = series.astype(int).values
        unique_vals = np.unique(codes)
        mapping = {str(val): idx for idx, val in enumerate(sorted(unique_vals))}
        remapped = np.vectorize(lambda v: mapping[str(v)])(codes)
    else:
        
        categories = series.astype("category")
        remapped = categories.cat.codes.values
        mapping = {
            str(cat): int(code)
            for cat, code in zip(categories.cat.categories, range(len(categories.cat.categories)))
        }

    remapped = remapped.astype(np.int64)
    if remapped.min() < 0:
        raise ValueError("å²—ä½ç¼–ç å­˜åœ¨ç¼ºå¤±ï¼Œæ— æ³•è½¬æ¢ï¼Œè¯·æ£€æŸ¥åŸå§‹CSV")

    return torch.from_numpy(remapped), mapping


def print_config(args, job_mapping: Dict[str, int], data: Data):
    """æ‰“å°è®­ç»ƒé…ç½®"""
    print("\n" + "=" * 70)
    print("ğŸ“‹ è®­ç»ƒé…ç½® v4")
    print("=" * 70)
    print("\næ•°æ®:")
    print(f"   data_path: {args.data_path}")
    print(f"   raw_csv: {args.raw_csv}")
    print(f"   èŠ‚ç‚¹: {data.x.size(0)}")
    print(f"   ç‰¹å¾: {data.x.size(1)}")
    print(f"   è¾¹: {data.edge_index.size(1)}")
    print(f"   è®­ç»ƒ/éªŒè¯/æµ‹è¯•: {data.train_mask.sum().item()}/{data.val_mask.sum().item()}/{data.test_mask.sum().item()}")

    print("\nå²—ä½æ˜ å°„:")
    for name, idx in sorted(job_mapping.items(), key=lambda kv: kv[1]):
        print(f"   {idx:2d} â†” {name}")

    print("\næ¨¡å‹:")
    print(f"   hidden_dim: {args.hidden_dim}")
    print(f"   num_layers: {args.num_layers}")
    print(f"   dropout: {args.dropout}")
    print(f"   edge_dropout: {args.edge_dropout}")
    print(f"   feature_dropout: {args.feature_dropout}")
    print(f"   job_embed_dim: {args.job_embed_dim}")
    print(f"   mlp_hidden_dim: {args.mlp_hidden_dim}")

    print("\nè®­ç»ƒ:")
    print(f"   lr: {args.lr}")
    print(f"   weight_decay: {args.weight_decay}")
    print(f"   epochs: {args.epochs}")
    print(f"   patience: {args.patience}")
    print(f"   device: {args.device}")
    print(f"   use_focal_loss: {args.use_focal_loss} (alpha={args.focal_alpha}, gamma={args.focal_gamma})")
    print("=" * 70)


@torch.no_grad()
def evaluate_split(
    model: EmployeeJobFusionModel,
    data: Data,
    job_indices: torch.Tensor,
    mask: torch.Tensor,
    loss_fn: nn.Module,
) -> Dict[str, float]:
    model.eval()
    mask = mask.bool()
    logits, _, _ = model(data.x, data.edge_index, job_indices)
    logits = logits[mask]
    labels = data.y[mask].float()
    loss = loss_fn(logits, labels).item()
    probs = torch.sigmoid(logits).cpu().numpy()
    metrics = compute_metrics(labels.cpu().numpy(), probs, threshold=0.5)
    metrics.update({"loss": loss})
    return metrics


def train(
    model: EmployeeJobFusionModel,
    data: Data,
    job_indices: torch.Tensor,
    optimizer: Adam,
    scheduler: ReduceLROnPlateau,
    criterion: nn.Module,
    bce_loss: nn.Module,
    args,
    save_dir: Path,
):
    history = {
        "train_loss": [],
        "val_loss": [],
        "val_f1": [],
        "val_recall": [],
        "lr": [],
    }

    best_state = None
    best_val_loss = float("inf")
    best_val_f1 = 0.0
    patience_counter = 0

    train_mask = data.train_mask.bool()
    val_mask = data.val_mask.bool()
    labels = data.y.float()

    for epoch in range(1, args.epochs + 1):
        model.train()
        optimizer.zero_grad()

        logits, _, _ = model(data.x, data.edge_index, job_indices)
        train_logits = logits[train_mask]
        train_labels = labels[train_mask]

        loss = criterion(train_logits, train_labels)
        loss.backward()
        nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
        optimizer.step()

        # éªŒè¯
        val_metrics = evaluate_split(model, data, job_indices, val_mask, bce_loss)
        scheduler.step(val_metrics["loss"])

        history["train_loss"].append(loss.item())
        history["val_loss"].append(val_metrics["loss"])
        history["val_f1"].append(val_metrics["f1"])
        history["val_recall"].append(val_metrics["recall"])
        history["lr"].append(optimizer.param_groups[0]["lr"])

        print(
            f"Epoch {epoch:03d} | "
            f"Train Loss: {loss.item():.4f} | "
            f"Val Loss: {val_metrics['loss']:.4f} | "
            f"Val F1: {val_metrics['f1']:.4f} | "
            f"Val Recall: {val_metrics['recall']:.4f} | "
            f"LR: {optimizer.param_groups[0]['lr']:.6f}"
        )

        improved = False
        if val_metrics["loss"] < best_val_loss - 1e-4:
            improved = True
        elif abs(val_metrics["loss"] - best_val_loss) <= 1e-4 and val_metrics["f1"] > best_val_f1:
            improved = True

        if improved:
            best_val_loss = val_metrics["loss"]
            best_val_f1 = val_metrics["f1"]
            best_state = {
                "model": model.state_dict(),
                "optimizer": optimizer.state_dict(),
                "scheduler": scheduler.state_dict(),
                "epoch": epoch,
                "val_metrics": val_metrics,
            }
            patience_counter = 0
            torch.save(best_state, save_dir / "best_model.pt")
            print(
                f"   â­ ä¿å­˜æœ€ä½³æ¨¡å‹ (epoch={epoch}, val_loss={best_val_loss:.4f}, val_f1={best_val_f1:.4f})"
            )
        else:
            patience_counter += 1
            if patience_counter >= args.patience:
                print(f"   ğŸ›‘ æ—©åœè§¦å‘ï¼Œepoch={epoch}")
                break

    if best_state is None:
        best_state = {
            "model": model.state_dict(),
            "optimizer": optimizer.state_dict(),
            "scheduler": scheduler.state_dict(),
            "epoch": args.epochs,
            "val_metrics": evaluate_split(model, data, job_indices, val_mask, bce_loss),
        }
        torch.save(best_state, save_dir / "best_model.pt")

    model.load_state_dict(best_state["model"])
    return history, best_state


def parse_args():
    parser = argparse.ArgumentParser(description="GCN v4 - ç¦»èŒé¢„æµ‹ï¼ˆå²—ä½èåˆï¼‰")

    # æ•°æ®
    parser.add_argument("--data-path", type=str, default="data/processed/homo_graph.pt")
    parser.add_argument("--raw-csv", type=str, default="data/raw/originaldata.csv")
    parser.add_argument("--job-column", type=str, default=None, help="åŸå§‹CSVä¸­çš„å²—ä½åˆ—åç§°")
    parser.add_argument("--encoding", type=str, default="gbk")
    parser.add_argument("--skiprows", type=int, default=1)

    # æ¨¡å‹
    parser.add_argument("--hidden-dim", type=int, default=64)
    parser.add_argument("--num-layers", type=int, default=2)
    parser.add_argument("--dropout", type=float, default=0.5)
    parser.add_argument("--edge-dropout", type=float, default=0.0)
    parser.add_argument("--feature-dropout", type=float, default=0.0)
    parser.add_argument("--job-embed-dim", type=int, default=32)
    parser.add_argument("--mlp-hidden-dim", type=int, default=128)

    # è®­ç»ƒ
    parser.add_argument("--lr", type=float, default=0.01)
    parser.add_argument("--weight-decay", type=float, default=5e-4)
    parser.add_argument("--epochs", type=int, default=200)
    parser.add_argument("--patience", type=int, default=20)
    parser.add_argument("--device", type=str, default="cpu", choices=["cpu", "cuda"])
    parser.add_argument("--seed", type=int, default=42)
    parser.add_argument("--save-dir", type=str, default="outputs/models")
    parser.add_argument("--eval-dir", type=str, default="outputs/evaluation")

    # Loss
    parser.add_argument("--use-focal-loss", action="store_true")
    parser.add_argument("--focal-alpha", type=float, default=0.25)
    parser.add_argument("--focal-gamma", type=float, default=2.0)

    return parser.parse_args()


def main():
    args = parse_args()
    set_seed(args.seed)

    device = torch.device(args.device if torch.cuda.is_available() or args.device == "cpu" else "cpu")

    data: Data = torch.load(args.data_path)
    data = data.to(device)
    if data.y.dim() > 1:
        data.y = data.y.view(-1)

    job_indices, job_mapping = load_job_indices(
        Path(args.raw_csv),
        num_nodes=data.x.size(0),
        job_column=args.job_column,
        encoding=args.encoding,
        skiprows=args.skiprows,
    )
    job_indices = job_indices.to(device)
    num_jobs = int(job_indices.max().item() + 1)

    print_config(args, job_mapping, data)

    run_id = datetime.now().strftime("run_%Y%m%d_%H%M%S")
    print(f"\nğŸ†” Run ID: {run_id}")
    save_dir = Path(args.save_dir) / run_id
    eval_dir = Path(args.eval_dir) / run_id
    save_dir.mkdir(parents=True, exist_ok=True)
    eval_dir.mkdir(parents=True, exist_ok=True)

    encoder = EmployeeGCNEncoder(
        in_channels=data.num_features,
        hidden_channels=args.hidden_dim,
        num_layers=args.num_layers,
        dropout=args.dropout,
        edge_dropout=args.edge_dropout,
        feature_dropout=args.feature_dropout,
    )
    model = EmployeeJobFusionModel(
        encoder=encoder,
        num_jobs=num_jobs,
        job_embed_dim=args.job_embed_dim,
        mlp_hidden_dim=args.mlp_hidden_dim,
        mlp_dropout=args.dropout,
    ).to(device)

    num_pos = data.y[data.train_mask].sum().item()
    num_total = data.train_mask.sum().item()
    pos_weight_value = (num_total - num_pos) / num_pos if num_pos > 0 else 1.0
    pos_weight = torch.tensor([pos_weight_value], device=device)

    bce_loss = nn.BCEWithLogitsLoss(pos_weight=pos_weight)
    if args.use_focal_loss:
        criterion = FocalLoss(alpha=args.focal_alpha, gamma=args.focal_gamma, pos_weight=pos_weight)
    else:
        criterion = bce_loss

    optimizer = Adam(model.parameters(), lr=args.lr, weight_decay=args.weight_decay)
    scheduler = ReduceLROnPlateau(optimizer, mode="min", factor=0.5, patience=5, verbose=True)

    history, best_state = train(
        model=model,
        data=data,
        job_indices=job_indices,
        optimizer=optimizer,
        scheduler=scheduler,
        criterion=criterion,
        bce_loss=bce_loss,
        args=args,
        save_dir=save_dir,
    )

    # æœ€ä½³æ¨¡å‹è¯„ä¼°
    val_metrics = evaluate_split(model, data, job_indices, data.val_mask, bce_loss)
    test_metrics = evaluate_split(model, data, job_indices, data.test_mask, bce_loss)

    print("\n" + "=" * 70)
    print("âœ… è®­ç»ƒå®Œæˆ (GCN v4)")
    print("=" * 70)
    print(f"æœ€ä½³Epoch: {best_state['epoch']}")
    print(f"éªŒè¯é›† - Loss: {val_metrics['loss']:.4f}, F1: {val_metrics['f1']:.4f}, Recall: {val_metrics['recall']:.4f}")
    print(f"æµ‹è¯•é›† -  Loss: {test_metrics['loss']:.4f}, F1: {test_metrics['f1']:.4f}, Recall: {test_metrics['recall']:.4f}")
    print("=" * 70)

    # ä¿å­˜å†å²
    with open(save_dir / "training_history.json", "w") as f:
        json.dump(history, f, indent=2)

    results = {
        "run_id": run_id,
        "config": vars(args),
        "best_epoch": best_state["epoch"],
        "val_metrics": val_metrics,
        "test_metrics": test_metrics,
        "pos_weight": pos_weight_value,
        "num_jobs": num_jobs,
        "job_mapping": job_mapping,
    }
    with open(eval_dir / "results.json", "w") as f:
        json.dump(results, f, indent=2)

    with open(eval_dir / "job_mapping.json", "w") as f:
        json.dump(job_mapping, f, ensure_ascii=False, indent=2)

    print(f"\nğŸ’¾ æœ€ä½³æ¨¡å‹å·²ä¿å­˜è‡³: {save_dir / 'best_model.pt'}")
    print(f"ğŸ“Š è¯„ä¼°ç»“æœå·²ä¿å­˜è‡³: {eval_dir / 'results.json'}")


if __name__ == "__main__":
    main()
