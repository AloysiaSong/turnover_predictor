"""
å¯¹æ¯”ä¸åŒæ·±åº¦çš„GCNæ¶æ„
shallow, default, deep, very_deep
"""

import torch
import sys
sys.path.insert(0, 'src/models')

from gcn import create_gcn_model
from trainer import quick_train
from evaluator import GCNEvaluator
import json


def train_and_evaluate(architecture, data, device='cpu'):
    """è®­ç»ƒå¹¶è¯„ä¼°æŒ‡å®šæ¶æ„"""
    print("\n" + "="*70)
    print(f"è®­ç»ƒæ¶æ„: {architecture.upper()}")
    print("="*70)
    
    # åˆ›å»ºæ¨¡å‹
    model = create_gcn_model(
        in_channels=data.num_node_features,
        architecture=architecture,
        dropout=0.5
    )
    
    num_params = sum(p.numel() for p in model.parameters())
    print(f"å‚æ•°é‡: {num_params:,}")
    
    # è®­ç»ƒ
    trainer, history = quick_train(
        model=model,
        data=data,
        epochs=200,
        lr=0.01,
        weight_decay=5e-4,
        early_stopping_patience=20,
        device=device,
        save_dir=f'outputs/models_{architecture}'
    )
    
    # è¯„ä¼°
    evaluator = GCNEvaluator(model, data, device=device)
    results = evaluator.full_evaluation(
        save_dir=f'outputs/evaluation_{architecture}'
    )
    
    # è®­ç»ƒæ›²çº¿
    evaluator.plot_training_curves(
        history,
        save_path=f'outputs/evaluation_{architecture}/training_curves.png'
    )
    
    return {
        'architecture': architecture,
        'num_params': num_params,
        'best_epoch': trainer.best_epoch + 1,
        'best_val_f1': max(history['val_f1']),
        'test_acc': results['test']['accuracy'],
        'test_precision': results['test']['precision'],
        'test_recall': results['test']['recall'],
        'test_f1': results['test']['f1'],
        'test_auc_roc': results['test']['roc_auc'],
        'test_auc_pr': results['test']['pr_auc']
    }


print("\n" + "="*70)
print("ğŸ—ï¸ GCNæ¶æ„å¯¹æ¯”å®éªŒ")
print("="*70)

# åŠ è½½æ•°æ®
print("\nåŠ è½½æ•°æ®...")
data = torch.load('data/processed/homo_graph.pt')
print(f"âœ“ èŠ‚ç‚¹: {data.num_nodes}, è¾¹: {data.num_edges}")

# æµ‹è¯•æ‰€æœ‰æ¶æ„
architectures = ['shallow', 'default', 'deep', 'very_deep']
all_results = []

for arch in architectures:
    result = train_and_evaluate(arch, data, device='cpu')
    all_results.append(result)

# å¯¹æ¯”æ€»ç»“
print("\n" + "="*70)
print("ğŸ“Š æ¶æ„å¯¹æ¯”æ€»ç»“")
print("="*70)

print(f"\n{'æ¶æ„':<12} {'å‚æ•°é‡':<10} {'Best Epoch':<12} {'Test F1':<10} {'Test Recall':<12} {'æ’å'}")
print("="*70)

# æŒ‰F1æ’åº
sorted_results = sorted(all_results, key=lambda x: x['test_f1'], reverse=True)

for rank, result in enumerate(sorted_results, 1):
    marker = "â­" if rank == 1 else ""
    print(f"{result['architecture']:<12} {result['num_params']:<10,} "
          f"{result['best_epoch']:<12} {result['test_f1']:<10.4f} "
          f"{result['test_recall']:<12.4f} #{rank} {marker}")

print("="*70)

# è¯¦ç»†å¯¹æ¯”è¡¨
print(f"\nè¯¦ç»†æŒ‡æ ‡å¯¹æ¯”:")
print("="*90)
print(f"{'æ¶æ„':<12} {'F1':<8} {'Recall':<10} {'Precision':<12} {'AUC-ROC':<10} {'AUC-PR':<10}")
print("="*90)

for result in sorted_results:
    print(f"{result['architecture']:<12} {result['test_f1']:<8.4f} "
          f"{result['test_recall']:<10.4f} {result['test_precision']:<12.4f} "
          f"{result['test_auc_roc']:<10.4f} {result['test_auc_pr']:<10.4f}")

print("="*90)

# ä¿å­˜å¯¹æ¯”ç»“æœ
comparison_file = 'outputs/architecture_comparison.json'
with open(comparison_file, 'w') as f:
    json.dump(all_results, f, indent=2)

print(f"\nâœ“ å¯¹æ¯”ç»“æœå·²ä¿å­˜: {comparison_file}")

# æ¨è
best = sorted_results[0]
print(f"\nğŸ† æ¨èæ¶æ„: {best['architecture'].upper()}")
print(f"   F1-Score:  {best['test_f1']:.4f}")
print(f"   Recall:    {best['test_recall']:.4f}")
print(f"   å‚æ•°é‡:    {best['num_params']:,}")

print("\n" + "="*70)
print("âœ… æ¶æ„å¯¹æ¯”å®Œæˆï¼")
print("="*70)
