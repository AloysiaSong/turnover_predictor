"""
GCNå®Œæ•´è®­ç»ƒè„šæœ¬

ä¸€é”®è¿è¡Œå®Œæ•´æµç¨‹:
1. åŠ è½½æ•°æ®
2. åˆ›å»ºæ¨¡å‹
3. è®­ç»ƒæ¨¡å‹
4. è¯„ä¼°æ€§èƒ½
5. ç”ŸæˆæŠ¥å‘Š
"""

import torch
import sys
from pathlib import Path


# æ·»åŠ src/modelsåˆ°è·¯å¾„
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root / 'src' / 'models'))

from gcn import create_gcn_model
from trainer2 import quick_train
from evaluator2 import GCNEvaluator

import json


def main():
    """ä¸»è®­ç»ƒæµç¨‹"""
    print("\n" + "="*70)
    print("ğŸš€ GCNå®Œæ•´è®­ç»ƒæµç¨‹")
    print("="*70)
    
    # ========== é…ç½® ==========
    config = {
        'data_path': 'data/processed/homo_graph.pt',
        'architecture': 'default',  # shallow, default, deep, very_deep
        'dropout': 0.5,
        'lr': 0.01,
        'weight_decay': 5e-4,
        'epochs': 200,
        'early_stopping_patience': 20,
        'device': 'cpu',  # 'cuda' if torch.cuda.is_available() else 'cpu'
        'save_dir': 'outputs/models',
        'eval_dir': 'outputs/evaluation'
    }
    
    print("\nğŸ“‹ è®­ç»ƒé…ç½®:")
    for key, value in config.items():
        print(f"   {key}: {value}")
    
    # ========== Step 1: åŠ è½½æ•°æ® ==========
    print("\n" + "="*70)
    print("ğŸ“‚ Step 1/5: åŠ è½½æ•°æ®")
    print("="*70)
    
    data = torch.load(config['data_path'])
    print(f"\n   âœ“ æ•°æ®åŠ è½½æˆåŠŸ")
    print(f"   âœ“ èŠ‚ç‚¹æ•°: {data.num_nodes}")
    print(f"   âœ“ è¾¹æ•°: {data.num_edges}")
    print(f"   âœ“ ç‰¹å¾ç»´åº¦: {data.num_node_features}")
    print(f"   âœ“ è®­ç»ƒé›†: {data.train_mask.sum()}")
    print(f"   âœ“ éªŒè¯é›†: {data.val_mask.sum()}")
    print(f"   âœ“ æµ‹è¯•é›†: {data.test_mask.sum()}")
    
    # ========== Step 2: åˆ›å»ºæ¨¡å‹ ==========
    print("\n" + "="*70)
    print("ğŸ—ï¸ Step 2/5: åˆ›å»ºæ¨¡å‹")
    print("="*70)
    
    model = create_gcn_model(
        in_channels=data.num_node_features,
        architecture=config['architecture'],
        dropout=config['dropout']
    )
    
    num_params = sum(p.numel() for p in model.parameters())
    print(f"\n   âœ“ æ¨¡å‹åˆ›å»ºæˆåŠŸ")
    print(f"   âœ“ æ¶æ„: {config['architecture']}")
    print(f"   âœ“ å‚æ•°é‡: {num_params:,}")
    
    # ========== Step 3: è®­ç»ƒæ¨¡å‹ ==========
    print("\n" + "="*70)
    print("ğŸ¯ Step 3/5: è®­ç»ƒæ¨¡å‹")
    print("="*70)
    
    trainer, history = quick_train(
        model=model,
        data=data,
        epochs=config['epochs'],
        lr=config['lr'],
        weight_decay=config['weight_decay'],
        early_stopping_patience=config['early_stopping_patience'],
        device=config['device'],
        save_dir=config['save_dir']
    )
    
    # ========== Step 4: æµ‹è¯•é›†è¯„ä¼° ==========
    print("\n" + "="*70)
    print("ğŸ“Š Step 4/5: æµ‹è¯•é›†å¿«é€Ÿè¯„ä¼°")
    print("="*70)
    
    test_loss, test_acc, test_f1 = trainer.evaluate(data.test_mask)
    print(f"\n   æµ‹è¯•Loss: {test_loss:.4f}")
    print(f"   æµ‹è¯•Acc: {test_acc:.4f}")
    print(f"   æµ‹è¯•F1: {test_f1:.4f}")
    
    # ========== Step 5: å®Œæ•´è¯„ä¼° ==========
    print("\n" + "="*70)
    print("ğŸ¨ Step 5/5: å®Œæ•´è¯„ä¼°ä¸å¯è§†åŒ–")
    print("="*70)
    
    evaluator = GCNEvaluator(model, data, device=config['device'])
    results = evaluator.full_evaluation(save_dir=config['eval_dir'])
    
    # ç»˜åˆ¶è®­ç»ƒæ›²çº¿
    print("\nç»˜åˆ¶è®­ç»ƒæ›²çº¿...")
    evaluator.plot_training_curves(
        history,
        save_path=f"{config['eval_dir']}/training_curves.png"
    )
    
    # ========== ç”Ÿæˆæ€»ç»“æŠ¥å‘Š ==========
    print("\n" + "="*70)
    print("ğŸ“„ ç”Ÿæˆæ€»ç»“æŠ¥å‘Š")
    print("="*70)
    
    report = {
        'config': config,
        'training': {
            'best_epoch': trainer.best_epoch + 1,
            'best_val_loss': trainer.best_val_loss,
            'best_val_f1': max(history['val_f1']),
            'total_epochs': len(history['train_loss'])
        },
        'test_results': {
            'accuracy': float(results['test']['accuracy']),
            'precision': float(results['test']['precision']),
            'recall': float(results['test']['recall']),
            'f1': float(results['test']['f1']),
            'roc_auc': float(results['test']['roc_auc']),
            'pr_auc': float(results['test']['pr_auc'])
        }
    }
    
    report_path = Path(config['eval_dir']) / 'training_report.json'
    with open(report_path, 'w') as f:
        json.dump(report, f, indent=2)
    
    print(f"\n   âœ“ æŠ¥å‘Šå·²ä¿å­˜: {report_path}")
    
    # ========== æœ€ç»ˆæ€»ç»“ ==========
    print("\n" + "="*70)
    print("âœ… è®­ç»ƒæµç¨‹å®Œæˆï¼")
    print("="*70)
    
    print("\nğŸ“Š æœ€ç»ˆç»“æœ:")
    print(f"   æµ‹è¯•Accuracy: {results['test']['accuracy']:.4f}")
    print(f"   æµ‹è¯•Precision: {results['test']['precision']:.4f}")
    print(f"   æµ‹è¯•Recall: {results['test']['recall']:.4f}")
    print(f"   æµ‹è¯•F1-Score: {results['test']['f1']:.4f}")
    print(f"   æµ‹è¯•AUC-ROC: {results['test']['roc_auc']:.4f}")
    print(f"   æµ‹è¯•AUC-PR: {results['test']['pr_auc']:.4f}")
    
    print("\nğŸ“ ç”Ÿæˆæ–‡ä»¶:")
    print(f"   æ¨¡å‹: {config['save_dir']}/best_model.pt")
    print(f"   è®­ç»ƒå†å²: {config['save_dir']}/training_history.json")
    print(f"   è¯„ä¼°ç»“æœ: {config['eval_dir']}/metrics.json")
    print(f"   è®­ç»ƒæ›²çº¿: {config['eval_dir']}/training_curves.png")
    print(f"   æ··æ·†çŸ©é˜µ: {config['eval_dir']}/test_confusion_matrix.png")
    print(f"   ROCæ›²çº¿: {config['eval_dir']}/test_roc_curve.png")
    print(f"   PRæ›²çº¿: {config['eval_dir']}/test_pr_curve.png")
    print(f"   æ€»ç»“æŠ¥å‘Š: {config['eval_dir']}/training_report.json")
    
    print("\n" + "="*70)
    print("ğŸ‰ æ­å–œï¼GCNè®­ç»ƒå®Œæˆï¼")
    print("="*70)
    
    return model, results


if __name__ == '__main__':
    model, results = main()
