"""
ç‰¹å¾å·¥ç¨‹ç®¡é“
============
åŠŸèƒ½ï¼š
1. Top-Nè¿ç»­ç‰¹å¾é€‰æ‹©ï¼ˆåŸºäºAUPR/MI/LRç³»æ•°ï¼‰
2. åˆ†ç®± + One-hotç¼–ç 
3. äºŒé˜¶äº¤äº’é¡¹
4. ä¸¥æ ¼é˜²æ­¢æ•°æ®æ³„æ¼ï¼ˆä»…åœ¨Trainä¸Šfitï¼‰
"""

import torch
import numpy as np
import pickle
from typing import List, Tuple, Optional, Dict
from sklearn.metrics import average_precision_score
from sklearn.linear_model import LogisticRegression
from sklearn.feature_selection import mutual_info_classif
from sklearn.preprocessing import StandardScaler
import warnings
warnings.filterwarnings('ignore')


class NodeFeatureTransformer:
    """
    èŠ‚ç‚¹ç‰¹å¾è½¬æ¢å™¨
    
    æ”¯æŒåŠŸèƒ½ï¼š
    1. è¿ç»­ç‰¹å¾é‡è¦æ€§è¯„ä¼°ï¼ˆAUPR/MI/LRç³»æ•°ï¼‰
    2. Top-Nç‰¹å¾åˆ†ç®± + One-hot
    3. äºŒé˜¶äº¤äº’é¡¹
    
    é‡è¦ï¼šæ‰€æœ‰ç»Ÿè®¡é‡ä»…åœ¨è®­ç»ƒé›†ä¸Šè®¡ç®—ï¼
    """
    
    def __init__(
        self,
        topn: int = 3,
        n_bins: int = 5,
        add_interactions: bool = True,
        importance_metric: str = 'aupr',
        random_state: int = 42
    ):
        """
        å‚æ•°:
            topn: é€‰æ‹©å‰Nä¸ªé‡è¦ç‰¹å¾
            n_bins: åˆ†ç®±æ•°é‡
            add_interactions: æ˜¯å¦æ·»åŠ äº¤äº’é¡¹
            importance_metric: é‡è¦æ€§æŒ‡æ ‡ {'aupr', 'mi', 'lr_coef'}
            random_state: éšæœºç§å­
        """
        self.topn = topn
        self.n_bins = n_bins
        self.add_interactions = add_interactions
        self.importance_metric = importance_metric
        self.random_state = random_state
        
        # çŠ¶æ€ï¼ˆfitåä¿å­˜ï¼‰
        self.is_fitted = False
        self.top_feature_indices: Optional[List[int]] = None
        self.feature_names: Optional[List[str]] = None
        self.bin_edges: Optional[Dict[int, np.ndarray]] = None
        self.scaler: Optional[StandardScaler] = None
        self.n_original_features: Optional[int] = None
        self.n_augmented_features: Optional[int] = None
        
    def _compute_feature_importance(
        self,
        X: np.ndarray,
        y: np.ndarray
    ) -> np.ndarray:
        """
        è®¡ç®—ç‰¹å¾é‡è¦æ€§
        
        å‚æ•°:
            X: [N, D] ç‰¹å¾çŸ©é˜µ
            y: [N] æ ‡ç­¾
            
        è¿”å›:
            importance: [D] é‡è¦æ€§åˆ†æ•°
        """
        n_features = X.shape[1]
        importance = np.zeros(n_features)
        
        if self.importance_metric == 'aupr':
            # ä½¿ç”¨AUPRä½œä¸ºé‡è¦æ€§
            for i in range(n_features):
                try:
                    # ç®€å•çš„å•ç‰¹å¾é¢„æµ‹èƒ½åŠ›
                    feature = X[:, i].reshape(-1, 1)
                    lr = LogisticRegression(random_state=self.random_state, max_iter=100)
                    lr.fit(feature, y)
                    y_prob = lr.predict_proba(feature)[:, 1]
                    importance[i] = average_precision_score(y, y_prob)
                except:
                    importance[i] = 0.0
                    
        elif self.importance_metric == 'mi':
            # äº’ä¿¡æ¯
            importance = mutual_info_classif(
                X, y,
                random_state=self.random_state,
                n_neighbors=min(3, len(y) // 10)
            )
            
        elif self.importance_metric == 'lr_coef':
            # é€»è¾‘å›å½’ç³»æ•°ç»å¯¹å€¼
            lr = LogisticRegression(
                random_state=self.random_state,
                max_iter=200,
                penalty='l2',
                C=1.0
            )
            lr.fit(X, y)
            importance = np.abs(lr.coef_[0])
            
        else:
            raise ValueError(f"Unknown importance metric: {self.importance_metric}")
        
        return importance
    
    def _identify_continuous_features(self, X: np.ndarray) -> List[int]:
        """
        è¯†åˆ«è¿ç»­ç‰¹å¾ï¼ˆéäºŒå€¼ï¼‰
        
        å‚æ•°:
            X: [N, D] ç‰¹å¾çŸ©é˜µ
            
        è¿”å›:
            continuous_indices: è¿ç»­ç‰¹å¾ç´¢å¼•åˆ—è¡¨
        """
        continuous_indices = []
        n_features = X.shape[1]
        
        for i in range(n_features):
            unique_values = np.unique(X[:, i])
            # å¦‚æœuniqueå€¼æ•°é‡>10ï¼Œè®¤ä¸ºæ˜¯è¿ç»­ç‰¹å¾
            if len(unique_values) > 10:
                continuous_indices.append(i)
        
        return continuous_indices
    
    def fit(
        self,
        X: torch.Tensor,
        y: torch.Tensor,
        train_mask: torch.Tensor
    ) -> 'NodeFeatureTransformer':
        """
        æ‹Ÿåˆè½¬æ¢å™¨ï¼ˆä»…ä½¿ç”¨è®­ç»ƒé›†ï¼‰
        
        å‚æ•°:
            X: [N, D] å…¨éƒ¨èŠ‚ç‚¹ç‰¹å¾
            y: [N] å…¨éƒ¨æ ‡ç­¾
            train_mask: [N] è®­ç»ƒé›†mask
            
        è¿”å›:
            self
        """
        print("\n" + "="*70)
        print("ğŸ”§ ç‰¹å¾å·¥ç¨‹ç®¡é“æ‹Ÿåˆ")
        print("="*70)
        
        # è½¬æ¢ä¸ºnumpy
        X_np = X.cpu().numpy()
        y_np = y.cpu().numpy()
        train_mask_np = train_mask.cpu().numpy()
        
        # ä»…ä½¿ç”¨è®­ç»ƒé›†
        X_train = X_np[train_mask_np]
        y_train = y_np[train_mask_np]
        
        self.n_original_features = X_np.shape[1]
        
        print(f"\n1. è®­ç»ƒé›†ç»Ÿè®¡:")
        print(f"   æ ·æœ¬æ•°: {len(X_train)}")
        print(f"   æ­£æ ·æœ¬: {y_train.sum()} ({y_train.mean()*100:.1f}%)")
        print(f"   ç‰¹å¾æ•°: {self.n_original_features}")
        
        # è¯†åˆ«è¿ç»­ç‰¹å¾
        continuous_indices = self._identify_continuous_features(X_train)
        print(f"\n2. è¯†åˆ«è¿ç»­ç‰¹å¾: {len(continuous_indices)}ä¸ª")
        
        if len(continuous_indices) == 0:
            print("   âš ï¸  æœªæ‰¾åˆ°è¿ç»­ç‰¹å¾ï¼Œè·³è¿‡ç‰¹å¾æ‰©å±•")
            self.is_fitted = True
            self.n_augmented_features = self.n_original_features
            return self
        
        # è®¡ç®—é‡è¦æ€§ï¼ˆä»…åœ¨è¿ç»­ç‰¹å¾ä¸Šï¼‰
        print(f"\n3. è®¡ç®—ç‰¹å¾é‡è¦æ€§ (metric={self.importance_metric})...")
        X_continuous = X_train[:, continuous_indices]
        importance = self._compute_feature_importance(X_continuous, y_train)
        
        # é€‰æ‹©Top-N
        topn = min(self.topn, len(continuous_indices))
        top_indices_local = np.argsort(importance)[-topn:][::-1]
        self.top_feature_indices = [continuous_indices[i] for i in top_indices_local]
        
        print(f"\n4. Top-{topn} é‡è¦ç‰¹å¾:")
        for rank, idx in enumerate(self.top_feature_indices, 1):
            imp = importance[top_indices_local[rank-1]]
            print(f"   #{rank}: ç‰¹å¾{idx} (é‡è¦æ€§={imp:.4f})")
        
        # åˆ†ç®±ï¼ˆä»…åœ¨è®­ç»ƒé›†ä¸Šè®¡ç®—åˆ†ä½ç‚¹ï¼‰
        print(f"\n5. åˆ†ç®± (n_bins={self.n_bins})...")
        self.bin_edges = {}
        
        for idx in self.top_feature_indices:
            feature_train = X_train[:, idx]
            # è®¡ç®—åˆ†ä½ç‚¹
            quantiles = np.linspace(0, 1, self.n_bins + 1)
            edges = np.quantile(feature_train, quantiles)
            # ç¡®ä¿è¾¹ç•Œå”¯ä¸€
            edges = np.unique(edges)
            self.bin_edges[idx] = edges
            print(f"   ç‰¹å¾{idx}: {len(edges)-1}ä¸ªç®± {edges[:3]}...{edges[-3:]}")
        
        # æ ‡å‡†åŒ–å™¨ï¼ˆç”¨äºäº¤äº’é¡¹ï¼‰
        if self.add_interactions and topn >= 2:
            print(f"\n6. æ ‡å‡†åŒ–å™¨æ‹Ÿåˆï¼ˆç”¨äºäº¤äº’é¡¹ï¼‰...")
            X_top_train = X_train[:, self.top_feature_indices]
            self.scaler = StandardScaler()
            self.scaler.fit(X_top_train)
        
        self.is_fitted = True
        
        # è®¡ç®—æ‰©å±•åçš„ç‰¹å¾æ•°
        n_onehot = sum(len(self.bin_edges[idx]) - 1 for idx in self.top_feature_indices)
        n_interactions = topn * (topn - 1) // 2 if self.add_interactions and topn >= 2 else 0
        self.n_augmented_features = self.n_original_features + n_onehot + n_interactions
        
        print(f"\n7. ç‰¹å¾æ‰©å±•å®Œæˆ:")
        print(f"   åŸå§‹ç‰¹å¾: {self.n_original_features}")
        print(f"   åˆ†ç®±ç‰¹å¾: {n_onehot}")
        print(f"   äº¤äº’ç‰¹å¾: {n_interactions}")
        print(f"   æ€»è®¡: {self.n_augmented_features} (+{self.n_augmented_features - self.n_original_features})")
        
        print("\n" + "="*70)
        
        return self
    
    def transform(self, X: torch.Tensor) -> torch.Tensor:
        """
        è½¬æ¢ç‰¹å¾
        
        å‚æ•°:
            X: [N, D] èŠ‚ç‚¹ç‰¹å¾
            
        è¿”å›:
            X_aug: [N, D'] æ‰©å±•åçš„ç‰¹å¾
        """
        if not self.is_fitted:
            raise RuntimeError("Transformer not fitted. Call fit() first.")
        
        if self.top_feature_indices is None:
            # æ²¡æœ‰è¿ç»­ç‰¹å¾ï¼Œç›´æ¥è¿”å›
            return X
        
        X_np = X.cpu().numpy()
        
        # 1. åˆ†ç®± + One-hot
        onehot_features = []
        for idx in self.top_feature_indices:
            feature = X_np[:, idx]
            edges = self.bin_edges[idx]
            
            # æ•°å­—åŒ–
            bins = np.digitize(feature, edges[1:-1])  # ä¸åŒ…æ‹¬æœ€å°å’Œæœ€å¤§è¾¹ç•Œ
            bins = np.clip(bins, 0, len(edges) - 2)  # ç¡®ä¿åœ¨èŒƒå›´å†…
            
            # One-hot
            n_bins = len(edges) - 1
            onehot = np.zeros((len(feature), n_bins))
            onehot[np.arange(len(feature)), bins] = 1
            
            onehot_features.append(onehot)
        
        onehot_features = np.hstack(onehot_features) if onehot_features else np.empty((len(X_np), 0))
        
        # 2. äº¤äº’é¡¹
        interaction_features = np.empty((len(X_np), 0))
        if self.add_interactions and len(self.top_feature_indices) >= 2:
            X_top = X_np[:, self.top_feature_indices]
            X_top_scaled = self.scaler.transform(X_top)
            
            # ä¸¤ä¸¤ç›¸ä¹˜
            interactions = []
            for i in range(len(self.top_feature_indices)):
                for j in range(i + 1, len(self.top_feature_indices)):
                    interaction = X_top_scaled[:, i] * X_top_scaled[:, j]
                    interactions.append(interaction.reshape(-1, 1))
            
            if interactions:
                interaction_features = np.hstack(interactions)
        
        # 3. æ‹¼æ¥
        X_aug = np.hstack([X_np, onehot_features, interaction_features])
        
        return torch.from_numpy(X_aug).float().to(X.device)
    
    def fit_transform(
        self,
        X: torch.Tensor,
        y: torch.Tensor,
        train_mask: torch.Tensor
    ) -> torch.Tensor:
        """
        æ‹Ÿåˆå¹¶è½¬æ¢
        
        å‚æ•°:
            X: [N, D] èŠ‚ç‚¹ç‰¹å¾
            y: [N] æ ‡ç­¾
            train_mask: [N] è®­ç»ƒé›†mask
            
        è¿”å›:
            X_aug: [N, D'] æ‰©å±•åçš„ç‰¹å¾
        """
        self.fit(X, y, train_mask)
        return self.transform(X)
    
    def save(self, path: str):
        """ä¿å­˜è½¬æ¢å™¨çŠ¶æ€"""
        state = {
            'topn': self.topn,
            'n_bins': self.n_bins,
            'add_interactions': self.add_interactions,
            'importance_metric': self.importance_metric,
            'random_state': self.random_state,
            'is_fitted': self.is_fitted,
            'top_feature_indices': self.top_feature_indices,
            'feature_names': self.feature_names,
            'bin_edges': self.bin_edges,
            'scaler': self.scaler,
            'n_original_features': self.n_original_features,
            'n_augmented_features': self.n_augmented_features
        }
        
        with open(path, 'wb') as f:
            pickle.dump(state, f)
        
        print(f"âœ“ ç‰¹å¾è½¬æ¢å™¨å·²ä¿å­˜: {path}")
    
    @classmethod
    def load(cls, path: str) -> 'NodeFeatureTransformer':
        """åŠ è½½è½¬æ¢å™¨çŠ¶æ€"""
        with open(path, 'rb') as f:
            state = pickle.load(f)
        
        transformer = cls(
            topn=state['topn'],
            n_bins=state['n_bins'],
            add_interactions=state['add_interactions'],
            importance_metric=state['importance_metric'],
            random_state=state['random_state']
        )
        
        transformer.is_fitted = state['is_fitted']
        transformer.top_feature_indices = state['top_feature_indices']
        transformer.feature_names = state['feature_names']
        transformer.bin_edges = state['bin_edges']
        transformer.scaler = state['scaler']
        transformer.n_original_features = state['n_original_features']
        transformer.n_augmented_features = state['n_augmented_features']
        
        print(f"âœ“ ç‰¹å¾è½¬æ¢å™¨å·²åŠ è½½: {path}")
        
        return transformer
    
    def get_augmented_feature_dim(self) -> int:
        """è·å–æ‰©å±•åçš„ç‰¹å¾ç»´åº¦"""
        if not self.is_fitted:
            raise RuntimeError("Transformer not fitted")
        return self.n_augmented_features
