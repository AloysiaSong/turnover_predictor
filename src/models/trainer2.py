"""
GCNè®­ç»ƒå™¨

èŒè´£:
1. ç®¡ç†å®Œæ•´è®­ç»ƒæµç¨‹
2. ä¼˜åŒ–å™¨å’Œå­¦ä¹ ç‡è°ƒåº¦
3. æ—©åœæœºåˆ¶
4. æ¨¡å‹ä¿å­˜å’ŒåŠ è½½
5. è®­ç»ƒæ—¥å¿—è®°å½•
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.optim import Adam
from torch.optim.lr_scheduler import ReduceLROnPlateau
import numpy as np
from pathlib import Path
import time
import json
from tqdm import tqdm


class GCNTrainer:
    """GCNè®­ç»ƒå™¨"""
    
    def __init__(
        self,
        model,
        data,
        device='cpu',
        lr=0.01,
        weight_decay=5e-4,
        pos_weight=None,
        scheduler_patience=10,
        scheduler_factor=0.5,
        min_lr=1e-6
    ):
        """
        Args:
            model: GCNæ¨¡å‹
            data: PyG Dataå¯¹è±¡
            device: è®¾å¤‡ ('cpu' or 'cuda')
            lr: åˆå§‹å­¦ä¹ ç‡
            weight_decay: L2æ­£åˆ™åŒ–ç³»æ•°
            pos_weight: æ­£æ ·æœ¬æƒé‡ï¼ˆå¤„ç†ç±»åˆ«ä¸å¹³è¡¡ï¼‰
            scheduler_patience: å­¦ä¹ ç‡è°ƒåº¦å™¨è€å¿ƒå€¼
            scheduler_factor: å­¦ä¹ ç‡è¡°å‡å› å­
            min_lr: æœ€å°å­¦ä¹ ç‡
        """
        self.model = model.to(device)
        self.data = data.to(device)
        self.device = device
        
        # ä¼˜åŒ–å™¨
        self.optimizer = Adam(
            model.parameters(),
            lr=lr,
            weight_decay=weight_decay
        )
        
        # æŸå¤±å‡½æ•°ï¼ˆå¸¦ç±»åˆ«æƒé‡ï¼‰
        if pos_weight is not None:
            pos_weight_tensor = torch.tensor([pos_weight]).to(device)
            self.criterion = nn.BCEWithLogitsLoss(pos_weight=pos_weight_tensor)
        else:
            self.criterion = nn.BCEWithLogitsLoss()
        
        # å­¦ä¹ ç‡è°ƒåº¦å™¨
        self.scheduler = ReduceLROnPlateau(
            self.optimizer,
            mode='min',
            factor=scheduler_factor,
            patience=scheduler_patience,
            min_lr=min_lr,
            verbose=True
        )
        
        # è®­ç»ƒå†å²
        self.history = {
            'train_loss': [],
            'train_acc': [],
            'val_loss': [],
            'val_acc': [],
            'val_f1': [],
            'lr': []
        }
        
        # æœ€ä½³æ¨¡å‹è¿½è¸ª
        self.best_val_loss = float('inf')
        self.best_epoch = 0
        self.best_model_state = None
    
    def train_epoch(self):
        """è®­ç»ƒä¸€ä¸ªepoch"""
        self.model.train()
        
        # å‰å‘ä¼ æ’­
        out = self.model(
            self.data.x,
            self.data.edge_index,
            self.data.edge_attr if hasattr(self.data, 'edge_attr') else None
        )
        
        # è®¡ç®—æŸå¤±ï¼ˆåªåœ¨è®­ç»ƒé›†ä¸Šï¼‰
        loss = self.criterion(
            out[self.data.train_mask].squeeze(),
            self.data.y[self.data.train_mask].float()
        )
        
        # åå‘ä¼ æ’­
        self.optimizer.zero_grad()
        loss.backward()
        
        # æ¢¯åº¦è£å‰ªï¼ˆé˜²æ­¢æ¢¯åº¦çˆ†ç‚¸ï¼‰
        torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=1.0)
        
        self.optimizer.step()
        
        # è®¡ç®—è®­ç»ƒå‡†ç¡®ç‡
        with torch.no_grad():
            probs = torch.sigmoid(out[self.data.train_mask]).squeeze()
            preds = (probs > 0.5).long()
            acc = (preds == self.data.y[self.data.train_mask]).float().mean()
        
        return loss.item(), acc.item()
    
    @torch.no_grad()
    def evaluate(self, mask):
        """
        åœ¨æŒ‡å®šmaskä¸Šè¯„ä¼°
        
        Args:
            mask: å¸ƒå°”mask (train_mask, val_mask, or test_mask)
            
        Returns:
            loss, accuracy, f1_score
        """
        self.model.eval()
        
        # å‰å‘ä¼ æ’­
        out = self.model(
            self.data.x,
            self.data.edge_index,
            self.data.edge_attr if hasattr(self.data, 'edge_attr') else None
        )
        
        # è®¡ç®—æŸå¤±
        loss = self.criterion(
            out[mask].squeeze(),
            self.data.y[mask].float()
        )
        
        # é¢„æµ‹
        probs = torch.sigmoid(out[mask]).squeeze()
        preds = (probs > 0.5).long()
        labels = self.data.y[mask]
        
        # å‡†ç¡®ç‡
        acc = (preds == labels).float().mean().item()
        
        # F1-Score
        tp = ((preds == 1) & (labels == 1)).sum().float()
        fp = ((preds == 1) & (labels == 0)).sum().float()
        fn = ((preds == 0) & (labels == 1)).sum().float()
        
        precision = tp / (tp + fp + 1e-8)
        recall = tp / (tp + fn + 1e-8)
        f1 = 2 * precision * recall / (precision + recall + 1e-8)
        
        return loss.item(), acc, f1.item()
    
    @torch.no_grad()
    def predict(self, mask, threshold: float = 0.5):
        """
        è·å–æŒ‡å®šmaskä¸Šçš„é¢„æµ‹ç»“æœ
        
        Args:
            mask: PyGå¸ƒå°”mask (train/val/test)
            threshold: äºŒåˆ†ç±»é˜ˆå€¼ï¼ˆç”¨äºè¿”å›predsï¼‰
        
        Returns:
            probs: é¢„æµ‹æ¦‚ç‡ (np.ndarray)
            preds: é¢„æµ‹æ ‡ç­¾ (np.ndarray)
            labels: çœŸå®æ ‡ç­¾ (np.ndarray)
        """
        self.model.eval()
        
        mask = mask.to(self.device)
        
        out = self.model(
            self.data.x,
            self.data.edge_index,
            self.data.edge_attr if hasattr(self.data, 'edge_attr') else None
        )
        
        logits = out[mask].view(-1)
        probs = torch.sigmoid(logits)
        preds = (probs >= threshold).long()
        labels = self.data.y[mask].view(-1)
        
        return (
            probs.cpu().numpy(),
            preds.cpu().numpy(),
            labels.cpu().numpy()
        )
    
    def train(
        self,
        epochs=200,
        early_stopping_patience=20,
        verbose=True,
        save_dir='outputs/models'
    ):
        """
        å®Œæ•´è®­ç»ƒæµç¨‹
        
        Args:
            epochs: æœ€å¤§è®­ç»ƒè½®æ•°
            early_stopping_patience: æ—©åœè€å¿ƒå€¼
            verbose: æ˜¯å¦æ˜¾ç¤ºè¯¦ç»†ä¿¡æ¯
            save_dir: æ¨¡å‹ä¿å­˜ç›®å½•
            
        Returns:
            history: è®­ç»ƒå†å²
        """
        if verbose:
            print("\n" + "="*70)
            print("ğŸš€ GCNè®­ç»ƒå¼€å§‹")
            print("="*70)
            self._print_config(epochs, early_stopping_patience)
        
        # åˆ›å»ºä¿å­˜ç›®å½•
        save_dir = Path(save_dir)
        save_dir.mkdir(parents=True, exist_ok=True)
        
        # æ—©åœè®¡æ•°å™¨
        patience_counter = 0
        start_time = time.time()
        
        # è®­ç»ƒå¾ªç¯
        if verbose:
            print("\n" + "="*70)
            print("ğŸ“ˆ è®­ç»ƒè¿›åº¦")
            print("="*70)
            pbar = tqdm(range(epochs), desc="Training")
        else:
            pbar = range(epochs)
        
        for epoch in pbar:
            # è®­ç»ƒä¸€ä¸ªepoch
            train_loss, train_acc = self.train_epoch()
            
            # éªŒè¯é›†è¯„ä¼°
            val_loss, val_acc, val_f1 = self.evaluate(self.data.val_mask)
            
            # å­¦ä¹ ç‡è°ƒåº¦
            self.scheduler.step(val_loss)
            current_lr = self.optimizer.param_groups[0]['lr']
            
            # è®°å½•å†å²
            self.history['train_loss'].append(train_loss)
            self.history['train_acc'].append(train_acc)
            self.history['val_loss'].append(val_loss)
            self.history['val_acc'].append(val_acc)
            self.history['val_f1'].append(val_f1)
            self.history['lr'].append(current_lr)
            
            # æ›´æ–°è¿›åº¦æ¡
            if verbose:
                pbar.set_postfix({
                    'T_Loss': f'{train_loss:.4f}',
                    'V_Loss': f'{val_loss:.4f}',
                    'V_F1': f'{val_f1:.4f}',
                    'LR': f'{current_lr:.6f}'
                })
            
            # ä¿å­˜æœ€ä½³æ¨¡å‹
            if val_loss < self.best_val_loss:
                self.best_val_loss = val_loss
                self.best_epoch = epoch
                self.best_model_state = self.model.state_dict().copy()
                patience_counter = 0
                
                if verbose and (epoch + 1) % 10 == 0:
                    print(f"\n   â­ Epoch {epoch+1}: æ–°çš„æœ€ä½³æ¨¡å‹! Val Loss={val_loss:.4f}, Val F1={val_f1:.4f}")
            else:
                patience_counter += 1
            
            # æ—©åœæ£€æŸ¥
            if patience_counter >= early_stopping_patience:
                if verbose:
                    print(f"\n   ğŸ›‘ Early stopping triggered at epoch {epoch+1}")
                    print(f"   ğŸ“Œ Best epoch: {self.best_epoch+1}")
                break
        
        # è®­ç»ƒå®Œæˆ
        total_time = time.time() - start_time
        
        if verbose:
            print("\n" + "="*70)
            print("âœ… è®­ç»ƒå®Œæˆ")
            print("="*70)
            print(f"\n   æ€»è®­ç»ƒæ—¶é—´: {total_time:.2f}s")
            print(f"   æœ€ä½³Epoch: {self.best_epoch + 1}")
            print(f"   æœ€ä½³Val Loss: {self.best_val_loss:.4f}")
            print(f"   æœ€ä½³Val F1: {max(self.history['val_f1']):.4f}")
        
        # æ¢å¤æœ€ä½³æ¨¡å‹
        if self.best_model_state is not None:
            self.model.load_state_dict(self.best_model_state)
        
        # ä¿å­˜æœ€ä½³æ¨¡å‹
        self.save_checkpoint(save_dir / 'best_model.pt')
        
        # ä¿å­˜è®­ç»ƒå†å²
        self._save_history(save_dir / 'training_history.json')
        
        if verbose:
            print(f"\n   ğŸ’¾ æ¨¡å‹å·²ä¿å­˜: {save_dir / 'best_model.pt'}")
            print(f"   ğŸ“Š è®­ç»ƒå†å²å·²ä¿å­˜: {save_dir / 'training_history.json'}")
        
        return self.history
    
    def save_checkpoint(self, path):
        """ä¿å­˜æ¨¡å‹æ£€æŸ¥ç‚¹"""
        checkpoint = {
            'model_state_dict': self.model.state_dict(),
            'optimizer_state_dict': self.optimizer.state_dict(),
            'scheduler_state_dict': self.scheduler.state_dict(),
            'history': self.history,
            'best_val_loss': self.best_val_loss,
            'best_epoch': self.best_epoch
        }
        torch.save(checkpoint, path)
    
    def load_checkpoint(self, path):
        """åŠ è½½æ¨¡å‹æ£€æŸ¥ç‚¹"""
        checkpoint = torch.load(path, map_location=self.device)
        self.model.load_state_dict(checkpoint['model_state_dict'])
        self.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
        self.scheduler.load_state_dict(checkpoint['scheduler_state_dict'])
        self.history = checkpoint['history']
        self.best_val_loss = checkpoint['best_val_loss']
        self.best_epoch = checkpoint['best_epoch']
        return checkpoint
    
    def _print_config(self, epochs, patience):
        """æ‰“å°è®­ç»ƒé…ç½®"""
        print("\né…ç½®:")
        print(f"   æ¨¡å‹: GCN")
        print(f"   å‚æ•°é‡: {sum(p.numel() for p in self.model.parameters()):,}")
        print(f"   è®¾å¤‡: {self.device}")
        print(f"   ä¼˜åŒ–å™¨: Adam")
        print(f"   åˆå§‹å­¦ä¹ ç‡: {self.optimizer.param_groups[0]['lr']}")
        print(f"   æƒé‡è¡°å‡: {self.optimizer.param_groups[0]['weight_decay']}")
        print(f"   æœ€å¤§Epochs: {epochs}")
        print(f"   æ—©åœè€å¿ƒ: {patience}")
        
        # æ•°æ®é›†ä¿¡æ¯
        print(f"\næ•°æ®é›†:")
        print(f"   è®­ç»ƒé›†: {self.data.train_mask.sum().item()} èŠ‚ç‚¹")
        print(f"   éªŒè¯é›†: {self.data.val_mask.sum().item()} èŠ‚ç‚¹")
        print(f"   æµ‹è¯•é›†: {self.data.test_mask.sum().item()} èŠ‚ç‚¹")
        print(f"   è®­ç»ƒé›†ç¦»èŒç‡: {self.data.y[self.data.train_mask].float().mean().item():.2%}")
    
    def _save_history(self, path):
        """ä¿å­˜è®­ç»ƒå†å²"""
        with open(path, 'w') as f:
            json.dump(self.history, f, indent=2)


def quick_train(
    model,
    data,
    epochs=200,
    lr=0.01,
    weight_decay=5e-4,
    early_stopping_patience=20,
    device='cpu',
    save_dir='outputs/models'
):
    """
    å¿«é€Ÿè®­ç»ƒå‡½æ•°ï¼ˆç®€åŒ–æ¥å£ï¼‰
    
    Args:
        model: GCNæ¨¡å‹
        data: PyG Dataå¯¹è±¡
        epochs: è®­ç»ƒè½®æ•°
        lr: å­¦ä¹ ç‡
        weight_decay: æƒé‡è¡°å‡
        early_stopping_patience: æ—©åœè€å¿ƒ
        device: è®¾å¤‡
        save_dir: ä¿å­˜ç›®å½•
        
    Returns:
        trainer: è®­ç»ƒå™¨å¯¹è±¡
        history: è®­ç»ƒå†å²
    """
    # è®¡ç®—ç±»åˆ«æƒé‡
    num_pos = data.y[data.train_mask].sum().item()
    num_neg = data.train_mask.sum().item() - num_pos
    pos_weight = num_neg / num_pos  if num_pos > 0 else 1.0
    
    print(f"\nç±»åˆ«å¹³è¡¡:")
    print(f"   æ­£æ ·æœ¬(ç¦»èŒ): {num_pos}")
    print(f"   è´Ÿæ ·æœ¬(åœ¨èŒ): {num_neg}")
    print(f"   æ­£æ ·æœ¬æƒé‡: {pos_weight:.2f}")
    
    # åˆ›å»ºè®­ç»ƒå™¨
    trainer = GCNTrainer(
        model=model,
        data=data,
        device=device,
        lr=lr,
        weight_decay=weight_decay,
        pos_weight=pos_weight
    )
    
    # è®­ç»ƒ
    history = trainer.train(
        epochs=epochs,
        early_stopping_patience=early_stopping_patience,
        save_dir=save_dir
    )
    
    return trainer, history


if __name__ == '__main__':
    """æµ‹è¯•è®­ç»ƒå™¨"""
    print("\n" + "="*70)
    print("ğŸ§ª GCNè®­ç»ƒå™¨æµ‹è¯•")
    print("="*70)
    
    # åŠ è½½æ•°æ®å’Œæ¨¡å‹
    print("\n1. åŠ è½½æ•°æ®...")
    data = torch.load('data/processed/homo_graph.pt')
    print(f"   âœ“ æ•°æ®åŠ è½½æˆåŠŸ")
    
    print("\n2. åˆ›å»ºæ¨¡å‹...")
    from gcn import create_gcn_model
    model = create_gcn_model(
        in_channels=data.num_node_features,
        architecture='default',
        dropout=0.5
    )
    print(f"   âœ“ æ¨¡å‹åˆ›å»ºæˆåŠŸ")
    
    print("\n3. å¼€å§‹è®­ç»ƒ...")
    trainer, history = quick_train(
        model=model,
        data=data,
        epochs=50,  # æµ‹è¯•ç”¨å°‘é‡epoch
        lr=0.01,
        early_stopping_patience=10,
        device='cpu'
    )
    
    print("\n4. æµ‹è¯•é›†è¯„ä¼°...")
    test_loss, test_acc, test_f1 = trainer.evaluate(data.test_mask)
    print(f"   æµ‹è¯•Loss: {test_loss:.4f}")
    print(f"   æµ‹è¯•Acc: {test_acc:.4f}")
    print(f"   æµ‹è¯•F1: {test_f1:.4f}")
    
    print("\n" + "="*70)
    print("âœ… è®­ç»ƒå™¨æµ‹è¯•å®Œæˆ")
    print("="*70)
