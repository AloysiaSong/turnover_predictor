"""
GCNè¯„ä¼°å™¨

èŒè´£:
1. å®Œæ•´æ€§èƒ½è¯„ä¼°
2. è®¡ç®—å„ç§æŒ‡æ ‡
3. ç”Ÿæˆæ··æ·†çŸ©é˜µ
4. ç»˜åˆ¶ROC/PRæ›²çº¿
5. ä¸åŸºçº¿æ¨¡å‹å¯¹æ¯”
"""

import torch
import numpy as np
import matplotlib
matplotlib.use('Agg')  # éäº¤äº’å¼åç«¯
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.metrics import (
    accuracy_score, precision_score, recall_score, f1_score,
    roc_auc_score, average_precision_score,
    confusion_matrix, classification_report,
    roc_curve, precision_recall_curve
)
from pathlib import Path
import json


class GCNEvaluator:
    """GCNè¯„ä¼°å™¨"""
    
    def __init__(self, model, data, device='cpu'):
        """
        Args:
            model: è®­ç»ƒå¥½çš„GCNæ¨¡å‹
            data: PyG Dataå¯¹è±¡
            device: è®¾å¤‡
        """
        self.model = model.to(device)
        self.data = data.to(device)
        self.device = device
        
    @torch.no_grad()
    def predict(self, mask):
        """
        è·å–é¢„æµ‹ç»“æœ
        
        Args:
            mask: å¸ƒå°”mask
            
        Returns:
            probs: é¢„æµ‹æ¦‚ç‡
            preds: é¢„æµ‹æ ‡ç­¾
            labels: çœŸå®æ ‡ç­¾
        """
        self.model.eval()
        
        # å‰å‘ä¼ æ’­
        out = self.model(
            self.data.x,
            self.data.edge_index,
            self.data.edge_attr if hasattr(self.data, 'edge_attr') else None
        )
        
        # è·å–æ¦‚ç‡å’Œé¢„æµ‹
        probs = torch.sigmoid(out[mask]).squeeze().cpu().numpy()
        preds = (probs > 0.5).astype(int)
        labels = self.data.y[mask].cpu().numpy()
        
        return probs, preds, labels
    
    def evaluate(self, mask, mask_name='Test'):
        """
        å®Œæ•´è¯„ä¼°
        
        Args:
            mask: å¸ƒå°”mask
            mask_name: maskåç§° (ç”¨äºæ˜¾ç¤º)
            
        Returns:
            metrics: æŒ‡æ ‡å­—å…¸
        """
        # è·å–é¢„æµ‹
        probs, preds, labels = self.predict(mask)
        
        # è®¡ç®—æ‰€æœ‰æŒ‡æ ‡
        metrics = {}
        
        # åŸºç¡€æŒ‡æ ‡
        metrics['accuracy'] = accuracy_score(labels, preds)
        metrics['precision'] = precision_score(labels, preds, zero_division=0)
        metrics['recall'] = recall_score(labels, preds, zero_division=0)
        metrics['f1'] = f1_score(labels, preds, zero_division=0)
        
        # AUCæŒ‡æ ‡
        if len(np.unique(labels)) > 1:
            metrics['roc_auc'] = roc_auc_score(labels, probs)
            metrics['pr_auc'] = average_precision_score(labels, probs)
        else:
            metrics['roc_auc'] = 0.0
            metrics['pr_auc'] = 0.0
        
        # æ··æ·†çŸ©é˜µ
        metrics['confusion_matrix'] = confusion_matrix(labels, preds)
        
        # åˆ†ç±»æŠ¥å‘Š
        metrics['classification_report'] = classification_report(
            labels, preds,
            target_names=['Stay', 'Turnover'],
            zero_division=0
        )
        
        return metrics
    
    def print_metrics(self, metrics, mask_name='Test'):
        """æ‰“å°è¯„ä¼°æŒ‡æ ‡"""
        print("\n" + "="*70)
        print(f"ğŸ“Š {mask_name}é›†è¯„ä¼°æŠ¥å‘Š")
        print("="*70)
        
        print(f"\næ ¸å¿ƒæŒ‡æ ‡:")
        print(f"â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”")
        print(f"   Accuracy:   {metrics['accuracy']:.4f}")
        print(f"   Precision:  {metrics['precision']:.4f}")
        print(f"   Recall:     {metrics['recall']:.4f}")
        print(f"   F1-Score:   {metrics['f1']:.4f}")
        print(f"   AUC-ROC:    {metrics['roc_auc']:.4f}")
        print(f"   AUC-PR:     {metrics['pr_auc']:.4f}")
        print(f"â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”")
        
        print(f"\næ··æ·†çŸ©é˜µ:")
        cm = metrics['confusion_matrix']
        print(f"                é¢„æµ‹ä¸ç¦»èŒ    é¢„æµ‹ç¦»èŒ")
        print(f"  å®é™…ä¸ç¦»èŒ    {cm[0,0]:>8}      {cm[0,1]:>8}")
        print(f"  å®é™…ç¦»èŒ      {cm[1,0]:>8}      {cm[1,1]:>8}")
        
        print(f"\nè¯¦ç»†æŠ¥å‘Š:")
        print(metrics['classification_report'])
    
    def plot_confusion_matrix(self, metrics, save_path=None, mask_name='Test'):
        """ç»˜åˆ¶æ··æ·†çŸ©é˜µ"""
        cm = metrics['confusion_matrix']
        
        plt.figure(figsize=(8, 6))
        sns.heatmap(
            cm,
            annot=True,
            fmt='d',
            cmap='Blues',
            xticklabels=['Stay', 'Turnover'],
            yticklabels=['Stay', 'Turnover']
        )
        plt.title(f'Confusion Matrix - {mask_name} Set')
        plt.ylabel('True Label')
        plt.xlabel('Predicted Label')
        plt.tight_layout()
        
        if save_path:
            Path(save_path).parent.mkdir(parents=True, exist_ok=True)
            plt.savefig(save_path, dpi=150, bbox_inches='tight')
            print(f"   âœ“ æ··æ·†çŸ©é˜µå·²ä¿å­˜: {save_path}")
        
        plt.close()
    
    def plot_roc_curve(self, mask, save_path=None, mask_name='Test'):
        """ç»˜åˆ¶ROCæ›²çº¿"""
        probs, preds, labels = self.predict(mask)
        
        if len(np.unique(labels)) <= 1:
            print("   âš ï¸ åªæœ‰ä¸€ä¸ªç±»åˆ«ï¼Œè·³è¿‡ROCæ›²çº¿")
            return
        
        fpr, tpr, thresholds = roc_curve(labels, probs)
        auc = roc_auc_score(labels, probs)
        
        plt.figure(figsize=(8, 6))
        plt.plot(fpr, tpr, label=f'GCN (AUC = {auc:.3f})', linewidth=2)
        plt.plot([0, 1], [0, 1], 'k--', label='Random (AUC = 0.500)')
        plt.xlim([0.0, 1.0])
        plt.ylim([0.0, 1.05])
        plt.xlabel('False Positive Rate')
        plt.ylabel('True Positive Rate')
        plt.title(f'ROC Curve - {mask_name} Set')
        plt.legend(loc='lower right')
        plt.grid(alpha=0.3)
        plt.tight_layout()
        
        if save_path:
            Path(save_path).parent.mkdir(parents=True, exist_ok=True)
            plt.savefig(save_path, dpi=150, bbox_inches='tight')
            print(f"   âœ“ ROCæ›²çº¿å·²ä¿å­˜: {save_path}")
        
        plt.close()
    
    def plot_pr_curve(self, mask, save_path=None, mask_name='Test'):
        """ç»˜åˆ¶Precision-Recallæ›²çº¿"""
        probs, preds, labels = self.predict(mask)
        
        if len(np.unique(labels)) <= 1:
            print("   âš ï¸ åªæœ‰ä¸€ä¸ªç±»åˆ«ï¼Œè·³è¿‡PRæ›²çº¿")
            return
        
        precision, recall, thresholds = precision_recall_curve(labels, probs)
        auc_pr = average_precision_score(labels, probs)
        
        # è®¡ç®—åŸºçº¿ï¼ˆéšæœºåˆ†ç±»å™¨ï¼‰
        baseline = labels.mean()
        
        plt.figure(figsize=(8, 6))
        plt.plot(recall, precision, label=f'GCN (AUC = {auc_pr:.3f})', linewidth=2)
        plt.axhline(y=baseline, color='k', linestyle='--', 
                   label=f'Baseline (AUC = {baseline:.3f})')
        plt.xlim([0.0, 1.0])
        plt.ylim([0.0, 1.05])
        plt.xlabel('Recall')
        plt.ylabel('Precision')
        plt.title(f'Precision-Recall Curve - {mask_name} Set')
        plt.legend(loc='best')
        plt.grid(alpha=0.3)
        plt.tight_layout()
        
        if save_path:
            Path(save_path).parent.mkdir(parents=True, exist_ok=True)
            plt.savefig(save_path, dpi=150, bbox_inches='tight')
            print(f"   âœ“ PRæ›²çº¿å·²ä¿å­˜: {save_path}")
        
        plt.close()
    
    def plot_training_curves(self, history, save_path=None):
        """ç»˜åˆ¶è®­ç»ƒæ›²çº¿"""
        fig, axes = plt.subplots(2, 2, figsize=(14, 10))
        
        # Lossæ›²çº¿
        ax = axes[0, 0]
        ax.plot(history['train_loss'], label='Train Loss', linewidth=2)
        ax.plot(history['val_loss'], label='Val Loss', linewidth=2)
        ax.set_xlabel('Epoch')
        ax.set_ylabel('Loss')
        ax.set_title('Training and Validation Loss')
        ax.legend()
        ax.grid(alpha=0.3)
        
        # Accuracyæ›²çº¿
        ax = axes[0, 1]
        ax.plot(history['train_acc'], label='Train Acc', linewidth=2)
        ax.plot(history['val_acc'], label='Val Acc', linewidth=2)
        ax.set_xlabel('Epoch')
        ax.set_ylabel('Accuracy')
        ax.set_title('Training and Validation Accuracy')
        ax.legend()
        ax.grid(alpha=0.3)
        
        # F1æ›²çº¿
        ax = axes[1, 0]
        ax.plot(history['val_f1'], label='Val F1', linewidth=2, color='green')
        ax.set_xlabel('Epoch')
        ax.set_ylabel('F1-Score')
        ax.set_title('Validation F1-Score')
        ax.legend()
        ax.grid(alpha=0.3)
        
        # å­¦ä¹ ç‡æ›²çº¿
        ax = axes[1, 1]
        ax.plot(history['lr'], label='Learning Rate', linewidth=2, color='orange')
        ax.set_xlabel('Epoch')
        ax.set_ylabel('Learning Rate')
        ax.set_title('Learning Rate Schedule')
        ax.set_yscale('log')
        ax.legend()
        ax.grid(alpha=0.3)
        
        plt.tight_layout()
        
        if save_path:
            Path(save_path).parent.mkdir(parents=True, exist_ok=True)
            plt.savefig(save_path, dpi=150, bbox_inches='tight')
            print(f"   âœ“ è®­ç»ƒæ›²çº¿å·²ä¿å­˜: {save_path}")
        
        plt.close()
    
    def full_evaluation(self, save_dir='outputs/evaluation'):
        """
        å®Œæ•´è¯„ä¼°æµç¨‹
        
        Args:
            save_dir: ç»“æœä¿å­˜ç›®å½•
            
        Returns:
            results: æ‰€æœ‰ç»“æœå­—å…¸
        """
        print("\n" + "="*70)
        print("ğŸ¯ å¼€å§‹å®Œæ•´è¯„ä¼°")
        print("="*70)
        
        save_dir = Path(save_dir)
        save_dir.mkdir(parents=True, exist_ok=True)
        
        results = {}
        
        # è¯„ä¼°å„ä¸ªé›†åˆ
        for mask_name, mask in [
            ('Train', self.data.train_mask),
            ('Val', self.data.val_mask),
            ('Test', self.data.test_mask)
        ]:
            print(f"\n{'='*70}")
            print(f"è¯„ä¼°{mask_name}é›†...")
            print(f"{'='*70}")
            
            metrics = self.evaluate(mask, mask_name)
            self.print_metrics(metrics, mask_name)
            results[mask_name.lower()] = metrics
            
            # ç”Ÿæˆå¯è§†åŒ–
            self.plot_confusion_matrix(
                metrics,
                save_path=save_dir / f'{mask_name.lower()}_confusion_matrix.png',
                mask_name=mask_name
            )
            
            self.plot_roc_curve(
                mask,
                save_path=save_dir / f'{mask_name.lower()}_roc_curve.png',
                mask_name=mask_name
            )
            
            self.plot_pr_curve(
                mask,
                save_path=save_dir / f'{mask_name.lower()}_pr_curve.png',
                mask_name=mask_name
            )
        
        # ä¿å­˜ç»“æœ
        results_for_save = {}
        for split in ['train', 'val', 'test']:
            results_for_save[split] = {
                'accuracy': float(results[split]['accuracy']),
                'precision': float(results[split]['precision']),
                'recall': float(results[split]['recall']),
                'f1': float(results[split]['f1']),
                'roc_auc': float(results[split]['roc_auc']),
                'pr_auc': float(results[split]['pr_auc'])
            }
        
        with open(save_dir / 'metrics.json', 'w') as f:
            json.dump(results_for_save, f, indent=2)
        
        print(f"\n{'='*70}")
        print("âœ… è¯„ä¼°å®Œæˆ")
        print(f"{'='*70}")
        print(f"\nğŸ“ ç»“æœå·²ä¿å­˜åˆ°: {save_dir}")
        
        return results


def compare_with_baseline(gcn_metrics, baseline_metrics):
    """
    ä¸åŸºçº¿æ¨¡å‹å¯¹æ¯”
    
    Args:
        gcn_metrics: GCNæŒ‡æ ‡
        baseline_metrics: åŸºçº¿æŒ‡æ ‡ (å¦‚RandomForest)
    """
    print("\n" + "="*70)
    print("ğŸ“Š GCN vs åŸºçº¿æ¨¡å‹å¯¹æ¯”")
    print("="*70)
    
    print(f"\n{'æŒ‡æ ‡':<15} {'GCN':<12} {'åŸºçº¿':<12} {'æå‡':<12} {'çŠ¶æ€'}")
    print("â”"*70)
    
    metrics_to_compare = ['f1', 'roc_auc', 'pr_auc', 'recall', 'precision']
    
    for metric in metrics_to_compare:
        gcn_val = gcn_metrics.get(metric, 0)
        base_val = baseline_metrics.get(metric, 0)
        
        if base_val > 0:
            improvement = ((gcn_val - base_val) / base_val) * 100
            status = "âœ…" if improvement > 0 else "âš ï¸"
        else:
            improvement = 0
            status = "â–"
        
        print(f"{metric.upper():<15} {gcn_val:<12.4f} {base_val:<12.4f} "
              f"{improvement:>+10.2f}%  {status}")
    
    print("â”"*70)


if __name__ == '__main__':
    """æµ‹è¯•è¯„ä¼°å™¨"""
    print("\n" + "="*70)
    print("ğŸ§ª GCNè¯„ä¼°å™¨æµ‹è¯•")
    print("="*70)
    
    # åŠ è½½æ¨¡å‹å’Œæ•°æ®
    print("\n1. åŠ è½½æ¨¡å‹å’Œæ•°æ®...")
    data = torch.load('data/processed/homo_graph.pt')
    
    from gcn import create_gcn_model
    model = create_gcn_model(data.num_node_features, architecture='default')
    
    # å°è¯•åŠ è½½è®­ç»ƒå¥½çš„æ¨¡å‹
    try:
        checkpoint = torch.load('outputs/models/best_model.pt', map_location='cpu')
        model.load_state_dict(checkpoint['model_state_dict'])
        print("   âœ“ åŠ è½½è®­ç»ƒå¥½çš„æ¨¡å‹")
    except:
        print("   âš ï¸ æœªæ‰¾åˆ°è®­ç»ƒå¥½çš„æ¨¡å‹ï¼Œä½¿ç”¨éšæœºåˆå§‹åŒ–")
    
    # åˆ›å»ºè¯„ä¼°å™¨
    print("\n2. åˆ›å»ºè¯„ä¼°å™¨...")
    evaluator = GCNEvaluator(model, data, device='cpu')
    
    # å®Œæ•´è¯„ä¼°
    print("\n3. å¼€å§‹å®Œæ•´è¯„ä¼°...")
    results = evaluator.full_evaluation(save_dir='outputs/evaluation')
    
    print("\n" + "="*70)
    print("âœ… è¯„ä¼°å™¨æµ‹è¯•å®Œæˆ")
    print("="*70)
