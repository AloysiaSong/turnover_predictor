"""
åˆ†ç±»é˜ˆå€¼ä¼˜åŒ–å·¥å…·
æ‰¾åˆ°æœ€ä½³çš„åˆ†ç±»é˜ˆå€¼ä»¥å¹³è¡¡Precisionå’ŒRecall
"""
import numpy as np
import matplotlib.pyplot as plt
from sklearn.metrics import precision_recall_curve, f1_score, precision_score, recall_score
import json
from pathlib import Path


class ThresholdOptimizer:
    """åˆ†ç±»é˜ˆå€¼ä¼˜åŒ–å™¨"""
    
    def __init__(self, y_true, y_prob):
        """
        Args:
            y_true: çœŸå®æ ‡ç­¾
            y_prob: é¢„æµ‹æ¦‚ç‡
        """
        self.y_true = y_true
        self.y_prob = y_prob
        
    def find_best_threshold(self, metric='f1'):
        """
        å¯»æ‰¾æœ€ä½³é˜ˆå€¼
        
        Args:
            metric: ä¼˜åŒ–æŒ‡æ ‡ ('f1', 'balanced', 'youden')
            
        Returns:
            best_threshold, best_score, results
        """
        # æ‰©å¤§æœç´¢èŒƒå›´ï¼Œä½¿ç”¨æ›´ç»†çš„æ­¥é•¿
        thresholds = np.arange(0.05, 0.95, 0.01)
        
        results = {
            'thresholds': [],
            'precision': [],
            'recall': [],
            'f1': [],
            'accuracy': []
        }
        
        for threshold in thresholds:
            y_pred = (self.y_prob >= threshold).astype(int)
            
            # è·³è¿‡æ‰€æœ‰é¢„æµ‹ä¸ºåŒä¸€ç±»çš„æƒ…å†µ
            if len(np.unique(y_pred)) < 2:
                continue
            
            precision = precision_score(self.y_true, y_pred, zero_division=0)
            recall = recall_score(self.y_true, y_pred, zero_division=0)
            f1 = f1_score(self.y_true, y_pred, zero_division=0)
            accuracy = (self.y_true == y_pred).mean()
            
            results['thresholds'].append(threshold)
            results['precision'].append(precision)
            results['recall'].append(recall)
            results['f1'].append(f1)
            results['accuracy'].append(accuracy)
        
        # æ ¹æ®æŒ‡æ ‡é€‰æ‹©æœ€ä½³é˜ˆå€¼
        if metric == 'f1':
            idx = np.argmax(results['f1'])
            best_score = results['f1'][idx]
        elif metric == 'balanced':
            # å¹³è¡¡Precisionå’ŒRecall
            scores = [p * r for p, r in zip(results['precision'], results['recall'])]
            idx = np.argmax(scores)
            best_score = scores[idx]
        elif metric == 'youden':
            # Youden's J statistic
            scores = [r - (1 - p) for p, r in zip(results['precision'], results['recall'])]
            idx = np.argmax(scores)
            best_score = scores[idx]
        else:
            raise ValueError(f"Unknown metric: {metric}")
        
        best_threshold = results['thresholds'][idx]
        
        return best_threshold, best_score, results
    
    def plot_threshold_analysis(self, save_path='results/mlp/threshold_analysis.png'):
        """ç»˜åˆ¶é˜ˆå€¼åˆ†æå›¾"""
        _, _, results = self.find_best_threshold('f1')
        
        fig, axes = plt.subplots(2, 2, figsize=(14, 10))
        
        # 1. Precision vs Threshold
        axes[0, 0].plot(results['thresholds'], results['precision'], 'b-', linewidth=2)
        axes[0, 0].set_xlabel('Threshold', fontsize=11)
        axes[0, 0].set_ylabel('Precision', fontsize=11)
        axes[0, 0].set_title('Precision vs Threshold', fontsize=12)
        axes[0, 0].grid(True, alpha=0.3)
        axes[0, 0].axvline(0.5, color='r', linestyle='--', label='Default (0.5)')
        axes[0, 0].legend()
        
        # 2. Recall vs Threshold
        axes[0, 1].plot(results['thresholds'], results['recall'], 'g-', linewidth=2)
        axes[0, 1].set_xlabel('Threshold', fontsize=11)
        axes[0, 1].set_ylabel('Recall', fontsize=11)
        axes[0, 1].set_title('Recall vs Threshold', fontsize=12)
        axes[0, 1].grid(True, alpha=0.3)
        axes[0, 1].axvline(0.5, color='r', linestyle='--', label='Default (0.5)')
        axes[0, 1].legend()
        
        # 3. F1 Score vs Threshold
        axes[1, 0].plot(results['thresholds'], results['f1'], 'orange', linewidth=2)
        best_f1_idx = np.argmax(results['f1'])
        best_f1_threshold = results['thresholds'][best_f1_idx]
        best_f1 = results['f1'][best_f1_idx]
        axes[1, 0].scatter([best_f1_threshold], [best_f1], color='red', s=100, zorder=5)
        axes[1, 0].set_xlabel('Threshold', fontsize=11)
        axes[1, 0].set_ylabel('F1 Score', fontsize=11)
        axes[1, 0].set_title(f'F1 Score vs Threshold (Best={best_f1_threshold:.2f})', fontsize=12)
        axes[1, 0].grid(True, alpha=0.3)
        axes[1, 0].axvline(0.5, color='r', linestyle='--', label='Default (0.5)')
        axes[1, 0].legend()
        
        # 4. Precision-Recall Trade-off
        axes[1, 1].plot(results['thresholds'], results['precision'], 'b-', label='Precision', linewidth=2)
        axes[1, 1].plot(results['thresholds'], results['recall'], 'g-', label='Recall', linewidth=2)
        axes[1, 1].plot(results['thresholds'], results['f1'], 'orange', label='F1', linewidth=2)
        axes[1, 1].set_xlabel('Threshold', fontsize=11)
        axes[1, 1].set_ylabel('Score', fontsize=11)
        axes[1, 1].set_title('Precision-Recall-F1 Trade-off', fontsize=12)
        axes[1, 1].grid(True, alpha=0.3)
        axes[1, 1].axvline(0.5, color='r', linestyle='--', alpha=0.5)
        axes[1, 1].legend()
        
        plt.tight_layout()
        
        # ä¿å­˜
        Path(save_path).parent.mkdir(parents=True, exist_ok=True)
        plt.savefig(save_path, dpi=300, bbox_inches='tight')
        plt.close()
        
        print(f"âœ… é˜ˆå€¼åˆ†æå›¾å·²ä¿å­˜: {save_path}")
        
        return best_f1_threshold, best_f1
    
    def generate_threshold_report(self, save_dir='results/mlp'):
        """ç”Ÿæˆé˜ˆå€¼ä¼˜åŒ–æŠ¥å‘Š"""
        save_dir = Path(save_dir)
        save_dir.mkdir(parents=True, exist_ok=True)
        
        print("\n" + "="*60)
        print("é˜ˆå€¼ä¼˜åŒ–åˆ†æ")
        print("="*60)
        
        # ä¸åŒæŒ‡æ ‡ä¸‹çš„æœ€ä½³é˜ˆå€¼
        metrics_to_optimize = ['f1', 'balanced']
        
        results_summary = {}
        
        for metric in metrics_to_optimize:
            best_threshold, best_score, _ = self.find_best_threshold(metric)
            
            # è®¡ç®—è¯¥é˜ˆå€¼ä¸‹çš„æ‰€æœ‰æŒ‡æ ‡
            y_pred = (self.y_prob >= best_threshold).astype(int)
            
            precision = precision_score(self.y_true, y_pred, zero_division=0)
            recall = recall_score(self.y_true, y_pred, zero_division=0)
            f1 = f1_score(self.y_true, y_pred, zero_division=0)
            accuracy = (self.y_true == y_pred).mean()
            
            results_summary[metric] = {
                'threshold': float(best_threshold),
                'precision': float(precision),
                'recall': float(recall),
                'f1': float(f1),
                'accuracy': float(accuracy)
            }
            
            print(f"\nä¼˜åŒ–æŒ‡æ ‡: {metric}")
            print(f"  æœ€ä½³é˜ˆå€¼: {best_threshold:.3f}")
            print(f"  Precision: {precision:.4f}")
            print(f"  Recall: {recall:.4f}")
            print(f"  F1: {f1:.4f}")
            print(f"  Accuracy: {accuracy:.4f}")
        
        # é»˜è®¤é˜ˆå€¼0.5çš„æ€§èƒ½
        y_pred_default = (self.y_prob >= 0.5).astype(int)
        default_results = {
            'threshold': 0.5,
            'precision': float(precision_score(self.y_true, y_pred_default, zero_division=0)),
            'recall': float(recall_score(self.y_true, y_pred_default, zero_division=0)),
            'f1': float(f1_score(self.y_true, y_pred_default, zero_division=0)),
            'accuracy': float((self.y_true == y_pred_default).mean())
        }
        results_summary['default'] = default_results
        
        print(f"\né»˜è®¤é˜ˆå€¼: 0.5")
        print(f"  Precision: {default_results['precision']:.4f}")
        print(f"  Recall: {default_results['recall']:.4f}")
        print(f"  F1: {default_results['f1']:.4f}")
        print(f"  Accuracy: {default_results['accuracy']:.4f}")
        
        # æ·»åŠ å»ºè®®
        print("\n" + "="*60)
        print("ğŸ’¡ é˜ˆå€¼é€‰æ‹©å»ºè®®")
        print("="*60)
        
        f1_threshold = results_summary['f1']['threshold']
        f1_improvement = results_summary['f1']['f1'] - default_results['f1']
        
        print(f"\n1. å¦‚æœè¿½æ±‚F1æœ€ä¼˜:")
        print(f"   æ¨èé˜ˆå€¼: {f1_threshold:.3f}")
        print(f"   F1æå‡: {f1_improvement:+.4f} ({f1_improvement/default_results['f1']*100:+.1f}%)")
        print(f"   æƒè¡¡: Precision={results_summary['f1']['precision']:.3f}, Recall={results_summary['f1']['recall']:.3f}")
        
        # æ‰¾ä¸€ä¸ªå¹³è¡¡çš„é˜ˆå€¼ï¼ˆPrecisionå’ŒRecallè¾ƒæ¥è¿‘ï¼‰
        best_balanced_idx = None
        min_diff = float('inf')
        _, _, all_results = self.find_best_threshold('f1')
        
        for i, (p, r) in enumerate(zip(all_results['precision'], all_results['recall'])):
            if p > 0.3 and r > 0.5:  # ç¡®ä¿æœ€ä½æ ‡å‡†
                diff = abs(p - r)
                if diff < min_diff:
                    min_diff = diff
                    best_balanced_idx = i
        
        if best_balanced_idx is not None:
            balanced_threshold = all_results['thresholds'][best_balanced_idx]
            balanced_p = all_results['precision'][best_balanced_idx]
            balanced_r = all_results['recall'][best_balanced_idx]
            balanced_f1 = all_results['f1'][best_balanced_idx]
            
            print(f"\n2. å¦‚æœè¿½æ±‚Precisionå’ŒRecallå¹³è¡¡:")
            print(f"   æ¨èé˜ˆå€¼: {balanced_threshold:.3f}")
            print(f"   Precision: {balanced_p:.3f}")
            print(f"   Recall: {balanced_r:.3f}")
            print(f"   F1: {balanced_f1:.3f}")
        
        print(f"\n3. ä¸šåŠ¡åœºæ™¯å»ºè®®:")
        print(f"   â€¢ é‡è§†å¬å›ç‡ï¼ˆä¸æƒ³æ¼æ‰ç¦»èŒå‘˜å·¥ï¼‰â†’ ä½¿ç”¨è¾ƒä½é˜ˆå€¼ï¼ˆ0.3-0.4ï¼‰")
        print(f"   â€¢ é‡è§†ç²¾ç¡®ç‡ï¼ˆå‡å°‘è¯¯æŠ¥æˆæœ¬ï¼‰â†’ ä½¿ç”¨è¾ƒé«˜é˜ˆå€¼ï¼ˆ0.5-0.6ï¼‰")
        print(f"   â€¢ å¹³è¡¡è€ƒè™‘ â†’ ä½¿ç”¨F1æœ€ä¼˜é˜ˆå€¼")
        
        # ä¿å­˜ç»“æœ
        with open(save_dir / 'threshold_optimization.json', 'w') as f:
            json.dump(results_summary, f, indent=2)
        
        # ç»˜åˆ¶åˆ†æå›¾
        best_threshold, best_f1 = self.plot_threshold_analysis(
            save_path=save_dir / 'threshold_analysis.png'
        )
        
        print(f"\nâœ… é˜ˆå€¼ä¼˜åŒ–æŠ¥å‘Šå·²ä¿å­˜: {save_dir}")
        
        return results_summary


def optimize_threshold(y_true, y_prob, save_dir='results/mlp'):
    """
    å¿«é€Ÿè¿›è¡Œé˜ˆå€¼ä¼˜åŒ–
    
    Args:
        y_true: çœŸå®æ ‡ç­¾
        y_prob: é¢„æµ‹æ¦‚ç‡
        save_dir: ä¿å­˜ç›®å½•
        
    Returns:
        results_summary: ä¼˜åŒ–ç»“æœæ‘˜è¦
    """
    optimizer = ThresholdOptimizer(y_true, y_prob)
    results_summary = optimizer.generate_threshold_report(save_dir)
    
    return results_summary


if __name__ == '__main__':
    # ç¤ºä¾‹ç”¨æ³•
    np.random.seed(42)
    y_true = np.random.randint(0, 2, 100)
    y_prob = np.random.rand(100)
    
    results = optimize_threshold(y_true, y_prob)
    print("\nä¼˜åŒ–ç»“æœ:")
    print(json.dumps(results, indent=2))