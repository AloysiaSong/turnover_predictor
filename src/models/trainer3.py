"""
GCNè®­ç»ƒå™¨ï¼ˆå¢å¼ºç‰ˆï¼‰
==================
æ ¸å¿ƒæ”¹è¿›:
1. æ—©åœç›‘æ§Val_AUCPR
2. å®Œæ•´çš„è¯„ä¼°æŒ‡æ ‡ï¼ˆAUPR, AUROC, F1, Accç­‰ï¼‰
3. é˜ˆå€¼æ‰«æä¸é€‰æ‹©
4. æ— æ•°æ®æ³„æ¼
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.optim import Adam
from torch.optim.lr_scheduler import ReduceLROnPlateau
from sklearn.metrics import (
    average_precision_score,
    roc_auc_score,
    f1_score,
    accuracy_score,
    precision_score,
    recall_score,
    confusion_matrix,
    precision_recall_curve,
    roc_curve
)
import numpy as np
from typing import Dict, Tuple, Optional
from pathlib import Path
import json
from tqdm import tqdm


def compute_metrics(
    y_true: np.ndarray,
    y_prob: np.ndarray,
    threshold: float = 0.5
) -> Dict[str, float]:
    """
    è®¡ç®—å®Œæ•´è¯„ä¼°æŒ‡æ ‡
    
    å‚æ•°:
        y_true: çœŸå®æ ‡ç­¾ [N]
        y_prob: é¢„æµ‹æ¦‚ç‡ [N]
        threshold: äºŒåˆ†ç±»é˜ˆå€¼
        
    è¿”å›:
        metrics: æŒ‡æ ‡å­—å…¸
    """
    y_pred = (y_prob >= threshold).astype(int)
    
    metrics = {
        # æ¦‚ç‡æŒ‡æ ‡
        'aupr': average_precision_score(y_true, y_prob),
        'auroc': roc_auc_score(y_true, y_prob),
        
        # äºŒåˆ†ç±»æŒ‡æ ‡
        'accuracy': accuracy_score(y_true, y_pred),
        'precision': precision_score(y_true, y_pred, zero_division=0),
        'recall': recall_score(y_true, y_pred, zero_division=0),
        'f1': f1_score(y_true, y_pred, zero_division=0),
        
        # æ··æ·†çŸ©é˜µ
        'tn': 0, 'fp': 0, 'fn': 0, 'tp': 0
    }
    
    try:
        tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()
        metrics.update({'tn': int(tn), 'fp': int(fp), 'fn': int(fn), 'tp': int(tp)})
    except:
        pass
    
    return metrics


def find_optimal_threshold(
    y_true: np.ndarray,
    y_prob: np.ndarray,
    metric: str = 'f1',
    thresholds: Optional[np.ndarray] = None
) -> Tuple[float, float]:
    """
    åœ¨éªŒè¯é›†ä¸Šå¯»æ‰¾æœ€ä¼˜é˜ˆå€¼
    
    å‚æ•°:
        y_true: çœŸå®æ ‡ç­¾
        y_prob: é¢„æµ‹æ¦‚ç‡
        metric: ä¼˜åŒ–ç›®æ ‡ {'f1', 'recall', 'precision'}
        thresholds: å€™é€‰é˜ˆå€¼åˆ—è¡¨
        
    è¿”å›:
        best_threshold: æœ€ä¼˜é˜ˆå€¼
        best_score: æœ€ä¼˜åˆ†æ•°
    """
    if thresholds is None:
        thresholds = np.arange(0.05, 0.75, 0.02)
    
    best_threshold = 0.5
    best_score = 0.0
    
    for t in thresholds:
        y_pred = (y_prob >= t).astype(int)
        
        if metric == 'f1':
            score = f1_score(y_true, y_pred, zero_division=0)
        elif metric == 'recall':
            score = recall_score(y_true, y_pred, zero_division=0)
        elif metric == 'precision':
            score = precision_score(y_true, y_pred, zero_division=0)
        else:
            raise ValueError(f"Unknown metric: {metric}")
        
        if score > best_score:
            best_score = score
            best_threshold = t
    
    return best_threshold, best_score


class GCNTrainer:
    """
    GCNè®­ç»ƒå™¨
    
    å‚æ•°:
        model: GCNæ¨¡å‹
        data: PyG Dataå¯¹è±¡
        device: è®¾å¤‡
        lr: å­¦ä¹ ç‡
        weight_decay: L2æ­£åˆ™åŒ–
        pos_weight: æ­£æ ·æœ¬æƒé‡ï¼ˆè‡ªåŠ¨è®¡ç®—æˆ–æ‰‹åŠ¨æŒ‡å®šï¼‰
    """
    
    def __init__(
        self,
        model: nn.Module,
        data,
        device: str = 'cpu',
        lr: float = 0.01,
        weight_decay: float = 5e-4,
        pos_weight: Optional[float] = None
    ):
        self.model = model.to(device)
        self.data = data
        self.device = device
        
        # è‡ªåŠ¨è®¡ç®—pos_weight
        if pos_weight is None:
            num_pos = data.y[data.train_mask].sum().item()
            num_neg = data.train_mask.sum().item() - num_pos
            pos_weight = num_neg / num_pos if num_pos > 0 else 1.0
        
        self.pos_weight = torch.tensor([pos_weight], device=device)
        
        # æŸå¤±å‡½æ•°
        self.criterion = nn.BCEWithLogitsLoss(pos_weight=self.pos_weight)
        
        # ä¼˜åŒ–å™¨
        self.optimizer = Adam(model.parameters(), lr=lr, weight_decay=weight_decay)
        
        # å­¦ä¹ ç‡è°ƒåº¦
        self.scheduler = ReduceLROnPlateau(
            self.optimizer,
            mode='max',  # ç›‘æ§Val_AUPRï¼ˆè¶Šå¤§è¶Šå¥½ï¼‰
            factor=0.5,
            patience=10,
            verbose=True
        )
        
        # æ—©åœçŠ¶æ€
        self.best_val_aupr = 0.0
        self.best_epoch = 0
        self.patience_counter = 0
        
    def train_epoch(self) -> float:
        """è®­ç»ƒä¸€ä¸ªepoch"""
        self.model.train()
        self.optimizer.zero_grad()
        
        # å‰å‘ä¼ æ’­
        out = self.model(
            self.data.x,
            self.data.edge_index,
            self.data.edge_attr if hasattr(self.data, 'edge_attr') else None
        )
        
        # è®¡ç®—æŸå¤±ï¼ˆä»…è®­ç»ƒé›†ï¼‰
        loss = self.criterion(
            out[self.data.train_mask].squeeze(),
            self.data.y[self.data.train_mask].float()
        )
        
        # åå‘ä¼ æ’­
        loss.backward()
        self.optimizer.step()
        
        return loss.item()
    
    @torch.no_grad()
    def evaluate(self, mask: torch.Tensor) -> Dict[str, float]:
        """
        è¯„ä¼°æ¨¡å‹
        
        å‚æ•°:
            mask: æ•°æ®é›†mask
            
        è¿”å›:
            metrics: è¯„ä¼°æŒ‡æ ‡
        """
        self.model.eval()
        
        # é¢„æµ‹
        out = self.model(
            self.data.x,
            self.data.edge_index,
            self.data.edge_attr if hasattr(self.data, 'edge_attr') else None
        )
        
        logits = out[mask].squeeze()
        probs = torch.sigmoid(logits).cpu().numpy()
        labels = self.data.y[mask].cpu().numpy()
        
        # è®¡ç®—æŒ‡æ ‡ï¼ˆé»˜è®¤é˜ˆå€¼0.5ï¼‰
        metrics = compute_metrics(labels, probs, threshold=0.5)
        
        return metrics
    
    def train(
        self,
        epochs: int = 200,
        early_stopping_patience: int = 20,
        save_dir: str = 'outputs/models',
        verbose: bool = True
    ) -> Dict:
        """
        å®Œæ•´è®­ç»ƒæµç¨‹
        
        å‚æ•°:
            epochs: æœ€å¤§è®­ç»ƒè½®æ•°
            early_stopping_patience: æ—©åœè€å¿ƒ
            save_dir: æ¨¡å‹ä¿å­˜ç›®å½•
            verbose: æ˜¯å¦æ‰“å°è¿›åº¦
            
        è¿”å›:
            history: è®­ç»ƒå†å²
        """
        Path(save_dir).mkdir(parents=True, exist_ok=True)
        
        history = {
            'train_loss': [],
            'val_aupr': [],
            'val_auroc': [],
            'val_f1': [],
            'lr': []
        }
        
        if verbose:
            print(f"\n{'='*70}")
            print(f"ğŸš€ å¼€å§‹è®­ç»ƒï¼ˆæ—©åœç›‘æ§: Val_AUPRï¼‰")
            print(f"{'='*70}")
            print(f"   Epochs: {epochs}")
            print(f"   Early Stopping Patience: {early_stopping_patience}")
            print(f"   pos_weight: {self.pos_weight.item():.2f}")
        
        pbar = tqdm(range(epochs), desc='Training') if verbose else range(epochs)
        
        for epoch in pbar:
            # è®­ç»ƒ
            train_loss = self.train_epoch()
            
            # éªŒè¯
            val_metrics = self.evaluate(self.data.val_mask)
            val_aupr = val_metrics['aupr']
            
            # è®°å½•å†å²
            history['train_loss'].append(train_loss)
            history['val_aupr'].append(val_aupr)
            history['val_auroc'].append(val_metrics['auroc'])
            history['val_f1'].append(val_metrics['f1'])
            history['lr'].append(self.optimizer.param_groups[0]['lr'])
            
            # æ›´æ–°è¿›åº¦æ¡
            if verbose:
                pbar.set_postfix({
                    'T_Loss': f"{train_loss:.4f}",
                    'V_AUPR': f"{val_aupr:.4f}",
                    'V_F1': f"{val_metrics['f1']:.4f}"
                })
            
            # æ—©åœæ£€æŸ¥ï¼ˆåŸºäºVal_AUPRï¼‰
            if val_aupr > self.best_val_aupr:
                self.best_val_aupr = val_aupr
                self.best_epoch = epoch
                self.patience_counter = 0
                
                # ä¿å­˜æœ€ä½³æ¨¡å‹
                torch.save({
                    'epoch': epoch,
                    'model_state_dict': self.model.state_dict(),
                    'optimizer_state_dict': self.optimizer.state_dict(),
                    'val_aupr': val_aupr,
                    'val_metrics': val_metrics
                }, f"{save_dir}/best_model.pt")
                
                if verbose and epoch > 0:
                    print(f"\n   â­ Epoch {epoch+1}: æ–°çš„æœ€ä½³æ¨¡å‹! Val_AUPR={val_aupr:.4f}")
            else:
                self.patience_counter += 1
            
            # å­¦ä¹ ç‡è°ƒåº¦
            self.scheduler.step(val_aupr)
            
            # æ—©åœ
            if self.patience_counter >= early_stopping_patience:
                if verbose:
                    print(f"\n   ğŸ›‘ Early stopping at epoch {epoch+1}")
                    print(f"   ğŸ“Œ Best epoch: {self.best_epoch+1}, Val_AUPR={self.best_val_aupr:.4f}")
                break
        
        if verbose:
            print(f"\n{'='*70}")
            print(f"âœ… è®­ç»ƒå®Œæˆ")
            print(f"{'='*70}")
            print(f"   æœ€ä½³Epoch: {self.best_epoch+1}")
            print(f"   æœ€ä½³Val_AUPR: {self.best_val_aupr:.4f}")
        
        # ä¿å­˜è®­ç»ƒå†å²
        with open(f"{save_dir}/training_history.json", 'w') as f:
            json.dump(history, f, indent=2)
        
        return history
    
    @torch.no_grad()
    def predict_proba(self, mask: Optional[torch.Tensor] = None) -> Tuple[np.ndarray, np.ndarray]:
        """
        é¢„æµ‹æ¦‚ç‡
        
        å‚æ•°:
            mask: æ•°æ®é›†maskï¼ˆé»˜è®¤æ‰€æœ‰èŠ‚ç‚¹ï¼‰
            
        è¿”å›:
            probs: é¢„æµ‹æ¦‚ç‡
            labels: çœŸå®æ ‡ç­¾
        """
        self.model.eval()
        
        out = self.model(
            self.data.x,
            self.data.edge_index,
            self.data.edge_attr if hasattr(self.data, 'edge_attr') else None
        )
        
        if mask is None:
            mask = torch.ones(len(out), dtype=torch.bool, device=out.device)
        
        logits = out[mask].squeeze()
        probs = torch.sigmoid(logits).cpu().numpy()
        labels = self.data.y[mask].cpu().numpy()
        
        return probs, labels


def quick_train(
    model: nn.Module,
    data,
    epochs: int = 200,
    lr: float = 0.01,
    weight_decay: float = 5e-4,
    early_stopping_patience: int = 20,
    device: str = 'cpu',
    save_dir: str = 'outputs/models'
) -> Tuple[GCNTrainer, Dict]:
    """
    å¿«é€Ÿè®­ç»ƒå‡½æ•°
    
    è¿”å›:
        trainer: è®­ç»ƒå™¨å¯¹è±¡
        history: è®­ç»ƒå†å²
    """
    trainer = GCNTrainer(
        model=model,
        data=data,
        device=device,
        lr=lr,
        weight_decay=weight_decay
    )
    
    history = trainer.train(
        epochs=epochs,
        early_stopping_patience=early_stopping_patience,
        save_dir=save_dir,
        verbose=True
    )
    
    return trainer, history
