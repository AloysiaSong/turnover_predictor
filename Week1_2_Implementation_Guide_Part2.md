# Week 1-2 å®æ–½æŒ‡å—ï¼ˆç»­ï¼‰ï¼šDay 7-14

**æ¥Day 1-6ï¼ˆè§Week1_2_Implementation_Guide.mdï¼‰**

---

## ğŸ¯ Day 11-12: è¯„ä¼°æŒ‡æ ‡ä¸æ¨¡å‹åˆ†æ

### ä»»åŠ¡æ¸…å•
- [ ] å®ç°è¯„ä¼°æŒ‡æ ‡è®¡ç®—
- [ ] ç”Ÿæˆæ··æ·†çŸ©é˜µ
- [ ] ç»˜åˆ¶ROC/PRæ›²çº¿
- [ ] ç‰¹å¾é‡è¦æ€§åˆ†æ

### 6.1 è¯„ä¼°å™¨

**æ–‡ä»¶**: `src/evaluation/evaluator.py`

```python
"""
æ¨¡å‹è¯„ä¼°æ¨¡å—
"""
import numpy as np
import torch
from sklearn.metrics import (
    roc_auc_score, f1_score, precision_score, recall_score,
    accuracy_score, confusion_matrix, classification_report,
    roc_curve, precision_recall_curve, average_precision_score
)
import matplotlib.pyplot as plt
import seaborn as sns
from pathlib import Path


class Evaluator:
    """æ¨¡å‹è¯„ä¼°å™¨"""
    
    @staticmethod
    def compute_metrics(y_true, y_pred, y_prob):
        """
        è®¡ç®—æ‰€æœ‰è¯„ä¼°æŒ‡æ ‡
        
        Args:
            y_true: çœŸå®æ ‡ç­¾
            y_pred: é¢„æµ‹æ ‡ç­¾ (0/1)
            y_prob: é¢„æµ‹æ¦‚ç‡ (0-1)
            
        Returns:
            metrics: æŒ‡æ ‡å­—å…¸
        """
        metrics = {
            # åˆ†ç±»æŒ‡æ ‡
            'accuracy': accuracy_score(y_true, y_pred),
            'precision': precision_score(y_true, y_pred, zero_division=0),
            'recall': recall_score(y_true, y_pred, zero_division=0),
            'f1': f1_score(y_true, y_pred, zero_division=0),
            
            # AUCæŒ‡æ ‡
            'roc_auc': roc_auc_score(y_true, y_prob),
            'pr_auc': average_precision_score(y_true, y_prob),
            
            # æ··æ·†çŸ©é˜µ
            'confusion_matrix': confusion_matrix(y_true, y_pred).tolist()
        }
        
        return metrics
    
    @staticmethod
    def print_metrics(metrics, set_name='Test'):
        """æ‰“å°æŒ‡æ ‡"""
        print(f"\n{'='*60}")
        print(f"{set_name} Set Metrics")
        print(f"{'='*60}")
        
        print(f"Accuracy:  {metrics['accuracy']:.4f}")
        print(f"Precision: {metrics['precision']:.4f}")
        print(f"Recall:    {metrics['recall']:.4f}")
        print(f"F1 Score:  {metrics['f1']:.4f}")
        print(f"ROC AUC:   {metrics['roc_auc']:.4f}")
        print(f"PR AUC:    {metrics['pr_auc']:.4f}")
        
        cm = np.array(metrics['confusion_matrix'])
        print(f"\nConfusion Matrix:")
        print(f"              Predicted")
        print(f"              0      1")
        print(f"Actual   0  {cm[0,0]:4d}  {cm[0,1]:4d}")
        print(f"         1  {cm[1,0]:4d}  {cm[1,1]:4d}")
    
    @staticmethod
    def plot_confusion_matrix(y_true, y_pred, save_path=None):
        """ç»˜åˆ¶æ··æ·†çŸ©é˜µ"""
        cm = confusion_matrix(y_true, y_pred)
        
        plt.figure(figsize=(8, 6))
        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',
                    xticklabels=['ä¸ç¦»èŒ', 'ç¦»èŒ'],
                    yticklabels=['ä¸ç¦»èŒ', 'ç¦»èŒ'])
        plt.title('Confusion Matrix', fontsize=14, fontweight='bold')
        plt.ylabel('True Label', fontsize=12)
        plt.xlabel('Predicted Label', fontsize=12)
        
        if save_path:
            plt.savefig(save_path, dpi=300, bbox_inches='tight')
            print(f"âœ… æ··æ·†çŸ©é˜µå·²ä¿å­˜: {save_path}")
        else:
            plt.show()
        
        plt.close()
    
    @staticmethod
    def plot_roc_curve(y_true, y_prob, save_path=None):
        """ç»˜åˆ¶ROCæ›²çº¿"""
        fpr, tpr, thresholds = roc_curve(y_true, y_prob)
        roc_auc = roc_auc_score(y_true, y_prob)
        
        plt.figure(figsize=(8, 6))
        plt.plot(fpr, tpr, color='darkorange', lw=2,
                 label=f'ROC curve (AUC = {roc_auc:.4f})')
        plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--',
                 label='Random Classifier')
        plt.xlim([0.0, 1.0])
        plt.ylim([0.0, 1.05])
        plt.xlabel('False Positive Rate', fontsize=12)
        plt.ylabel('True Positive Rate', fontsize=12)
        plt.title('ROC Curve', fontsize=14, fontweight='bold')
        plt.legend(loc="lower right")
        plt.grid(alpha=0.3)
        
        if save_path:
            plt.savefig(save_path, dpi=300, bbox_inches='tight')
            print(f"âœ… ROCæ›²çº¿å·²ä¿å­˜: {save_path}")
        else:
            plt.show()
        
        plt.close()
    
    @staticmethod
    def plot_precision_recall_curve(y_true, y_prob, save_path=None):
        """ç»˜åˆ¶Precision-Recallæ›²çº¿"""
        precision, recall, thresholds = precision_recall_curve(y_true, y_prob)
        pr_auc = average_precision_score(y_true, y_prob)
        
        plt.figure(figsize=(8, 6))
        plt.plot(recall, precision, color='purple', lw=2,
                 label=f'PR curve (AUC = {pr_auc:.4f})')
        plt.xlabel('Recall', fontsize=12)
        plt.ylabel('Precision', fontsize=12)
        plt.title('Precision-Recall Curve', fontsize=14, fontweight='bold')
        plt.legend(loc="lower left")
        plt.grid(alpha=0.3)
        
        if save_path:
            plt.savefig(save_path, dpi=300, bbox_inches='tight')
            print(f"âœ… PRæ›²çº¿å·²ä¿å­˜: {save_path}")
        else:
            plt.show()
        
        plt.close()
    
    @staticmethod
    def plot_training_history(history, save_path=None):
        """ç»˜åˆ¶è®­ç»ƒå†å²"""
        fig, axes = plt.subplots(1, 2, figsize=(14, 5))
        
        # Lossæ›²çº¿
        axes[0].plot(history['train_loss'], label='Train Loss', linewidth=2)
        axes[0].plot(history['val_loss'], label='Val Loss', linewidth=2)
        axes[0].set_xlabel('Epoch', fontsize=12)
        axes[0].set_ylabel('Loss', fontsize=12)
        axes[0].set_title('Training & Validation Loss', fontsize=14, fontweight='bold')
        axes[0].legend()
        axes[0].grid(alpha=0.3)
        
        # å¦‚æœæœ‰é¢å¤–æŒ‡æ ‡
        if history.get('train_metrics'):
            train_aucs = [m.get('roc_auc', 0) for m in history['train_metrics']]
            val_aucs = [m.get('roc_auc', 0) for m in history['val_metrics']]
            
            axes[1].plot(train_aucs, label='Train AUC', linewidth=2)
            axes[1].plot(val_aucs, label='Val AUC', linewidth=2)
            axes[1].set_xlabel('Epoch', fontsize=12)
            axes[1].set_ylabel('AUC', fontsize=12)
            axes[1].set_title('Training & Validation AUC', fontsize=14, fontweight='bold')
            axes[1].legend()
            axes[1].grid(alpha=0.3)
        
        plt.tight_layout()
        
        if save_path:
            plt.savefig(save_path, dpi=300, bbox_inches='tight')
            print(f"âœ… è®­ç»ƒå†å²å·²ä¿å­˜: {save_path}")
        else:
            plt.show()
        
        plt.close()
    
    @staticmethod
    def generate_report(y_true, y_pred, y_prob, save_path=None):
        """ç”Ÿæˆå®Œæ•´è¯„ä¼°æŠ¥å‘Š"""
        # è®¡ç®—æŒ‡æ ‡
        metrics = Evaluator.compute_metrics(y_true, y_pred, y_prob)
        
        # åˆ†ç±»æŠ¥å‘Š
        report = classification_report(
            y_true, y_pred,
            target_names=['ä¸ç¦»èŒ', 'ç¦»èŒ'],
            digits=4
        )
        
        # ç»„åˆæŠ¥å‘Š
        full_report = f"""
{'='*60}
æ¨¡å‹è¯„ä¼°æŠ¥å‘Š
{'='*60}

æ€»ä½“æŒ‡æ ‡:
-----------
Accuracy:  {metrics['accuracy']:.4f}
Precision: {metrics['precision']:.4f}
Recall:    {metrics['recall']:.4f}
F1 Score:  {metrics['f1']:.4f}
ROC AUC:   {metrics['roc_auc']:.4f}
PR AUC:    {metrics['pr_auc']:.4f}

æ··æ·†çŸ©é˜µ:
-----------
{np.array(metrics['confusion_matrix'])}

è¯¦ç»†åˆ†ç±»æŠ¥å‘Š:
-----------
{report}
"""
        
        print(full_report)
        
        if save_path:
            with open(save_path, 'w', encoding='utf-8') as f:
                f.write(full_report)
            print(f"\nâœ… æŠ¥å‘Šå·²ä¿å­˜: {save_path}")
        
        return metrics


def main():
    """æ¼”ç¤ºè¯„ä¼°å™¨ä½¿ç”¨"""
    # æ¨¡æ‹Ÿé¢„æµ‹ç»“æœ
    np.random.seed(42)
    n_samples = 100
    y_true = np.random.randint(0, 2, n_samples)
    y_prob = np.random.rand(n_samples)
    y_pred = (y_prob > 0.5).astype(int)
    
    # åˆ›å»ºè¾“å‡ºç›®å½•
    output_dir = Path('../outputs/evaluation')
    output_dir.mkdir(parents=True, exist_ok=True)
    
    # è¯„ä¼°
    evaluator = Evaluator()
    
    # ç”ŸæˆæŠ¥å‘Š
    metrics = evaluator.generate_report(
        y_true, y_pred, y_prob,
        save_path=output_dir / 'evaluation_report.txt'
    )
    
    # ç»˜åˆ¶å›¾è¡¨
    evaluator.plot_confusion_matrix(
        y_true, y_pred,
        save_path=output_dir / 'confusion_matrix.png'
    )
    
    evaluator.plot_roc_curve(
        y_true, y_prob,
        save_path=output_dir / 'roc_curve.png'
    )
    
    evaluator.plot_precision_recall_curve(
        y_true, y_prob,
        save_path=output_dir / 'pr_curve.png'
    )
    
    print("\nâœ… è¯„ä¼°å®Œæˆï¼")


if __name__ == '__main__':
    main()
```

---

## ğŸ¯ Day 13-14: å®Œæ•´è®­ç»ƒæµç¨‹ä¸æœ€ç»ˆæŠ¥å‘Š

### ä»»åŠ¡æ¸…å•
- [ ] æ•´åˆæ‰€æœ‰ç»„ä»¶
- [ ] åˆ›å»ºç«¯åˆ°ç«¯è®­ç»ƒè„šæœ¬
- [ ] ç”Ÿæˆå®Œæ•´æ€§èƒ½æŠ¥å‘Š
- [ ] æ•´ç†æ–‡æ¡£

### 7.1 å®Œæ•´è®­ç»ƒè„šæœ¬

**æ–‡ä»¶**: `train_mlp_baseline.py`

```python
"""
MLPåŸºçº¿æ¨¡å‹å®Œæ•´è®­ç»ƒè„šæœ¬
"""
import sys
sys.path.append('src')

import numpy as np
import torch
import json
from pathlib import Path

# å¯¼å…¥è‡ªå®šä¹‰æ¨¡å—
from data_processing.load_data import DataLoader
from features.feature_extractor import FeatureExtractor
from data_processing.label_extractor import LabelExtractor
from data_processing.data_splitter import DataSplitter
from models.mlp_baseline import create_mlp_model
from models.trainer import Trainer, create_dataloaders
from evaluation.evaluator import Evaluator


def main():
    """ä¸»è®­ç»ƒæµç¨‹"""
    print("\n" + "="*80)
    print("MLPåŸºçº¿æ¨¡å‹è®­ç»ƒç®¡é“")
    print("="*80)
    
    # ========================================
    # 1. åŠ è½½æ•°æ®
    # ========================================
    print("\nã€æ­¥éª¤1/7ã€‘åŠ è½½åŸå§‹æ•°æ®...")
    loader = DataLoader('data/raw/originaldata.csv')
    df = loader.load()
    
    # ========================================
    # 2. ç‰¹å¾æå–
    # ========================================
    print("\nã€æ­¥éª¤2/7ã€‘æå–ç‰¹å¾...")
    
    # æ£€æŸ¥æ˜¯å¦å·²æœ‰å¤„ç†å¥½çš„ç‰¹å¾
    features_path = Path('data/processed/employee_features.npy')
    
    if features_path.exists():
        print("âœ… åŠ è½½å·²ä¿å­˜çš„ç‰¹å¾...")
        X = np.load(features_path)
    else:
        print("âš™ï¸ æå–æ–°ç‰¹å¾...")
        feature_extractor = FeatureExtractor()
        X, feature_names = feature_extractor.extract_all_features(df, fit=True)
        
        # ä¿å­˜ç‰¹å¾
        np.save(features_path, X)
        feature_extractor.save('models/feature_extractor.pkl')
    
    print(f"ç‰¹å¾å½¢çŠ¶: {X.shape}")
    
    # ========================================
    # 3. æ ‡ç­¾æå–
    # ========================================
    print("\nã€æ­¥éª¤3/7ã€‘æå–æ ‡ç­¾...")
    
    labels_path = Path('data/processed/y_turnover_binary.npy')
    
    if labels_path.exists():
        print("âœ… åŠ è½½å·²ä¿å­˜çš„æ ‡ç­¾...")
        y = np.load(labels_path)
    else:
        print("âš™ï¸ æå–æ–°æ ‡ç­¾...")
        label_extractor = LabelExtractor()
        y, _ = label_extractor.extract_turnover_labels(df)
        np.save(labels_path, y)
    
    print(f"æ ‡ç­¾å½¢çŠ¶: {y.shape}")
    print(f"æ­£æ ·æœ¬: {y.sum()} ({y.mean()*100:.1f}%)")
    
    # ========================================
    # 4. æ•°æ®åˆ’åˆ†
    # ========================================
    print("\nã€æ­¥éª¤4/7ã€‘åˆ’åˆ†æ•°æ®é›†...")
    
    split_dir = Path('data/splits')
    
    if (split_dir / 'train_idx.npy').exists():
        print("âœ… åŠ è½½å·²ä¿å­˜çš„åˆ’åˆ†...")
        splitter = DataSplitter()
        splitter.load(split_dir)
    else:
        print("âš™ï¸ åˆ›å»ºæ–°åˆ’åˆ†...")
        splitter = DataSplitter(
            train_ratio=0.68,
            val_ratio=0.12,
            test_ratio=0.20,
            random_state=42
        )
        splitter.split(y, len(y))
        splitter.save(split_dir)
    
    # è·å–ç´¢å¼•
    train_idx = splitter.train_idx
    val_idx = splitter.val_idx
    test_idx = splitter.test_idx
    
    # åˆ’åˆ†æ•°æ®
    X_train, y_train = X[train_idx], y[train_idx]
    X_val, y_val = X[val_idx], y[val_idx]
    X_test, y_test = X[test_idx], y[test_idx]
    
    print(f"è®­ç»ƒé›†: {len(X_train)} æ ·æœ¬")
    print(f"éªŒè¯é›†: {len(X_val)} æ ·æœ¬")
    print(f"æµ‹è¯•é›†: {len(X_test)} æ ·æœ¬")
    
    # ========================================
    # 5. åˆ›å»ºæ•°æ®åŠ è½½å™¨
    # ========================================
    print("\nã€æ­¥éª¤5/7ã€‘åˆ›å»ºæ•°æ®åŠ è½½å™¨...")
    
    train_loader, val_loader, test_loader = create_dataloaders(
        X_train, y_train,
        X_val, y_val,
        X_test, y_test,
        batch_size=32,
        num_workers=0
    )
    
    print(f"âœ… Train batches: {len(train_loader)}")
    print(f"âœ… Val batches: {len(val_loader)}")
    print(f"âœ… Test batches: {len(test_loader)}")
    
    # ========================================
    # 6. è®­ç»ƒæ¨¡å‹
    # ========================================
    print("\nã€æ­¥éª¤6/7ã€‘è®­ç»ƒMLPæ¨¡å‹...")
    
    # åˆ›å»ºæ¨¡å‹
    model = create_mlp_model(
        input_dim=X.shape[1],
        architecture='default',  # å¯é€‰: 'shallow', 'default', 'deep'
        dropout=0.5
    )
    
    print(f"\næ¨¡å‹æ¶æ„:")
    print(model)
    n_params = sum(p.numel() for p in model.parameters())
    print(f"æ€»å‚æ•°é‡: {n_params:,}")
    
    # åˆ›å»ºè®­ç»ƒå™¨
    device = 'cuda' if torch.cuda.is_available() else 'cpu'
    trainer = Trainer(
        model=model,
        device=device,
        learning_rate=0.001,
        weight_decay=1e-4,
        pos_weight=7.9  # æ ¹æ®ç±»åˆ«ä¸å¹³è¡¡æ¯”è°ƒæ•´
    )
    
    # è®­ç»ƒ
    history = trainer.fit(
        train_loader=train_loader,
        val_loader=val_loader,
        epochs=100,
        early_stopping_patience=15,
        save_dir='models/mlp'
    )
    
    # ========================================
    # 7. è¯„ä¼°æ¨¡å‹
    # ========================================
    print("\nã€æ­¥éª¤7/7ã€‘è¯„ä¼°æ¨¡å‹...")
    
    # åœ¨æµ‹è¯•é›†ä¸Šè¯„ä¼°
    _, (y_prob_test, y_pred_test, y_true_test) = trainer.evaluate(
        test_loader, return_predictions=True
    )
    
    # åˆ›å»ºè¯„ä¼°å™¨
    evaluator = Evaluator()
    
    # è®¡ç®—æŒ‡æ ‡
    test_metrics = evaluator.compute_metrics(
        y_true_test, y_pred_test, y_prob_test
    )
    
    # æ‰“å°æŒ‡æ ‡
    evaluator.print_metrics(test_metrics, set_name='Test')
    
    # åˆ›å»ºè¾“å‡ºç›®å½•
    output_dir = Path('outputs/mlp_baseline')
    output_dir.mkdir(parents=True, exist_ok=True)
    
    # ç”ŸæˆæŠ¥å‘Š
    evaluator.generate_report(
        y_true_test, y_pred_test, y_prob_test,
        save_path=output_dir / 'evaluation_report.txt'
    )
    
    # ç»˜åˆ¶å›¾è¡¨
    print("\nğŸ“Š ç”Ÿæˆå¯è§†åŒ–...")
    
    evaluator.plot_confusion_matrix(
        y_true_test, y_pred_test,
        save_path=output_dir / 'confusion_matrix.png'
    )
    
    evaluator.plot_roc_curve(
        y_true_test, y_prob_test,
        save_path=output_dir / 'roc_curve.png'
    )
    
    evaluator.plot_precision_recall_curve(
        y_true_test, y_prob_test,
        save_path=output_dir / 'pr_curve.png'
    )
    
    evaluator.plot_training_history(
        history,
        save_path=output_dir / 'training_history.png'
    )
    
    # ä¿å­˜æœ€ç»ˆç»“æœ
    results = {
        'model': 'MLP Baseline',
        'architecture': 'default [128, 64, 32]',
        'n_parameters': n_params,
        'train_samples': len(X_train),
        'val_samples': len(X_val),
        'test_samples': len(X_test),
        'metrics': test_metrics,
        'best_epoch': len(history['train_loss'])
    }
    
    with open(output_dir / 'results.json', 'w') as f:
        json.dump(results, f, indent=2, default=str)
    
    print("\n" + "="*80)
    print("âœ… è®­ç»ƒå®Œæˆï¼")
    print("="*80)
    print(f"\næ¨¡å‹å·²ä¿å­˜åˆ°: models/mlp/best_model.pt")
    print(f"è¯„ä¼°ç»“æœå·²ä¿å­˜åˆ°: {output_dir}/")
    print(f"\næœ€ç»ˆæµ‹è¯•é›†æ€§èƒ½:")
    print(f"  - ROC AUC: {test_metrics['roc_auc']:.4f}")
    print(f"  - F1 Score: {test_metrics['f1']:.4f}")
    print(f"  - Precision: {test_metrics['precision']:.4f}")
    print(f"  - Recall: {test_metrics['recall']:.4f}")


if __name__ == '__main__':
    main()
```

### 7.2 ä¸€é”®è¿è¡Œè„šæœ¬

**æ–‡ä»¶**: `run_baseline.sh`

```bash
#!/bin/bash

echo "=================================="
echo "MLPåŸºçº¿æ¨¡å‹è®­ç»ƒæµç¨‹"
echo "=================================="

# æ£€æŸ¥Pythonç¯å¢ƒ
if ! command -v python &> /dev/null
then
    echo "âŒ Pythonæœªå®‰è£…"
    exit 1
fi

echo "âœ… Pythonç‰ˆæœ¬: $(python --version)"

# åˆ›å»ºå¿…è¦ç›®å½•
echo -e "\nğŸ“ åˆ›å»ºé¡¹ç›®ç›®å½•..."
mkdir -p data/{raw,processed,splits}
mkdir -p models/mlp
mkdir -p outputs/{figures,mlp_baseline}
mkdir -p src/{data_processing,features,models,evaluation}

# æ£€æŸ¥æ•°æ®æ–‡ä»¶
if [ ! -f "data/raw/originaldata.csv" ]; then
    echo "âŒ è¯·å°†originaldata.csvæ”¾åˆ° data/raw/ ç›®å½•"
    exit 1
fi

echo "âœ… æ•°æ®æ–‡ä»¶å·²æ‰¾åˆ°"

# å®‰è£…ä¾èµ–ï¼ˆå¦‚æœéœ€è¦ï¼‰
echo -e "\nğŸ“¦ æ£€æŸ¥ä¾èµ–..."
pip install -q -r requirements.txt

# è¿è¡Œè®­ç»ƒ
echo -e "\nğŸš€ å¼€å§‹è®­ç»ƒ..."
python train_mlp_baseline.py

# æ£€æŸ¥ç»“æœ
if [ -f "outputs/mlp_baseline/results.json" ]; then
    echo -e "\nâœ… è®­ç»ƒæˆåŠŸï¼"
    echo -e "\nğŸ“Š æŸ¥çœ‹ç»“æœ:"
    echo "  - æ¨¡å‹: models/mlp/best_model.pt"
    echo "  - æŠ¥å‘Š: outputs/mlp_baseline/evaluation_report.txt"
    echo "  - å›¾è¡¨: outputs/mlp_baseline/*.png"
else
    echo -e "\nâŒ è®­ç»ƒå¤±è´¥ï¼Œè¯·æ£€æŸ¥æ—¥å¿—"
    exit 1
fi

echo -e "\n=================================="
echo "å®Œæˆï¼"
echo "=================================="
```

### 7.3 Windowsè¿è¡Œè„šæœ¬

**æ–‡ä»¶**: `run_baseline.bat`

```batch
@echo off
echo ==================================
echo MLPåŸºçº¿æ¨¡å‹è®­ç»ƒæµç¨‹
echo ==================================

REM æ£€æŸ¥Python
python --version >nul 2>&1
if %errorlevel% neq 0 (
    echo âŒ Pythonæœªå®‰è£…
    exit /b 1
)

echo âœ… Pythonå·²å®‰è£…

REM åˆ›å»ºç›®å½•
echo.
echo ğŸ“ åˆ›å»ºé¡¹ç›®ç›®å½•...
mkdir data\raw 2>nul
mkdir data\processed 2>nul
mkdir data\splits 2>nul
mkdir models\mlp 2>nul
mkdir outputs\figures 2>nul
mkdir outputs\mlp_baseline 2>nul

REM æ£€æŸ¥æ•°æ®
if not exist data\raw\originaldata.csv (
    echo âŒ è¯·å°†originaldata.csvæ”¾åˆ° data\raw\ ç›®å½•
    exit /b 1
)

echo âœ… æ•°æ®æ–‡ä»¶å·²æ‰¾åˆ°

REM å®‰è£…ä¾èµ–
echo.
echo ğŸ“¦ æ£€æŸ¥ä¾èµ–...
pip install -q -r requirements.txt

REM è¿è¡Œè®­ç»ƒ
echo.
echo ğŸš€ å¼€å§‹è®­ç»ƒ...
python train_mlp_baseline.py

REM æ£€æŸ¥ç»“æœ
if exist outputs\mlp_baseline\results.json (
    echo.
    echo âœ… è®­ç»ƒæˆåŠŸï¼
    echo.
    echo ğŸ“Š æŸ¥çœ‹ç»“æœ:
    echo   - æ¨¡å‹: models\mlp\best_model.pt
    echo   - æŠ¥å‘Š: outputs\mlp_baseline\evaluation_report.txt
    echo   - å›¾è¡¨: outputs\mlp_baseline\*.png
) else (
    echo.
    echo âŒ è®­ç»ƒå¤±è´¥ï¼Œè¯·æ£€æŸ¥æ—¥å¿—
    exit /b 1
)

echo.
echo ==================================
echo å®Œæˆï¼
echo ==================================
pause
```

### ä½¿ç”¨æ–¹æ³•

```bash
# Linux/Mac
chmod +x run_baseline.sh
./run_baseline.sh

# Windows
run_baseline.bat

# æˆ–ç›´æ¥è¿è¡ŒPythonè„šæœ¬
python train_mlp_baseline.py
```

---

## ğŸ“ Week 1-2 å®Œæˆæ£€æŸ¥æ¸…å•

### Day 1-2: ç¯å¢ƒä¸æ•°æ® âœ“
- [x] Pythonç¯å¢ƒé…ç½®
- [x] ä¾èµ–åŒ…å®‰è£…
- [x] é¡¹ç›®ç›®å½•ç»“æ„åˆ›å»º
- [x] æ•°æ®åŠ è½½ä¸æ¢ç´¢
- [x] æ•°æ®è´¨é‡æŠ¥å‘Š

### Day 3-4: ç‰¹å¾å·¥ç¨‹ âœ“
- [x] åŸºç¡€ç‰¹å¾æå–ï¼ˆ7ç»´ï¼‰
- [x] äººå²—åŒ¹é…ç‰¹å¾ï¼ˆ5ç»´ï¼‰
- [x] æŠ€èƒ½ç‰¹å¾ï¼ˆ30ç»´ï¼‰
- [x] ç»æµæŸå¤±ç‰¹å¾ï¼ˆ5ç»´ï¼‰
- [x] ç‰¹å¾æ ‡å‡†åŒ–
- [x] ç‰¹å¾ä¿å­˜

### Day 5-6: å›¾æ•°æ®å‡†å¤‡ âœ“
- [x] å‘˜å·¥-å²—ä½è¾¹æ„å»º
- [x] å‘˜å·¥-å…¬å¸å±æ€§è¾¹æ„å»º
- [x] åå¥½è¾¹æ„å»ºï¼ˆå¯é€‰ï¼‰
- [x] è¾¹éªŒè¯ä¸ä¿å­˜

### Day 7-8: æ•°æ®åˆ’åˆ† âœ“
- [x] åˆ†å±‚æŠ½æ ·åˆ’åˆ†
- [x] Train/Val/Test split
- [x] Maskåˆ›å»º
- [x] æ•°æ®åˆ†å¸ƒéªŒè¯

### Day 9-10: MLPåŸºçº¿ âœ“
- [x] MLPæ¨¡å‹å®ç°
- [x] è®­ç»ƒå™¨å®ç°
- [x] æ•°æ®åŠ è½½å™¨åˆ›å»º
- [x] æŸå¤±å‡½æ•°è®¾ç½®

### Day 11-12: è¯„ä¼° âœ“
- [x] è¯„ä¼°æŒ‡æ ‡è®¡ç®—
- [x] æ··æ·†çŸ©é˜µ
- [x] ROC/PRæ›²çº¿
- [x] è®­ç»ƒå†å²å¯è§†åŒ–

### Day 13-14: æ•´åˆ âœ“
- [x] ç«¯åˆ°ç«¯è®­ç»ƒè„šæœ¬
- [x] ä¸€é”®è¿è¡Œè„šæœ¬
- [x] å®Œæ•´æŠ¥å‘Šç”Ÿæˆ
- [x] æ–‡æ¡£æ•´ç†

---

## ğŸ“Š é¢„æœŸè¾“å‡º

### æ–‡ä»¶ç»“æ„
```
hgnn_turnover_prediction/
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw/
â”‚   â”‚   â””â”€â”€ originaldata.csv
â”‚   â”œâ”€â”€ processed/
â”‚   â”‚   â”œâ”€â”€ employee_features.npy
â”‚   â”‚   â”œâ”€â”€ feature_names.txt
â”‚   â”‚   â”œâ”€â”€ y_turnover_binary.npy
â”‚   â”‚   â”œâ”€â”€ y_turnover_prob.npy
â”‚   â”‚   â””â”€â”€ edges/
â”‚   â”‚       â”œâ”€â”€ employee_works_as_post_type.pt
â”‚   â”‚       â”œâ”€â”€ employee_at_size_company_size.pt
â”‚   â”‚       â””â”€â”€ employee_at_type_company_type.pt
â”‚   â””â”€â”€ splits/
â”‚       â”œâ”€â”€ train_idx.npy
â”‚       â”œâ”€â”€ val_idx.npy
â”‚       â”œâ”€â”€ test_idx.npy
â”‚       â””â”€â”€ split_config.json
â”œâ”€â”€ models/
â”‚   â”œâ”€â”€ feature_extractor.pkl
â”‚   â””â”€â”€ mlp/
â”‚       â”œâ”€â”€ best_model.pt
â”‚       â””â”€â”€ training_history.json
â”œâ”€â”€ outputs/
â”‚   â”œâ”€â”€ figures/
â”‚   â”‚   â”œâ”€â”€ turnover_distribution.png
â”‚   â”‚   â”œâ”€â”€ post_distribution.png
â”‚   â”‚   â””â”€â”€ data_validation.png
â”‚   â””â”€â”€ mlp_baseline/
â”‚       â”œâ”€â”€ confusion_matrix.png
â”‚       â”œâ”€â”€ roc_curve.png
â”‚       â”œâ”€â”€ pr_curve.png
â”‚       â”œâ”€â”€ training_history.png
â”‚       â”œâ”€â”€ evaluation_report.txt
â”‚       â””â”€â”€ results.json
â””â”€â”€ src/
    â”œâ”€â”€ data_processing/
    â”œâ”€â”€ features/
    â”œâ”€â”€ models/
    â””â”€â”€ evaluation/
```

### å…³é”®æ€§èƒ½æŒ‡æ ‡ï¼ˆé¢„æœŸï¼‰
```
æ¨¡å‹: MLP Baseline
-----------------
ROC AUC:   0.72 - 0.78
F1 Score:  0.35 - 0.45
Precision: 0.40 - 0.55
Recall:    0.30 - 0.45
```

---

## ğŸš€ ä¸‹ä¸€æ­¥ï¼ˆWeek 3-4ï¼‰

1. **HomoGNNå®ç°**: ä½¿ç”¨åŒæ„å›¾ç¥ç»ç½‘ç»œ
2. **HeteroGNNå®ç°**: å®ç°å¼‚æ„å›¾ç‰ˆæœ¬
3. **å¤šä»»åŠ¡å­¦ä¹ **: æ·»åŠ å²—ä½åå¥½loss
4. **è¶…å‚æ•°ä¼˜åŒ–**: Grid searchæˆ–Bayesian optimization
5. **æ¶ˆèå®éªŒ**: å¯¹æ¯”ä¸åŒæ¨¡å‹ç»„ä»¶çš„è´¡çŒ®

---

**Week 1-2å®æ–½æŒ‡å—å®Œæˆï¼æ‰€æœ‰ä»£ç å‡å¯ç›´æ¥è¿è¡Œã€‚**
