# Week 3-4: GNNæ¨¡å‹å¼€å‘ - è¯¦ç»†å®æ–½æŒ‡å—

**é˜¶æ®µ**: ç¬¬äºŒéƒ¨åˆ†  
**æ—¶é—´**: 10-14å¤©  
**ç›®æ ‡**: å®ç°åŒæ„/å¼‚æ„GNNï¼Œè¶…è¶ŠMLPåŸºçº¿ï¼Œè¾¾åˆ°AUC 0.93+

---

## ğŸ“‹ æ€»ä½“è§„åˆ’

### ç¬¬äºŒéƒ¨åˆ†é‡Œç¨‹ç¢‘

| é˜¶æ®µ | å¤©æ•° | å…³é”®ä»»åŠ¡ | é¢„æœŸäº§å‡º | ç›®æ ‡æŒ‡æ ‡ |
|------|------|---------|---------|---------|
| **Week 3 Part 1** | Day 1-3 | åŒæ„å›¾æ„å»º + GCN | GCNæ¨¡å‹ | AUC 0.91+ |
| **Week 3 Part 2** | Day 4-7 | GAT + GraphSAGE | æ³¨æ„åŠ›æœºåˆ¶ | AUC 0.92+ |
| **Week 4 Part 1** | Day 8-10 | å¼‚æ„HAN | HANæ¨¡å‹ | AUC 0.93+ |
| **Week 4 Part 2** | Day 11-14 | å¤šä»»åŠ¡ + å¯¹æ¯” | å®Œæ•´æŠ¥å‘Š | æœ€ç»ˆè¯„ä¼° |

### å‰ç½®æ¡ä»¶æ£€æŸ¥

```bash
# âœ… ç¡®è®¤ç¬¬ä¸€éƒ¨åˆ†å·²å®Œæˆ
[ ] data/processed/employee_features.npy å­˜åœ¨
[ ] data/processed/y_turnover_binary.npy å­˜åœ¨
[ ] data/edges/ ç›®å½•å­˜åœ¨ï¼ˆ5ç§è¾¹ç±»å‹æ–‡ä»¶ï¼‰
[ ] data/splits/ ç›®å½•å­˜åœ¨ï¼ˆtrain/val/testç´¢å¼•ï¼‰
[ ] models/mlp/best_model.pt å­˜åœ¨
[ ] ç¬¬ä¸€éƒ¨åˆ† MLP åŸºçº¿ AUC â‰¥ 0.75
```

---

## ğŸ¯ Week 3: åŒæ„GNNæ¨¡å‹

### Day 1-3: GCNå®ç°

#### Day 1: åŒæ„å›¾æ•°æ®å‡†å¤‡

**ä»»åŠ¡æ¸…å•**
- [ ] å°†å¼‚æ„å›¾è½¬æ¢ä¸ºåŒæ„å›¾
- [ ] æ„å»ºPyG Dataå¯¹è±¡
- [ ] éªŒè¯å›¾è¿é€šæ€§
- [ ] æ•°æ®ç»Ÿè®¡åˆ†æ

**è¯¦ç»†æ­¥éª¤**

```python
# Step 1: åˆ›å»º src/graph/homogeneous_graph_builder.py

"""
åŒæ„å›¾æ„å»ºå™¨
å°†å¼‚æ„å›¾è½¬æ¢ä¸ºåŒæ„å›¾ï¼ˆæ‰€æœ‰èŠ‚ç‚¹è§†ä¸ºåŒä¸€ç±»å‹ï¼‰
"""

import torch
import numpy as np
from torch_geometric.data import Data
from pathlib import Path


class HomogeneousGraphBuilder:
    """åŒæ„å›¾æ„å»ºå™¨"""
    
    def __init__(self, data_dir='data'):
        self.data_dir = Path(data_dir)
        self.processed_dir = self.data_dir / 'processed'
        self.edges_dir = self.data_dir / 'edges'
        self.splits_dir = self.data_dir / 'splits'
        
    def build(self):
        """æ„å»ºåŒæ„å›¾"""
        print("\n" + "="*60)
        print("æ„å»ºåŒæ„å›¾")
        print("="*60)
        
        # 1. åŠ è½½èŠ‚ç‚¹ç‰¹å¾
        print("\n1. åŠ è½½å‘˜å·¥èŠ‚ç‚¹ç‰¹å¾...")
        X = np.load(self.processed_dir / 'employee_features.npy')
        y = np.load(self.processed_dir / 'y_turnover_binary.npy')
        
        print(f"   å‘˜å·¥èŠ‚ç‚¹æ•°: {len(X)}")
        print(f"   ç‰¹å¾ç»´åº¦: {X.shape[1]}")
        print(f"   ç¦»èŒæ¯”ä¾‹: {y.mean():.2%}")
        
        # 2. æ„å»ºè¾¹ç´¢å¼•ï¼ˆåªä½¿ç”¨å‘˜å·¥ä¹‹é—´çš„è¿æ¥ï¼‰
        print("\n2. æ„å»ºè¾¹ç´¢å¼•...")
        edge_index = self._build_employee_edges()
        
        print(f"   è¾¹æ•°: {edge_index.shape[1]}")
        print(f"   å¹³å‡åº¦æ•°: {edge_index.shape[1] / len(X):.2f}")
        
        # 3. åŠ è½½åˆ’åˆ†mask
        print("\n3. åŠ è½½æ•°æ®åˆ’åˆ†...")
        train_mask = np.load(self.splits_dir / 'train_mask.npy')
        val_mask = np.load(self.splits_dir / 'val_mask.npy')
        test_mask = np.load(self.splits_dir / 'test_mask.npy')
        
        print(f"   è®­ç»ƒé›†: {train_mask.sum()} ({train_mask.mean():.1%})")
        print(f"   éªŒè¯é›†: {val_mask.sum()} ({val_mask.mean():.1%})")
        print(f"   æµ‹è¯•é›†: {test_mask.sum()} ({test_mask.mean():.1%})")
        
        # 4. åˆ›å»ºPyG Dataå¯¹è±¡
        print("\n4. åˆ›å»ºPyG Dataå¯¹è±¡...")
        data = Data(
            x=torch.FloatTensor(X),
            edge_index=torch.LongTensor(edge_index),
            y=torch.LongTensor(y),
            train_mask=torch.BoolTensor(train_mask),
            val_mask=torch.BoolTensor(val_mask),
            test_mask=torch.BoolTensor(test_mask)
        )
        
        # 5. éªŒè¯å›¾ç»“æ„
        print("\n5. å›¾ç»“æ„éªŒè¯...")
        self._validate_graph(data)
        
        # 6. ä¿å­˜
        save_path = self.processed_dir / 'homo_graph.pt'
        torch.save(data, save_path)
        print(f"\nâœ… åŒæ„å›¾å·²ä¿å­˜: {save_path}")
        
        return data
    
    def _build_employee_edges(self):
        """
        æ„å»ºå‘˜å·¥ä¹‹é—´çš„è¾¹
        ç­–ç•¥: åŸºäºå…±åŒå±æ€§ï¼ˆå²—ä½ã€å…¬å¸è§„æ¨¡ã€å…¬å¸ç±»å‹ï¼‰å»ºç«‹è¿æ¥
        """
        # åŠ è½½åŸå§‹æ•°æ®
        import pandas as pd
        df = pd.read_csv(
            self.data_dir / 'raw' / 'originaldata.csv',
            encoding='gbk',
            skiprows=1
        )
        
        # æå–å‘˜å·¥å±æ€§
        post_types = df['Q7å²—ä½ç±»å‹'].values
        company_sizes = df['Q8å…¬å¸äººå‘˜è§„æ¨¡'].values
        company_types = df['Q9å…¬å¸ç±»å‹'].values
        
        edges = []
        
        # ç­–ç•¥1: åŒå²—ä½å‘˜å·¥è¿æ¥
        for i in range(len(df)):
            for j in range(i+1, len(df)):
                # åŒå²—ä½
                if post_types[i] == post_types[j]:
                    edges.append([i, j])
                    edges.append([j, i])  # æ— å‘å›¾
                # åŒå…¬å¸è§„æ¨¡
                elif company_sizes[i] == company_sizes[j]:
                    edges.append([i, j])
                    edges.append([j, i])
                # åŒå…¬å¸ç±»å‹
                elif company_types[i] == company_types[j]:
                    edges.append([i, j])
                    edges.append([j, i])
        
        if not edges:
            # å¦‚æœæ²¡æœ‰è¾¹ï¼Œåˆ›å»ºè‡ªç¯
            edges = [[i, i] for i in range(len(df))]
        
        edge_index = np.array(edges).T
        
        return edge_index
    
    def _validate_graph(self, data):
        """éªŒè¯å›¾ç»“æ„"""
        print(f"   èŠ‚ç‚¹æ•°: {data.num_nodes}")
        print(f"   è¾¹æ•°: {data.num_edges}")
        print(f"   ç‰¹å¾ç»´åº¦: {data.num_node_features}")
        print(f"   æ˜¯å¦æœ‰å‘: {data.is_directed()}")
        print(f"   æ˜¯å¦æœ‰è‡ªç¯: {data.has_self_loops()}")
        print(f"   æ˜¯å¦æœ‰å­¤ç«‹èŠ‚ç‚¹: {data.has_isolated_nodes()}")
        
        # è¿é€šæ€§æ£€æŸ¥
        from torch_geometric.utils import to_networkx
        import networkx as nx
        
        G = to_networkx(data, to_undirected=True)
        is_connected = nx.is_connected(G)
        num_components = nx.number_connected_components(G)
        
        print(f"   æ˜¯å¦è¿é€š: {is_connected}")
        print(f"   è¿é€šåˆ†é‡æ•°: {num_components}")


def main():
    """æµ‹è¯•åŒæ„å›¾æ„å»º"""
    builder = HomogeneousGraphBuilder()
    data = builder.build()
    
    print("\n" + "="*60)
    print("âœ… åŒæ„å›¾æ„å»ºå®Œæˆï¼")
    print("="*60)
    
    return data


if __name__ == '__main__':
    main()
```

**è¿è¡ŒéªŒè¯**
```bash
python src/graph/homogeneous_graph_builder.py
```

**é¢„æœŸè¾“å‡º**
```
============================================================
æ„å»ºåŒæ„å›¾
============================================================

1. åŠ è½½å‘˜å·¥èŠ‚ç‚¹ç‰¹å¾...
   å‘˜å·¥èŠ‚ç‚¹æ•°: 500
   ç‰¹å¾ç»´åº¦: 47
   ç¦»èŒæ¯”ä¾‹: 11.20%

2. æ„å»ºè¾¹ç´¢å¼•...
   è¾¹æ•°: 8,156
   å¹³å‡åº¦æ•°: 16.31

3. åŠ è½½æ•°æ®åˆ’åˆ†...
   è®­ç»ƒé›†: 340 (68.0%)
   éªŒè¯é›†: 60 (12.0%)
   æµ‹è¯•é›†: 100 (20.0%)

4. åˆ›å»ºPyG Dataå¯¹è±¡...

5. å›¾ç»“æ„éªŒè¯...
   èŠ‚ç‚¹æ•°: 500
   è¾¹æ•°: 8,156
   ç‰¹å¾ç»´åº¦: 47
   æ˜¯å¦æœ‰å‘: False
   æ˜¯å¦æœ‰è‡ªç¯: False
   æ˜¯å¦æœ‰å­¤ç«‹èŠ‚ç‚¹: False
   æ˜¯å¦è¿é€š: True
   è¿é€šåˆ†é‡æ•°: 1

âœ… åŒæ„å›¾å·²ä¿å­˜: data/processed/homo_graph.pt
```

---

#### Day 2: GCNæ¨¡å‹å®ç°

**ä»»åŠ¡æ¸…å•**
- [ ] å®ç°GCNæ¨¡å‹ç±»
- [ ] å®šä¹‰å‰å‘ä¼ æ’­
- [ ] æµ‹è¯•æ¨¡å‹ç»“æ„
- [ ] éªŒè¯è¾“å‡ºå½¢çŠ¶

**è¯¦ç»†æ­¥éª¤**

```python
# Step 2: åˆ›å»º src/models/gcn.py

"""
Graph Convolutional Network (GCN) å®ç°
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
from torch_geometric.nn import GCNConv


class GCN(nn.Module):
    """
    GCNæ¨¡å‹
    
    Architecture:
        Input â†’ GCNConv â†’ ReLU â†’ Dropout
              â†’ GCNConv â†’ ReLU â†’ Dropout  
              â†’ GCNConv â†’ Linear â†’ Output
    """
    
    def __init__(
        self,
        in_channels,
        hidden_channels=128,
        num_layers=3,
        dropout=0.5,
        use_batch_norm=False
    ):
        """
        Args:
            in_channels: è¾“å…¥ç‰¹å¾ç»´åº¦
            hidden_channels: éšè—å±‚ç»´åº¦
            num_layers: GCNå±‚æ•°
            dropout: Dropoutæ¯”ä¾‹
            use_batch_norm: æ˜¯å¦ä½¿ç”¨BatchNorm
        """
        super().__init__()
        
        self.in_channels = in_channels
        self.hidden_channels = hidden_channels
        self.num_layers = num_layers
        self.dropout = dropout
        self.use_batch_norm = use_batch_norm
        
        # GCNå±‚
        self.convs = nn.ModuleList()
        self.convs.append(GCNConv(in_channels, hidden_channels))
        
        for _ in range(num_layers - 2):
            self.convs.append(GCNConv(hidden_channels, hidden_channels))
        
        # æœ€åä¸€å±‚è¾“å‡ºå›ºå®šç»´åº¦ï¼ˆç”¨äºåˆ†ç±»ï¼‰
        self.convs.append(GCNConv(hidden_channels, hidden_channels))
        
        # BatchNormï¼ˆå¯é€‰ï¼‰
        if use_batch_norm:
            self.bns = nn.ModuleList()
            for _ in range(num_layers - 1):
                self.bns.append(nn.BatchNorm1d(hidden_channels))
        
        # åˆ†ç±»å¤´
        self.classifier = nn.Linear(hidden_channels, 1)
        
        # Dropout
        self.dropout_layer = nn.Dropout(dropout)
        
    def forward(self, x, edge_index):
        """
        å‰å‘ä¼ æ’­
        
        Args:
            x: èŠ‚ç‚¹ç‰¹å¾ [num_nodes, in_channels]
            edge_index: è¾¹ç´¢å¼• [2, num_edges]
            
        Returns:
            out: é¢„æµ‹logits [num_nodes, 1]
        """
        # GCNå±‚
        for i, conv in enumerate(self.convs[:-1]):
            x = conv(x, edge_index)
            
            if self.use_batch_norm:
                x = self.bns[i](x)
            
            x = F.relu(x)
            x = self.dropout_layer(x)
        
        # æœ€åä¸€å±‚GCNï¼ˆä¸åŠ æ¿€æ´»ï¼‰
        x = self.convs[-1](x, edge_index)
        
        # åˆ†ç±»
        out = self.classifier(x)
        
        return out
    
    def predict_proba(self, x, edge_index):
        """é¢„æµ‹æ¦‚ç‡"""
        self.eval()
        with torch.no_grad():
            logits = self.forward(x, edge_index)
            probs = torch.sigmoid(logits)
        return probs


def create_gcn_model(in_channels, architecture='default', dropout=0.5):
    """
    GCNæ¨¡å‹å·¥å‚
    
    Args:
        in_channels: è¾“å…¥ç‰¹å¾ç»´åº¦
        architecture: æ¨¡å‹æ¶æ„
            - 'shallow': 2å±‚ï¼Œhidden=64
            - 'default': 3å±‚ï¼Œhidden=128
            - 'deep': 4å±‚ï¼Œhidden=256
        dropout: Dropoutæ¯”ä¾‹
        
    Returns:
        model: GCNæ¨¡å‹å®ä¾‹
    """
    architectures = {
        'shallow': {'hidden_channels': 64, 'num_layers': 2},
        'default': {'hidden_channels': 128, 'num_layers': 3},
        'deep': {'hidden_channels': 256, 'num_layers': 4}
    }
    
    if architecture not in architectures:
        raise ValueError(f"Unknown architecture: {architecture}")
    
    config = architectures[architecture]
    
    model = GCN(
        in_channels=in_channels,
        hidden_channels=config['hidden_channels'],
        num_layers=config['num_layers'],
        dropout=dropout,
        use_batch_norm=True
    )
    
    return model


def test_gcn():
    """æµ‹è¯•GCNæ¨¡å‹"""
    print("\n" + "="*60)
    print("GCNæ¨¡å‹æµ‹è¯•")
    print("="*60)
    
    # åˆ›å»ºæ¨¡å‹
    model = create_gcn_model(
        in_channels=47,
        architecture='default',
        dropout=0.5
    )
    
    print(f"\næ¨¡å‹æ¶æ„: default")
    print(f"è¾“å…¥ç»´åº¦: 47")
    print(f"éšè—ç»´åº¦: 128")
    print(f"å±‚æ•°: 3")
    print(f"Dropout: 0.5")
    
    # ç»Ÿè®¡å‚æ•°
    total_params = sum(p.numel() for p in model.parameters())
    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
    
    print(f"\næ€»å‚æ•°é‡: {total_params:,}")
    print(f"å¯è®­ç»ƒå‚æ•°: {trainable_params:,}")
    
    # æµ‹è¯•å‰å‘ä¼ æ’­
    print("\n" + "="*60)
    print("å‰å‘ä¼ æ’­æµ‹è¯•")
    print("="*60)
    
    # åˆ›å»ºæµ‹è¯•æ•°æ®
    num_nodes = 500
    num_edges = 2000
    
    x = torch.randn(num_nodes, 47)
    edge_index = torch.randint(0, num_nodes, (2, num_edges))
    
    print(f"\nè¾“å…¥å½¢çŠ¶: x={x.shape}, edge_index={edge_index.shape}")
    
    # å‰å‘ä¼ æ’­
    model.eval()
    with torch.no_grad():
        logits = model(x, edge_index)
        probs = torch.sigmoid(logits)
    
    print(f"è¾“å‡ºå½¢çŠ¶: logits={logits.shape}")
    print(f"æ¦‚ç‡èŒƒå›´: [{probs.min():.4f}, {probs.max():.4f}]")
    print(f"å¹³å‡æ¦‚ç‡: {probs.mean():.4f}")
    
    print("\nâœ… GCNæ¨¡å‹æµ‹è¯•é€šè¿‡ï¼")
    
    return model


if __name__ == '__main__':
    test_gcn()
```

**è¿è¡ŒéªŒè¯**
```bash
python src/models/gcn.py
```

**é¢„æœŸè¾“å‡º**
```
============================================================
GCNæ¨¡å‹æµ‹è¯•
============================================================

æ¨¡å‹æ¶æ„: default
è¾“å…¥ç»´åº¦: 47
éšè—ç»´åº¦: 128
å±‚æ•°: 3
Dropout: 0.5

æ€»å‚æ•°é‡: 38,785
å¯è®­ç»ƒå‚æ•°: 38,785

============================================================
å‰å‘ä¼ æ’­æµ‹è¯•
============================================================

è¾“å…¥å½¢çŠ¶: x=torch.Size([500, 47]), edge_index=torch.Size([2, 2000])
è¾“å‡ºå½¢çŠ¶: logits=torch.Size([500, 1])
æ¦‚ç‡èŒƒå›´: [0.3245, 0.6812]
å¹³å‡æ¦‚ç‡: 0.5123

âœ… GCNæ¨¡å‹æµ‹è¯•é€šè¿‡ï¼
```

---

#### Day 3: GCNè®­ç»ƒ

**ä»»åŠ¡æ¸…å•**
- [ ] å®ç°GCNè®­ç»ƒå™¨
- [ ] å®Œæ•´è®­ç»ƒæµç¨‹
- [ ] è¯„ä¼°æ€§èƒ½
- [ ] ä¸MLPå¯¹æ¯”

**è¯¦ç»†æ­¥éª¤**

```python
# Step 3: åˆ›å»º train_gcn.py

"""
GCNæ¨¡å‹è®­ç»ƒè„šæœ¬
"""

import torch
import numpy as np
from pathlib import Path
from sklearn.metrics import roc_auc_score, f1_score, precision_score, recall_score

# å¯¼å…¥æ¨¡å—
from src.models.gcn import create_gcn_model
from src.evaluation.evaluator import Evaluator


class GCNTrainer:
    """GCNè®­ç»ƒå™¨"""
    
    def __init__(
        self,
        model,
        device='cpu',
        learning_rate=0.01,
        weight_decay=5e-4
    ):
        self.model = model.to(device)
        self.device = device
        
        # ä¼˜åŒ–å™¨
        self.optimizer = torch.optim.Adam(
            model.parameters(),
            lr=learning_rate,
            weight_decay=weight_decay
        )
        
        # æŸå¤±å‡½æ•°ï¼ˆè€ƒè™‘ç±»åˆ«ä¸å¹³è¡¡ï¼‰
        pos_weight = torch.tensor([7.9]).to(device)
        self.criterion = torch.nn.BCEWithLogitsLoss(pos_weight=pos_weight)
        
    def train_epoch(self, data):
        """è®­ç»ƒä¸€ä¸ªepoch"""
        self.model.train()
        self.optimizer.zero_grad()
        
        # å‰å‘ä¼ æ’­
        out = self.model(data.x, data.edge_index)
        
        # åªè®¡ç®—è®­ç»ƒé›†çš„æŸå¤±
        loss = self.criterion(
            out[data.train_mask].squeeze(),
            data.y[data.train_mask].float()
        )
        
        # åå‘ä¼ æ’­
        loss.backward()
        self.optimizer.step()
        
        return loss.item()
    
    @torch.no_grad()
    def evaluate(self, data, mask):
        """è¯„ä¼°æ¨¡å‹"""
        self.model.eval()
        
        # é¢„æµ‹
        out = self.model(data.x, data.edge_index)
        probs = torch.sigmoid(out).squeeze()
        
        # æå–å¯¹åº”maskçš„é¢„æµ‹å’Œæ ‡ç­¾
        y_true = data.y[mask].cpu().numpy()
        y_prob = probs[mask].cpu().numpy()
        y_pred = (y_prob >= 0.5).astype(int)
        
        # è®¡ç®—æŒ‡æ ‡
        metrics = {
            'auc': roc_auc_score(y_true, y_prob),
            'f1': f1_score(y_true, y_pred, zero_division=0),
            'precision': precision_score(y_true, y_pred, zero_division=0),
            'recall': recall_score(y_true, y_pred, zero_division=0),
            'accuracy': (y_true == y_pred).mean()
        }
        
        return metrics, y_prob
    
    def fit(
        self,
        data,
        epochs=200,
        early_stopping_patience=20,
        verbose=True
    ):
        """å®Œæ•´è®­ç»ƒæµç¨‹"""
        print("\n" + "="*60)
        print("å¼€å§‹è®­ç»ƒGCNæ¨¡å‹")
        print("="*60)
        
        data = data.to(self.device)
        
        best_val_auc = 0
        patience_counter = 0
        history = {
            'train_loss': [],
            'val_auc': [],
            'val_f1': []
        }
        
        for epoch in range(epochs):
            # è®­ç»ƒ
            train_loss = self.train_epoch(data)
            
            # éªŒè¯
            val_metrics, _ = self.evaluate(data, data.val_mask)
            
            # è®°å½•
            history['train_loss'].append(train_loss)
            history['val_auc'].append(val_metrics['auc'])
            history['val_f1'].append(val_metrics['f1'])
            
            # æ—©åœ
            if val_metrics['auc'] > best_val_auc:
                best_val_auc = val_metrics['auc']
                patience_counter = 0
                
                # ä¿å­˜æœ€ä½³æ¨¡å‹
                torch.save({
                    'epoch': epoch,
                    'model_state_dict': self.model.state_dict(),
                    'optimizer_state_dict': self.optimizer.state_dict(),
                    'val_auc': best_val_auc
                }, 'models/gcn/best_model.pt')
            else:
                patience_counter += 1
            
            # æ‰“å°è¿›åº¦
            if verbose and (epoch + 1) % 10 == 0:
                print(f"Epoch {epoch+1:3d} | "
                      f"Loss: {train_loss:.4f} | "
                      f"Val AUC: {val_metrics['auc']:.4f} | "
                      f"Val F1: {val_metrics['f1']:.4f}")
            
            # æ—©åœåˆ¤æ–­
            if patience_counter >= early_stopping_patience:
                print(f"\nEarly stopping at epoch {epoch+1}")
                break
        
        print(f"\nâœ… è®­ç»ƒå®Œæˆï¼æœ€ä½³éªŒè¯AUC: {best_val_auc:.4f}")
        
        return history


def main():
    """ä¸»å‡½æ•°"""
    # è®¾ç½®éšæœºç§å­
    torch.manual_seed(42)
    np.random.seed(42)
    
    # è®¾å¤‡
    device = 'cuda' if torch.cuda.is_available() else 'cpu'
    print(f"ä½¿ç”¨è®¾å¤‡: {device}")
    
    # 1. åŠ è½½æ•°æ®
    print("\n1. åŠ è½½åŒæ„å›¾æ•°æ®...")
    data = torch.load('data/processed/homo_graph.pt')
    print(f"   èŠ‚ç‚¹æ•°: {data.num_nodes}")
    print(f"   è¾¹æ•°: {data.num_edges}")
    print(f"   ç‰¹å¾ç»´åº¦: {data.num_node_features}")
    
    # 2. åˆ›å»ºæ¨¡å‹
    print("\n2. åˆ›å»ºGCNæ¨¡å‹...")
    model = create_gcn_model(
        in_channels=data.num_node_features,
        architecture='default',
        dropout=0.5
    )
    print(f"   å‚æ•°é‡: {sum(p.numel() for p in model.parameters()):,}")
    
    # 3. è®­ç»ƒ
    print("\n3. è®­ç»ƒæ¨¡å‹...")
    trainer = GCNTrainer(
        model=model,
        device=device,
        learning_rate=0.01,
        weight_decay=5e-4
    )
    
    history = trainer.fit(
        data=data,
        epochs=200,
        early_stopping_patience=20
    )
    
    # 4. æµ‹è¯•é›†è¯„ä¼°
    print("\n4. æµ‹è¯•é›†è¯„ä¼°...")
    test_metrics, test_probs = trainer.evaluate(data, data.test_mask)
    
    print("\næµ‹è¯•é›†æ€§èƒ½:")
    print(f"  AUC:       {test_metrics['auc']:.4f}")
    print(f"  F1:        {test_metrics['f1']:.4f}")
    print(f"  Precision: {test_metrics['precision']:.4f}")
    print(f"  Recall:    {test_metrics['recall']:.4f}")
    print(f"  Accuracy:  {test_metrics['accuracy']:.4f}")
    
    # 5. å®Œæ•´è¯„ä¼°æŠ¥å‘Š
    print("\n5. ç”Ÿæˆè¯„ä¼°æŠ¥å‘Š...")
    evaluator = Evaluator(save_dir='results/gcn')
    
    y_test = data.y[data.test_mask].cpu().numpy()
    evaluator.evaluate_and_report(
        y_true=y_test,
        y_pred=(test_probs >= 0.5).astype(int),
        y_prob=test_probs,
        model_name='GCN'
    )
    
    # 6. å¯¹æ¯”MLP
    print("\n6. å¯¹æ¯”MLPåŸºçº¿...")
    mlp_results = {
        'auc': 0.909,  # ä»ç¬¬ä¸€éƒ¨åˆ†è·å–
        'f1': 0.516,
        'precision': 0.400,
        'recall': 0.727
    }
    
    print(f"\n{'æŒ‡æ ‡':<12} {'MLPåŸºçº¿':<12} {'GCN':<12} {'æå‡':<12}")
    print("-" * 50)
    for metric in ['auc', 'f1', 'precision', 'recall']:
        mlp_val = mlp_results[metric]
        gcn_val = test_metrics[metric]
        improvement = (gcn_val - mlp_val) / mlp_val * 100
        
        print(f"{metric.upper():<12} {mlp_val:<12.4f} {gcn_val:<12.4f} "
              f"{improvement:+.2f}%")
    
    print("\n" + "="*60)
    print("âœ… GCNè®­ç»ƒä¸è¯„ä¼°å®Œæˆï¼")
    print("="*60)


if __name__ == '__main__':
    # åˆ›å»ºå¿…è¦ç›®å½•
    Path('models/gcn').mkdir(parents=True, exist_ok=True)
    Path('results/gcn').mkdir(parents=True, exist_ok=True)
    
    main()
```

**è¿è¡Œè®­ç»ƒ**
```bash
python train_gcn.py
```

**é¢„æœŸè¾“å‡º**
```
ä½¿ç”¨è®¾å¤‡: cpu

1. åŠ è½½åŒæ„å›¾æ•°æ®...
   èŠ‚ç‚¹æ•°: 500
   è¾¹æ•°: 8,156
   ç‰¹å¾ç»´åº¦: 47

2. åˆ›å»ºGCNæ¨¡å‹...
   å‚æ•°é‡: 38,785

3. è®­ç»ƒæ¨¡å‹...
============================================================
å¼€å§‹è®­ç»ƒGCNæ¨¡å‹
============================================================
Epoch  10 | Loss: 0.4523 | Val AUC: 0.8234 | Val F1: 0.4102
Epoch  20 | Loss: 0.3891 | Val AUC: 0.8567 | Val F1: 0.4523
Epoch  30 | Loss: 0.3245 | Val AUC: 0.8892 | Val F1: 0.4891
Epoch  40 | Loss: 0.2934 | Val AUC: 0.9034 | Val F1: 0.5123
Epoch  50 | Loss: 0.2712 | Val AUC: 0.9156 | Val F1: 0.5234

Early stopping at epoch 58

âœ… è®­ç»ƒå®Œæˆï¼æœ€ä½³éªŒè¯AUC: 0.9156

4. æµ‹è¯•é›†è¯„ä¼°...

æµ‹è¯•é›†æ€§èƒ½:
  AUC:       0.9134
  F1:        0.5401
  Precision: 0.4523
  Recall:    0.6818
  Accuracy:  0.8600

5. ç”Ÿæˆè¯„ä¼°æŠ¥å‘Š...
âœ… è¯„ä¼°æŠ¥å‘Šå·²ä¿å­˜

6. å¯¹æ¯”MLPåŸºçº¿...

æŒ‡æ ‡          MLPåŸºçº¿        GCN          æå‡          
--------------------------------------------------
AUC         0.9091       0.9134       +0.47%
F1          0.5161       0.5401       +4.65%
PRECISION   0.4000       0.4523       +13.08%
RECALL      0.7273       0.6818       -6.25%

============================================================
âœ… GCNè®­ç»ƒä¸è¯„ä¼°å®Œæˆï¼
============================================================
```

---

### Day 1-3 æ£€æŸ¥æ¸…å•

```
Week 3 - Day 1-3: GCNå®ç°
========================

[ ] Day 1: åŒæ„å›¾æ•°æ®å‡†å¤‡
    [ ] HomogeneousGraphBuilder ç±»å®ç°
    [ ] åŒæ„å›¾æ„å»ºæˆåŠŸ
    [ ] å›¾è¿é€šæ€§éªŒè¯é€šè¿‡
    [ ] homo_graph.pt å·²ä¿å­˜

[ ] Day 2: GCNæ¨¡å‹å®ç°  
    [ ] GCN ç±»å®šä¹‰å®Œæˆ
    [ ] å‰å‘ä¼ æ’­æµ‹è¯•é€šè¿‡
    [ ] å‚æ•°ç»Ÿè®¡æ­£ç¡®
    [ ] æ¨¡å‹å¯ä»¥æ­£å¸¸å®ä¾‹åŒ–

[ ] Day 3: GCNè®­ç»ƒ
    [ ] è®­ç»ƒè„šæœ¬è¿è¡ŒæˆåŠŸ
    [ ] æ—©åœæœºåˆ¶ç”Ÿæ•ˆ
    [ ] æœ€ä½³æ¨¡å‹å·²ä¿å­˜
    [ ] æµ‹è¯•é›† AUC â‰¥ 0.91
    [ ] è¯„ä¼°æŠ¥å‘Šå·²ç”Ÿæˆ
    [ ] ä¸MLPå¯¹æ¯”å®Œæˆ

æ€§èƒ½ç›®æ ‡:
    [ç›®æ ‡] GCN AUC â‰¥ 0.91
    [å®é™…] GCN AUC = _______
    
    [ç›®æ ‡] ç›¸æ¯”MLPæå‡ â‰¥ 0.5%
    [å®é™…] æå‡ = _______%
```

---

## ğŸ“ Day 4-7 é¢„å‘Š

æ¥ä¸‹æ¥å°†å®ç°ï¼š

### Day 4-5: GAT (Graph Attention Network)
- æ³¨æ„åŠ›æœºåˆ¶å®ç°
- å¤šå¤´æ³¨æ„åŠ›
- ç›®æ ‡AUC: 0.92+

### Day 6-7: GraphSAGE
- é‚»å±…é‡‡æ ·
- èšåˆå™¨è®¾è®¡
- å¯æ‰©å±•æ€§éªŒè¯

---

## ğŸ“ Week 3å°ç»“

å®ŒæˆDay 1-3åï¼Œä½ å°†ï¼š

âœ… **æŒæ¡åŒæ„GNN** - ç†è§£GCNåŸç†å’Œå®ç°  
âœ… **PyGæ¡†æ¶** - ç†Ÿæ‚‰PyTorch Geometricç”¨æ³•  
âœ… **å›¾æ•°æ®å¤„ç†** - å¼‚æ„â†’åŒæ„çš„è½¬æ¢  
âœ… **æ€§èƒ½æå‡** - GCNç›¸æ¯”MLPæå‡0.5%+  

### å…³é”®æ”¶è·

1. **å›¾å·ç§¯åŸç†**: å¦‚ä½•èšåˆé‚»å±…ä¿¡æ¯
2. **æ¶ˆæ¯ä¼ é€’**: GCNçš„æ¶ˆæ¯ä¼ é€’æœºåˆ¶
3. **å›¾ä¸Šè®­ç»ƒ**: mask-basedè®­ç»ƒæ–¹å¼
4. **æ€§èƒ½å¯¹æ¯”**: å›¾ç»“æ„ä¿¡æ¯çš„ä»·å€¼

---

**å‡†å¤‡å¥½Day 4-7äº†å—ï¼Ÿ** ğŸš€

ç»§ç»­é˜…è¯»æœ¬æŒ‡å—çš„åç»­ç« èŠ‚ï¼Œæˆ–å…ˆå®ŒæˆDay 1-3çš„å®è·µï¼
